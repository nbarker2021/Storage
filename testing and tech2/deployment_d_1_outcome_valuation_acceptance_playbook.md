# Deployment D1 — Outcome Valuation & Acceptance Playbook

**Guiding Tenet:** *All results are valuable.* We strictly **accept only the best admissible result** per decision, but we **review, value, and retain every result** (accepted, runner‑up, invalid, or null) for learning, safety, and future reuse. Safety and governance gates remain lexicographic hard constraints.

> This playbook prepares the next deployment with concrete processes, schemas, metrics, and toggles that reflect the “uniquely holistic” principle and the staged preference/scheduling stack from prior canvases. No numeric constants are fixed; all thresholds are learned by testing.

---

## 1) Outcome Classes and Semantics
- **Accepted Best (A‑Best):** top‑scoring, ARPC‑validated (or L‑ARPC + tentative monitor) option; becomes the committed state change.
- **Runner‑Up (A‑Runner):** admissible but not selected; preserved as **shadow option** with valuation.
- **Negative Evidence (N‑Evd):** options vetoed by rungs, failed consensus, or ridge/residual anomalies; logged as **witness SNAPs** and may spawn **guard glyphs** and **forbidden masks**.
- **Null/No‑Effect (N‑Null):** explored actions that yield no change; still valued for calibration and coverage.

Each outcome receives a **Value‑of‑Result (VoR)** score and dedicated indexing for retrieval.

---

## 2) Decision Lifecycle with Valuation Hooks
1. **Candidate set** from AutoAcquire (Section 15) under AutoCost budgets (Section 16); cohort seeds from AutoSeed (Section 27).
2. **Safety gates** (helix rungs) → **safe set**.
3. **Scoring and batching** (IDS/KG/PESC/qEHVI mix) → proposed batch.
4. **Placement checks** via ARPC/L‑ARPC (Section 24/25) for placement‑bearing actions.
5. **Commit A‑Best** (tentative when +16 with L‑ARPC); execute monitors if tentative.
6. **Classify the rest** as A‑Runner / N‑Evd / N‑Null.
7. **Assign VoR** to every non‑A‑Best outcome; write **OutcomeRecord** to the archive.
8. **Update indices** (per‑shell, Master digest), **witness/guard** structures, and **review queues**.

---

## 3) Value‑of‑Result (VoR) — What Makes a “Valuable” Non‑Accepted Result?
**Components (monotone, learned aggregation; no fixed numbers):**
- **Info gain:** reduction in posterior entropy of utility or constraints; contribution to CCR‑Scalar weight learning.
- **Coverage & geometry:** Δ shell/neighbor coverage, Δ bridge discovery (valid/provisional), path multiplicity retention.
- **Calibration & safety:** drift detection, contradiction localization, rung‑margin learning.
- **Consensus leverage:** improvement potential for cross‑layout NTC intersection after external evidence.
- **Counterfactual utility:** expected uplift if revisited after a specified probe or subspace change.
- **Cost efficiency:** info‑per‑cost normalized by AutoCost.

**VoR classes:** High / Medium / Low with reasons (e.g., “ridge localization,” “bridge lead,” “calibration relief”).

---

## 4) Shadow Execution & Knowledge Retention
- **Shadow options:** For top A‑Runners, maintain **shadow glyph chains** and **replay recipes** (seeds, subspace, placements) for quick re‑try when context shifts.
- **Witnessing:** N‑Evd produces **witness SNAPs** with precise failure modes; updates **forbidden masks** and **counterfactual branches**.
- **Archival tiers:** hot (recent, high‑VoR), warm (medium‑VoR), cold (low‑VoR). Promotion/demotion based on reuse and audits.

---

## 5) Acceptance Is Strict; Valuation Is Broad
- **Strict acceptance:** Only A‑Best modifies state; Gold remains **zero‑ambiguity** (Section 20). Tentative +16 follows Section 25.
- **Broad valuation:** All others get VoR and are eligible for future reuse, testing, or evidence acquisition.
- **Non‑override guarantee:** Valuation cannot bypass rungs; N‑Evd never auto‑promotes without new evidence.

---

## 6) Metrics & Dashboards
- **Acceptance mix:** % A‑Best vs A‑Runner vs N‑Evd vs N‑Null.
- **VoR yield:** cumulative VoR per tick and per budget unit; high‑VoR recovery rate (reused later).
- **Learning impact:** Δ calibration error, Δ ridge incidents, Δ consensus speed attributable to valued non‑accepted results.
- **Safety coupling:** veto/rollback rate, witness reuse, forbidden‑mask hit reduction.
- **Master health:** closure growth, glyph coverage, stability vs thrash.

---

## 7) Processes & Queues
- **Review Queue:** prioritized by VoR, governance risk, and proximity to acceptance thresholds.
- **Reprobe Queue:** counterfactual targets generated by ARPC; auto‑scheduled via AutoAcquire.
- **De‑thrash Guard:** no resubmission of identical low‑VoR actions until context changes (seed/layout/subspace/evidence).

---

## 8) APIs & Schemas
### 8.1 OutcomeRecord (for any non‑A‑Best result)
```
OutcomeRecord {
  id: "out:…",
  decision_scope: { layer, shells, layout },
  class: A_Runner | N_Evd | N_Null,
  vor: { class: High|Med|Low, components: { info_gain, coverage, calibration, consensus, counterfactual, cost }, reasons: [ … ] },
  shadow: { glyphchain_id?:…, replay?: { seed_vector, subspace, placement_hint } },
  witnesses: [ snap_id? ],
  counterfactual_targets: [ { neighborhood, expected_info_gain } ],
  ttl: { created_tick, review_by },
  provenance: { scorer, policy_versions }
}
```

### 8.2 SNAP extensions
```
non_accepts: [ OutcomeRecord.id, … ]
vor_ledger: { totals: { High, Med, Low }, last_tick_delta: … }
```

---

## 9) Governance, Privacy, and Retention
- **Safety first:** storing value does not allow unsafe re‑execution; rungs gate any revisit.
- **Privacy:** only hashed ids and geometry/topology summaries; no raw content features.
- **Retention schedule:** hot→warm→cold with audit‑driven purges; legal/fairness domains may require stricter retention and redaction.

---

## 10) Deployment Toggles (default: on unless domain‑overridden)
- **AutoAcquire mix** (Section 15), **AutoCost** (Section 16), **AutoSeed** (Section 27), **AutoEmbed** (Section 19), **Asymmetric Tolerance** (Section 22), **L‑ARPC Tentative +16** (Section 25), **AutoSubspace** (Section 26), **ARPC** (Section 24).
- **Valuation on:** OutcomeRecord creation for all non‑A‑Best results; VoR ledger & dashboards.

---

## 11) Test Plan (no invented numbers)
1. **Replay A/B:** accept‑only vs accept+valuation; measure ΔU_H, safety ceilings, closure speed, and re‑use yield.
2. **Queue throughput:** Review/Reprobe latency vs success; impact on STOP storms.
3. **Valuation model ablations:** drop VoR components; measure regression in learning/safety.
4. **Cold‑start benefit:** time‑to‑bridge or closure in new domains with valuation on vs off.

---

## 12) Open Decisions (recorded)
1. VoR component weights and learning schedule.
2. Promotion/demotion rules across archival tiers.
3. Review Queue service‑level goals by domain.
4. Retention windows and redaction in regulated domains.
5. Dashboard layouts and alert thresholds for “Valuation Yield.”

