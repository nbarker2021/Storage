# Final Build 81025 — 20250811_002617UTC

This is a single-file consolidated view of the current full build (code + selected scripts + doc pointers).

---

## Documentation pointers (full docs are in the split build)
- docs/INDEX.md
- docs/INTAKE_GUIDE.md
- docs/NEXT_PHASE_PLAN.md
- docs/OPERATIONS_TIPS.md
- docs/OPERATOR_GUIDE.md
- docs/README.md
- docs/README_OPERATIONAL.md
- docs/RELEASE_NOTES.md
- docs/SYSTEM_EXPOSITION.md
- docs/TRACE.md

---

## Source code (src/)

### src/unified/agrm_unified/__init__.py

```python

# Optional Frankenstein helpers
from unified.franken.agrm_franken import *  # AGRM domain
from unified.franken.util_franken import *  # UTIL domain (if exists)

```

### src/unified/agrm_unified/cmplx_core.py

```python
# Frankenstein (refined) merge for cmplx_core.py
# Rank by annotations -> lines -> source score; provenance inline per block.

# provenance: py.zip :: .py/cmplx_core.py (score=80)
class AGRMCoreModulator:
    def __init__(self):
        self.previous_distance = None
        self.entropy_slope = 0
        self.entropy_slope_drops = 0
        self.shell_failure_count = 0
        self.shell_failure_limit = 3
        self.last_feedback_signal_count = 0
        self.entropy_floor = 0.04
        self.gradient_threshold = 2.0
        self.hysteresis_delay = 3
        self.hysteresis_buffer = []

    def compute_entropy_slope(self, history: List[float]):
        if len(history) < 3:
            return 0.0
        recent = history[-3:]
        slope = (recent[-1] - recent[0]) / max(1, abs(recent[0]))
        self.entropy_slope = slope
        return slope

    def detect_shell_failure(self, feedback_count: int):
        if feedback_count == 0:
            self.shell_failure_count += 1
        else:
            self.shell_failure_count = 0
        return self.shell_failure_count >= self.shell_failure_limit

    def detect_entropy_floor(self):
        return self.entropy_slope < self.entropy_floor

    def should_trigger_reset(self, feedback_count: int, current_distance: float, history: List[float]):
        slope = self.compute_entropy_slope(history)
        self.hysteresis_buffer.append(current_distance)
        if len(self.hysteresis_buffer) > self.hysteresis_delay:
            self.hysteresis_buffer.pop(0)

        if self.detect_shell_failure(feedback_count):
            print("[AGRMCore] Shell failure detected — reroute required.")
            return True

        if self.detect_entropy_floor():
            print(f"[AGRMCore] Entropy floor breached (slope={slope:.5f}) — reroute required.")
            return True

        return False

```

### src/unified/agrm_unified/runner.py

```python
def run_once(params, seed=None):
    import random
    random.seed(seed or 0)
    n = int(params.get('n', 9))
    route = list(range(n)); random.shuffle(route)
    return {'ok': True, 'params': params, 'route': route, 'score': float(len(route))}

```

### src/unified/agrm_unified/snapshots.py

```python
def list_snapshots():
    return []

```

### src/unified/agrm_unified/utils.py

```python
# Auto-merged module for utils.py
# Blocks selected from multiple archives, ranked by score/length/annotations.
# Provenance listed per block.

# --- provenance: py.zip :: .py/utils.py (score=171)
def int_to_kmer(int_kmer, k):
    """Converts an integer to a k-mer tuple."""
    kmer = ()
    for _ in range(k):
        digit = int_kmer % 10
        kmer = (digit,) + kmer
        int_kmer //= 10
    return kmer

# --- provenance: py.zip :: .py/utils.py (score=171)
def hash_permutation(perm):
    """Hashes a permutation (tuple or int)."""
    if isinstance(perm, tuple):
        return hash(perm)
    elif isinstance(perm, int):
        return perm
    else:
        raise TypeError("Permutation must be int or tuple")

# --- provenance: py.zip :: .py/utils.py (score=171)
def calculate_overlap(s1: str, s2: str) -> int:
    """Calculates the maximum overlap between two strings."""
    max_len = min(len(s1), len(s2))
    for i in range(max_len, 0, -1):
        if s1[-i:] == s2[:i]:
            return i
    return 0

# --- provenance: py.zip :: .py/utils.py (score=171)
def normalize_sequence(seq: str) -> str:
    """Normalizes a sequence by rotating it."""
    min_char = min(seq)
    min_indices = [i for i, c in enumerate(seq) if c == min_char]
    rotations = [seq[i:] + seq[:i] for i in min_indices]
    return min(rotations)

# --- provenance: py.zip :: .py/utils.py (score=171)
def kmer_to_int(kmer):
    """Converts a k-mer tuple to an integer."""
    int_kmer = 0
    for i, digit in enumerate(kmer):
        int_kmer = int_kmer * 10 + digit
    return int_kmer

# --- provenance: py.zip :: .py/utils.py (score=171)
def unhash_permutation(perm_hash, n):
    """Unhashes a permutation (int to tuple)."""
    if isinstance(perm_hash, int):
        return int_to_kmer(perm_hash, n)
    else:
        raise TypeError("Permutation hash must be int")

# --- provenance: py.zip :: .py/utils.py (score=171)
def is_valid_permutation(perm: tuple, n: int) -> bool:
    """Checks if a tuple is a valid permutation."""
    return len(perm) == n and set(perm) == set(range(1, n + 1))

# --- provenance: py.zip :: .py/utils.py (score=171)
def compute_checksum(data: str) -> str:
    """Computes the SHA-256 checksum of a string."""
    return hashlib.sha256(data.encode('utf-8')).hexdigest()

# --- provenance: py.zip :: .py/utils.py (score=171)
def generate_permutations(n: int):
    """Generates all permutations of numbers from 1 to n."""
    return list(itertools.permutations(range(1, n + 1)))

# --- provenance: py.zip :: .py/utils.py (score=171)
def setup_logging():
    """Sets up logging to a file."""
    logging.basicConfig(level=logging.DEBUG, filename='superpermutation.log', filemode='w', format='%(asctime)s - %(levelname)s - %(message)s')

# --- provenance: ConStruct 6- New Build.zip :: ConStruct 6- New Build/Codebase/base code for backup/best code as it stands/utils.py (score=96)
class ConfigManager:
    def __init__(self, config_file: str):
        self.config_file = config_file
        self.config = self.load()

    def load(self) -> Dict[str, Any]:
        with open(self.config_file, 'r') as f:
            return json.load(f)

    def get(self, key: str, default: Any = None) -> Any:
        keys = key.split('.')
        value = self.config
        for k in keys:
            if k in value:
                value = value[k]
            else:
                return default
        return value

    def set(self, key: str, value: Any):
        keys = key.split('.')
        config = self.config
        for k in keys[:-1]:
            if k not in config:
                config[k] = {}
            config = config[k]
        config[keys[-1]] = value

    def save(self):
        with open(self.config_file, 'w') as f:
            json.dump(self.config, f, indent=4)

# --- provenance: ConStruct 6- New Build.zip :: ConStruct 6- New Build/Codebase/base code for backup/best code as it stands/utils.py (score=96)
class ProgressTracker:
    def __init__(self, total_steps: int):
        self.total_steps = total_steps
        self.current_step = 0
        self.start_time = time.time()

    def update(self, steps: int = 1):
        self.current_step += steps
        progress = (self.current_step / self.total_steps) * 100
        elapsed_time = time.time() - self.start_time
        print(f"Progress: {progress:.2f}% | Time elapsed: {elapsed_time:.2f} seconds")

    def reset(self):
        self.current_step = 0
        self.start_time = time.time()

# --- provenance: ConStruct 6- New Build.zip :: ConStruct 6- New Build/Codebase/base code for backup/best code as it stands/utils.py (score=96)
def split_problem(problem: str, num_parts: int) -> List[str]:
    n = len(problem)
    part_size = n // num_parts
    return [problem[i:i+part_size] for i in range(0, n, part_size)]

# --- provenance: ConStruct 6- New Build.zip :: ConStruct 6- New Build/Codebase/base code for backup/best code as it stands/utils.py (score=96)
def solve_subproblem(subproblem: str) -> str:
    # Placeholder implementation
    return subproblem[::-1]

# --- provenance: ConStruct 6- New Build.zip :: ConStruct 6- New Build/Codebase/base code for backup/best code as it stands/utils.py (score=96)
def combine_results(results: List[str]) -> str:
    return ''.join(results)


```

### src/unified/api.py

```python
from __future__ import annotations
from pathlib import Path
from typing import Dict, Any, Optional
from unified.pipeline import run_pipeline

def build_snapshots(params: Dict[str, Any], seed: Optional[int]=None):
    from unified.integration.agrm_unified_adapter import AGRMBuilderUnified
    b = AGRMBuilderUnified(); return b.build({'params': params}, seed=seed)

def index_put(snapshot: Dict[str, Any]):
    from unified.integration.mdhg_adapter import MDHGIndexAdapter
    from unified.integration import bridge as br
    from unified.e8.persistence import db
    idx = MDHGIndexAdapter();
    for g in br.snapshot_to_glyphs(snapshot):
        db.put_glyph(g)

def sweep(bits: int=2, seed: int=0):
    from unified.e8.jobs.queue import sweep_promotion
    return sweep_promotion(bits=bits, seed=seed)

def run(params: Dict[str, Any], seed: Optional[int]=None, reports_dir: Optional[Path]=None, db_path: Optional[Path]=None):
    return run_pipeline(params=params, seed=seed, reports_dir=reports_dir, db_path=db_path)

```

### src/unified/config/shaper.py

```python
DEFAULT_FAMILIES = ['A']
DEFAULT_KS = [3, 4, 5, 6]
# Rationale: family 'A' improves cohesion while maintaining novelty; k-range preserves variety.

```

### src/unified/e8/__init__.py

```python

```

### src/unified/e8/glyphs.py

```python

"""
E8 lattice scaffold — glyphs.py

Purpose: Glyph representations, claims, timestamps, and vector accessors.

This file intentionally contains NO implementation. It documents interfaces and
raises NotImplementedError to avoid inventing algorithms beyond the provided docs.
"""
from typing import Any, Dict, Iterable, Optional

def TODO_placeholder(*args, **kwargs):
    """Placeholder per spec. Replace with real implementation sourced from the project's E8 code when available."""
    raise NotImplementedError("E8 module 'glyphs.py' requires real implementation from project sources.")

```

### src/unified/e8/goldens.py

```python

from __future__ import annotations
import json, time, numpy as np
from dataclasses import dataclass
from typing import List, Tuple
from .persistence import db
from .jobs.queue import sweep_promotion

@dataclass
class GoldenCase:
    name: str
    action: str  # 'promotion'
    expect_min_promoted: int = 0

def run(cases: List[GoldenCase]) -> List[Tuple[str, bool, dict]]:
    db.init()
    out = []
    for c in cases:
        if c.action == 'promotion':
            res = sweep_promotion()
            ok = res.get('promoted',0) >= c.expect_min_promoted
            out.append((c.name, ok, res))
    return out

```

### src/unified/e8/governance.py

```python

"""
E8 lattice scaffold — governance.py

Purpose: Tenancy/roles/capability guards; policy contracts only.

This file intentionally contains NO implementation. It documents interfaces and
raises NotImplementedError to avoid inventing algorithms beyond the provided docs.
"""
from typing import Any, Dict, Iterable, Optional

def TODO_placeholder(*args, **kwargs):
    """Placeholder per spec. Replace with real implementation sourced from the project's E8 code when available."""
    raise NotImplementedError("E8 module 'governance.py' requires real implementation from project sources.")

```

### src/unified/e8/hashing.py

```python

from __future__ import annotations
import numpy as np
from dataclasses import dataclass

@dataclass
class SemanticHash:
    W: np.ndarray  # shape (B, D), B bits

    @classmethod
    def init(cls, bits: int, dim: int, seed: int = 0):
        rng = np.random.RandomState(seed)
        W = rng.randn(bits, dim).astype(np.float32)
        return cls(W=W)

    def infer_bucket(self, v: np.ndarray) -> str:
        z = self.W @ v  # (B,)
        b = (z >= 0).astype(np.uint8)
        # return as hex string (pack bits)
        acc = 0
        for bit in b:
            acc = (acc << 1) | int(bit)
        width = max(1, len(b)//4)
        return f"{acc:0{width}x}"

    def train(self, X: np.ndarray, labels: np.ndarray, lr: float=1e-3, epochs: int=5):
        # Simple perceptron-like alignment: move W to align with labeled cluster centers
        classes = sorted(set(labels.tolist()))
        centers = {c: X[labels==c].mean(axis=0) for c in classes}
        for _ in range(epochs):
            for c, mu in centers.items():
                # push W rows toward/away from mu randomly for diversity
                self.W += lr * (np.sign(mu)[None, :] - 0.001 * self.W)
        return self

```

### src/unified/e8/jobs/queue.py

```python

from __future__ import annotations
import time, numpy as np
from typing import Dict, Any
from ..persistence import db
from ..hashing import SemanticHash
from ..shells import build_shells, evaluate_shells
from ..policy import Policy

def sweep_promotion(bits: int = 32, seed: int = 0) -> Dict[str, Any]:
    db.init()
    glyphs_list = db.list_glyphs()
    glyphs = {g['gid']: {'vector': np.asarray(g['vector']), 'claims': g['claims']} for g in glyphs_list}
    if not glyphs: return {'promoted': 0, 'shells': 0}
    dim = len(next(iter(glyphs.values()))['vector'])
    hasher = SemanticHash.init(bits=bits, dim=dim, seed=seed)
    bucket_of = {gid: hasher.infer_bucket(g['vector']) for gid, g in glyphs.items()}
    shells = build_shells(glyphs, bucket_of)
    decisions = evaluate_shells(shells, glyphs, Policy())
    promoted = 0
    for sid, sh in shells.items():
        db.put_shell({'sid': sid, 'center': sh.center, 'promoted': sh.promoted, 'meta': decisions[sid], 'gids': sh.gids})
        if sh.promoted: promoted += 1
    return {'promoted': promoted, 'shells': len(shells), 'decisions': decisions}

def sweep_contradictions() -> Dict[str, Any]:
    # Simple pass since contradiction risk is calculated per shell
    return {'status': 'ok'}

def sweep_reindex() -> Dict[str, Any]:
    # Placeholder for a heavier reindex; v1 no-op
    return {'status': 'ok'}

```

### src/unified/e8/persistence/db.py

```python

from __future__ import annotations
import sqlite3, json, os
from typing import Dict, Any, List, Optional

DB_PATH = os.environ.get('UNIFIED_DB', 'unified.db')

def _conn():
    return sqlite3.connect(DB_PATH)

def init():
    with _conn() as cx:
        cx.execute("CREATE TABLE IF NOT EXISTS glyphs (gid TEXT PRIMARY KEY, vec TEXT, ts REAL, mdhg_key TEXT)")
        cx.execute("CREATE TABLE IF NOT EXISTS claims (gid TEXT, text TEXT, ts REAL, source TEXT, contradicted INT)")
        cx.execute("CREATE TABLE IF NOT EXISTS shells (sid TEXT PRIMARY KEY, center TEXT, promoted INT, meta TEXT)")
        cx.execute("CREATE TABLE IF NOT EXISTS shell_members (sid TEXT, gid TEXT)")
        cx.execute("CREATE TABLE IF NOT EXISTS weights (id INTEGER PRIMARY KEY, W TEXT)")
        cx.execute("CREATE TABLE IF NOT EXISTS metrics (ts REAL, name TEXT, value REAL)")

def put_glyph(g: Dict[str, Any]) -> None:
    with _conn() as cx:
        cx.execute("REPLACE INTO glyphs (gid, vec, ts, mdhg_key) VALUES (?,?,?,?)",
                   (g['gid'], json.dumps((g['vector'].tolist() if hasattr(g['vector'], 'tolist') else list(g['vector']))), g['ts'], g.get('mdhg_key')))
        for c in g.get('claims', []):
            cx.execute("INSERT INTO claims (gid, text, ts, source, contradicted) VALUES (?,?,?,?,?)",
                      (g['gid'], c.get('text',''), c.get('ts',0.0), c.get('source',''), int(bool(c.get('contradicted')))))

def list_glyphs() -> List[Dict[str, Any]]:
    out = []
    with _conn() as cx:
        for gid, vec, ts, mdhg_key in cx.execute("SELECT gid, vec, ts, mdhg_key FROM glyphs"):
            out.append({'gid': gid, 'vector': json.loads(vec), 'ts': ts, 'mdhg_key': mdhg_key, 'claims': list_claims(gid)})
    return out

def list_claims(gid: str) -> List[Dict[str, Any]]:
    with _conn() as cx:
        rows = list(cx.execute("SELECT text, ts, source, contradicted FROM claims WHERE gid=?", (gid,)))
    return [{'text': t, 'ts': ts, 'source': s, 'contradicted': bool(c)} for (t, ts, s, c) in rows]

def put_shell(s: Dict[str, Any]) -> None:
    with _conn() as cx:
        cx.execute("REPLACE INTO shells (sid, center, promoted, meta) VALUES (?,?,?,?)",
                   (s['sid'], json.dumps(s['center'].tolist()), int(s.get('promoted', False)), json.dumps(s.get('meta', {}))))
        cx.execute("DELETE FROM shell_members WHERE sid=?", (s['sid'],))
        for gid in s.get('gids', []):
            cx.execute("INSERT INTO shell_members (sid, gid) VALUES (?,?)", (s['sid'], gid))

def list_shells() -> List[Dict[str, Any]]:
    with _conn() as cx:
        rows = list(cx.execute("SELECT sid, center, promoted, meta FROM shells"))
    out = []
    for sid, center, promoted, meta in rows:
        out.append({'sid': sid, 'center': json.loads(center), 'promoted': bool(promoted), 'meta': json.loads(meta), 'gids': list_shell_members(sid)})
    return out

def list_shell_members(sid: str) -> List[str]:
    with _conn() as cx:
        return [gid for (gid,) in cx.execute("SELECT gid FROM shell_members WHERE sid=?", (sid,))]

def save_weights(W) -> None:
    import numpy as np, json
    with _conn() as cx:
        cx.execute("INSERT INTO weights (W) VALUES (?)", (json.dumps(np.asarray(W).tolist()),))

def load_weights():
    import json, numpy as np
    with _conn() as cx:
        row = cx.execute("SELECT W FROM weights ORDER BY id DESC LIMIT 1").fetchone()
    if not row: return None
    return np.asarray(json.loads(row[0]))

def record_metric(name: str, value: float, ts: float) -> None:
    with _conn() as cx:
        cx.execute("INSERT INTO metrics (ts, name, value) VALUES (?,?,?)", (ts, name, value))

```

### src/unified/e8/policy.py

```python

from __future__ import annotations
from dataclasses import dataclass
from typing import Dict, List, Tuple
import numpy as np
from .scoring import coherence, redundancy, novelty, contradiction_risk

@dataclass
class Policy:
    tau_coh: float = 0.60
    tau_red: float = 0.92
    tau_nov: Tuple[float,float] = (0.02, 0.40)
    flip_rate_max: float = 0.20

@dataclass
class ProbeReport:
    coh: float
    red: float
    nov: float
    contra: float
    size: int
    thresholds: Dict[str, float]
    decision: str

def evaluate(vectors: np.ndarray, claims: List[dict], policy: Policy) -> Tuple[bool, ProbeReport]:
    coh = coherence(vectors)
    red = redundancy(vectors)
    nov = novelty(vectors)
    contra = contradiction_risk(claims)
    ok = (
        coh >= policy.tau_coh and
        red <= policy.tau_red and
        policy.tau_nov[0] <= nov <= policy.tau_nov[1] and
        contra == 0.0
    )
    rep = ProbeReport(
        coh=coh, red=red, nov=nov, contra=contra, size=int(vectors.shape[0]),
        thresholds={'tau_coh': policy.tau_coh, 'tau_red': policy.tau_red, 'tau_nov_min': policy.tau_nov[0], 'tau_nov_max': policy.tau_nov[1]},
        decision='approve' if ok else 'reject'
    )
    return ok, rep

```

### src/unified/e8/probes.py

```python

"""
E8 lattice scaffold — probes.py

Purpose: Probe functions attached to promotion reports; no algorithms here.

This file intentionally contains NO implementation. It documents interfaces and
raises NotImplementedError to avoid inventing algorithms beyond the provided docs.
"""
from typing import Any, Dict, Iterable, Optional

def TODO_placeholder(*args, **kwargs):
    """Placeholder per spec. Replace with real implementation sourced from the project's E8 code when available."""
    raise NotImplementedError("E8 module 'probes.py' requires real implementation from project sources.")

```

### src/unified/e8/rest/ui.py

```python

"""
E8 lattice scaffold — rest/ui.py

Purpose: REST+UI endpoints described in docs; no handlers implemented here.

This file intentionally contains NO implementation. It documents interfaces and
raises NotImplementedError to avoid inventing algorithms beyond the provided docs.
"""
from typing import Any, Dict, Iterable, Optional

def TODO_placeholder(*args, **kwargs):
    """Placeholder per spec. Replace with real implementation sourced from the project's E8 code when available."""
    raise NotImplementedError("E8 module 'rest/ui.py' requires real implementation from project sources.")

```

### src/unified/e8/router.py

```python

from __future__ import annotations
import numpy as np, time
from typing import Dict, Any, List
from .persistence import db

class Router:
    def __init__(self, p95_ms_target: float = 150.0):
        self.p95_ms_target = p95_ms_target
        self.K = 50  # starting candidate pool size

    def query(self, qvec: np.ndarray, budget: int = 10) -> List[str]:
        # naive retrieval: linear scan over glyphs + top-K by cosine
        t0 = time.time()
        gl = db.list_glyphs()
        if not gl: return []
        V = np.stack([np.asarray(g['vector']) for g in gl], axis=0)
        qs = (V @ qvec) / ((np.linalg.norm(V, axis=1)+1e-12) * (np.linalg.norm(qvec)+1e-12))
        idx = np.argsort(-qs)[:min(self.K, len(gl))]
        gids = [gl[i]['gid'] for i in idx[:budget]]
        elapsed = (time.time() - t0) * 1000.0
        self._adapt_latency(elapsed)
        return gids

    def _adapt_latency(self, ms: float) -> None:
        # Simple controller: if over target, reduce K; else cautiously increase
        if ms > self.p95_ms_target and self.K > 10:
            self.K = int(self.K * 0.9)
        elif ms < 0.5 * self.p95_ms_target and self.K < 2000:
            self.K = int(self.K * 1.1)

```

### src/unified/e8/scoring.py

```python

from __future__ import annotations
import numpy as np
from dataclasses import dataclass
from typing import List
from math import isclose

def _cos(a: np.ndarray, b: np.ndarray) -> float:
    na = np.linalg.norm(a) + 1e-12
    nb = np.linalg.norm(b) + 1e-12
    return float(np.dot(a, b) / (na * nb))

def coherence(vs: np.ndarray, center: np.ndarray | None=None) -> float:
    if vs.size == 0: return 0.0
    c = center if center is not None else vs.mean(axis=0)
    sims = [ _cos(v, c) for v in vs ]
    return float(np.mean(sims))

def redundancy(vs: np.ndarray) -> float:
    n = vs.shape[0]
    if n < 2: return 0.0
    sims = []
    for i in range(n):
        for j in range(i+1, n):
            sims.append(_cos(vs[i], vs[j]))
    return float(np.mean(sims)) if sims else 0.0

def novelty(vs: np.ndarray, center: np.ndarray | None=None) -> float:
    if vs.size == 0: return 0.0
    c = center if center is not None else vs.mean(axis=0)
    sims = [ _cos(v, c) for v in vs ]
    return float(np.mean([1.0 - s for s in sims]))

def contradiction_risk(claims: List[dict]) -> float:
    # Simple rule per docs: any flagged contradiction -> risk 1.0
    return 1.0 if any(c.get('contradicted') for c in claims) else 0.0

```

### src/unified/e8/shell_policy.py

```python

"""
E8 lattice scaffold — shell_policy.py

Purpose: Promotion policy using coherence/redundancy/novelty thresholds and flip-rate ceilings.

This file intentionally contains NO implementation. It documents interfaces and
raises NotImplementedError to avoid inventing algorithms beyond the provided docs.
"""
from typing import Any, Dict, Iterable, Optional

def TODO_placeholder(*args, **kwargs):
    """Placeholder per spec. Replace with real implementation sourced from the project's E8 code when available."""
    raise NotImplementedError("E8 module 'shell_policy.py' requires real implementation from project sources.")

```

### src/unified/e8/shells.py

```python

from __future__ import annotations
from dataclasses import dataclass, field
from typing import List, Dict, Tuple
import numpy as np
from .policy import Policy, evaluate

@dataclass
class Shell:
    sid: str
    center: np.ndarray
    gids: List[str] = field(default_factory=list)
    promoted: bool = False
    meta: Dict = field(default_factory=dict)

def build_shells(glyphs: Dict[str, dict], bucket_of: Dict[str, str]) -> Dict[str, Shell]:
    # Group glyphs by bucket id into shells; center is mean vector
    shells: Dict[str, Shell] = {}
    for gid, g in glyphs.items():
        b = bucket_of[gid]
        if b not in shells:
            shells[b] = Shell(sid=b, center=np.zeros_like(g['vector']))
        shells[b].gids.append(gid)
    # Compute centers
    for b, sh in shells.items():
        vs = np.stack([glyphs[gid]['vector'] for gid in sh.gids], axis=0)
        sh.center = vs.mean(axis=0)
    return shells

def evaluate_shells(shells: Dict[str, Shell], glyphs: Dict[str, dict], policy: Policy):
    decisions = {}
    for sid, sh in shells.items():
        vs = np.stack([glyphs[gid]['vector'] for gid in sh.gids], axis=0)
        claims = sum([glyphs[gid]['claims'] for gid in sh.gids], [])
        ok, rep = evaluate(vs, claims, policy)
        sh.promoted = bool(ok)
        decisions[sid] = rep.__dict__
    return decisions

```

### src/unified/e8/tuner.py

```python

"""
E8 lattice scaffold — tuner.py

Purpose: Offline training of semantic-hash weights; only interface stub here.

This file intentionally contains NO implementation. It documents interfaces and
raises NotImplementedError to avoid inventing algorithms beyond the provided docs.
"""
from typing import Any, Dict, Iterable, Optional

def TODO_placeholder(*args, **kwargs):
    """Placeholder per spec. Replace with real implementation sourced from the project's E8 code when available."""
    raise NotImplementedError("E8 module 'tuner.py' requires real implementation from project sources.")

```

### src/unified/embedder/__init__.py

```python

```

### src/unified/embedder/autotune.py

```python
import numpy as np
import importlib
def to_embed(func, dim=64):
    def _wrap(payload: dict):
        v = func(payload)
        v = np.asarray(v, dtype=np.float32).reshape(-1)
        if v.size != dim:
            rng = np.random.RandomState(abs(hash(func.__name__)) % (2**32))
            P = rng.randn(v.size, dim).astype(np.float32)
            v = v @ P
        n = np.linalg.norm(v)+1e-12
        return (v/n).astype(np.float32)
    return _wrap
def find_embed_callables(mod_names):
    funcs=[]
    for mn in mod_names:
        try:
            M = importlib.import_module(mn)
        except Exception:
            continue
        for name, obj in M.__dict__.items():
            if callable(obj) and any(k in name.lower() for k in ['embed','encode','vector','feature','project']):
                funcs.append((mn, name, obj))
    return funcs

```

### src/unified/embedder/cand_10_benchmark_suite__1__py.py

```python
# benchmark_suite.py
# Benchmark runner for MDHGHashTable vs. native dict

import time, random
from mdhg_hash import MDHGHashTable

def run_benchmark(n_ops=10000):
    table = MDHGHashTable(capacity=n_ops * 2)
    native = {}

    keys = [random.randint(0, n_ops * 10) for _ in range(n_ops)]
    vals = [f"v{i}" for i in range(n_ops)]

    print("Benchmarking MDHGHashTable...")
    t0 = time.time()
    for k, v in zip(keys, vals):
        table.put(k, v)
    for k in keys:
        table.get(k)
    t1 = time.time()

    print("Benchmarking native dict...")
    t2 = time.time()
    for k, v in zip(keys, vals):
        native[k] = v
    for k in keys:
        native.get(k)
    t3 = time.time()

    print(f"MDHG total time: {t1 - t0:.4f}s")
    print(f"Dict total time: {t3 - t2:.4f}s")

if __name__ == "__main__":
    run_benchmark(10000)

```

### src/unified/embedder/cand_11_benchmark_suite_py.py

```python
# benchmark_suite.py
# Benchmarking framework for evaluating hash tables including MDHG and native dict

# (Full code from your HashTableBenchmark and run_benchmarks function goes here.
# Omitted for brevity, assumed to be the finalized code from earlier.)

if __name__ == "__main__":
    results, benchmark = run_benchmarks()

```

### src/unified/embedder/cand_12_Candidate_Generation_and_Scoring_py.py

```python
# construct_superpermutation.py
import random
import logging
import json
from utils import calculate_overlap, hash_permutation, unhash_permutation, kmer_to_int, int_to_kmer, is_valid_permutation
from layout_memory import LayoutMemory
from analysis_scripts_final import is_prodigal, generate_hypothetical_prodigals, generate_permutations

# ... (Other parts of the file - as before)

def generate_candidates(superpermutation, missing_permutations, prodigal_results, winners, losers, layout_memory, n, eput, limbo_list):
    """Generates candidate permutations.

    This function combines several strategies to generate promising candidates:
    1. Candidates from missing permutations (prioritized).
    2. Candidates extending known prodigal results.
    3. Candidates connecting to "winners" (good sequences).
    """

    candidates = set()
    k = n - 1  # k-mer length

    # 1. Candidates from missing permutations (prioritized)
    for missing_perm_hash in missing_permutations:
        missing_perm = unhash_permutation(missing_perm_hash, n)

        # Generate candidates by extending missing permutations
        for i in range(1, n + 1):  # Try adding each digit at the end
            candidate_perm = missing_perm + (i,)
            if is_valid_permutation(candidate_perm, n):
                candidate_hash = hash_permutation(candidate_perm)
                if candidate_hash not in eput and candidate_hash not in limbo_list:
                    candidates.add(candidate_hash)

        # Generate candidates by prepending to missing permutations
        for i in range(1, n + 1):  # Try adding each digit at the beginning
            candidate_perm = (i,) + missing_perm
            if is_valid_permutation(candidate_perm, n):
                candidate_hash = hash_permutation(candidate_perm)
                if candidate_hash not in eput and candidate_hash not in limbo_list:
                    candidates.add(candidate_hash)

        #Generate candidates by combining with existing superpermutation
        if superpermutation:
            last_k_minus_1 = tuple(int(digit) for digit in superpermutation[-(k):]) # Last k-1 digits
            for i in range(1, n + 1):
                candidate_perm = last_k_minus_1 + (i,)
                if is_valid_permutation(candidate_perm, n):
                    candidate_hash = hash_permutation(candidate_perm)
                    if candidate_hash not in eput and candidate_hash not in limbo_list:
                        candidates.add(candidate_hash)

    # 2. Candidates extending known prodigal results
    for prodigal_hash in prodigal_results:
        prodigal = unhash_permutation(prodigal_hash, n)
        for i in range(1, n + 1):
            candidate_perm = prodigal + (i,)
            if is_valid_permutation(candidate_perm, n):
                candidate_hash = hash_permutation(candidate_perm)
                if candidate_hash not in eput and candidate_hash not in limbo_list:
                    candidates.add(candidate_hash)


    # 3. Candidates connecting to "winners" (good sequences)
    for winner_hash in winners:
        winner = unhash_permutation(winner_hash, n)
        # ... (Add logic here to generate candidates connecting to winners)

    return candidates


def calculate_score(current_superpermutation, permutation_hash, prodigal_results, winners, losers, layout_memory, n, missing_permutations, eput):
    """Calculates the score for adding a permutation.

    The score combines several factors:
    1. Overlap with the current superpermutation.
    2. Bonus for covering a missing permutation.
    3. Layout score (from LayoutMemory).
    4. Prodigal bonus.
    5. Winner/loser bonus/penalty.
    """

    permutation = unhash_permutation(permutation_hash, n)
    permutation_string = "".join(str(x) for x in permutation)
    overlap = calculate_overlap(current_superpermutation, permutation_string)

    score = overlap * 5  # Base score (adjust weight)

    if permutation_hash in missing_permutations:
        score += 1000  # Bonus for covering missing permutation (adjust weight)

    # Layout score
    k = n - 1
    if current_superpermutation: # Make sure it is not empty
        last_k_minus_1 = tuple(int(digit) for digit in current_superpermutation[-k:])
        kmer2 = tuple(int(digit) for digit in permutation_string[:k])
        score += layout_memory.get_layout_score(last_k_minus_1, kmer2) * 2  # Adjust weight

    # Prodigal bonus (if applicable)
    if permutation_hash in prodigal_results:
        score += 500  # Adjust weight

    # Winner/loser bonus/penalty (if applicable)
    if permutation_hash in winners:
        score += 200 # Adjust weight
    elif permutation_hash in losers:
        score -= 100 # Adjust weight

    return score

# ... (Rest of the file - as before)
```

### src/unified/embedder/cand_14_completion_algorithm_n8_py.py

```python
def complete_superpermutation(superpermutation: str, n: int, all_permutations: list[tuple[int, ...]],
                              prodigal_results: dict, winners: dict, losers: dict,
                              layout_memory: LayoutMemory, limbo_list: set) -> str:
    """Adds missing permutations to a near-complete superpermutation.

    Args:
        superpermutation (str): The initial (incomplete) superpermutation.
        n (int): The value of n.
        all_permutations (list): A list of *all* n! permutations (as tuples).
        prodigal_results (dict): Dictionary of ProdigalResult objects.
        winners (dict): Dictionary of Winner k-mers.
        losers (dict): Dictionary of Loser k-mers.
        layout_memory (LayoutMemory): The LayoutMemory object.
        limbo_list (set): The Limbo List.

    Returns:
        str: The completed superpermutation.
    """
    missing_permutations = set()
    s_tuple = tuple(int(x) for x in superpermutation)

    for p in all_permutations:
        found = False
        for i in range(len(s_tuple) - n + 1):
            if s_tuple[i:i+n] == p:
                found = True
                break
        if not found:
            missing_permutations.add(hash_permutation(p))  # Store as hashes

    working_superpermutation = list(superpermutation)  # Convert to list for mutability
    eput = {} #Local ePUT

    #Populate ePUT with current superpermutation data:
    s_tuple = tuple(int(x) for x in superpermutation)
    for i in range(len(s_tuple) - n + 1):
        perm = s_tuple[i:i+n]
        if is_valid_permutation(perm, n):
            perm_hash = hash_permutation(perm)
            if perm_hash not in eput:
              eput[perm_hash] = PermutationData(perm, in_sample = False, creation_method="prodigal") # These are from previous runs
            eput[perm_hash].used_count += 1
            eput[perm_hash].used_in_final = True
            #Update neighbors
            if i > 0:
                prev_perm = s_tuple[i-1:i-1+n]
                if is_valid_permutation(prev_perm, n):
                     eput[perm_hash].neighbors.add(hash_permutation(prev_perm))
            if i < len(s_tuple) - n:
                next_perm = s_tuple[i+1:i+1+n]
                if is_valid_permutation(next_perm, n):
                    eput[perm_hash].neighbors.add(hash_permutation(next_perm))
    
    # --- Main Completion Loop ---
    num_added = 0
    total_missing = len(missing_permutations)
    
    #Create initial hypothetical prodigals
    hypothetical_prodigals = analysis_scripts.generate_hypothetical_prodigals(prodigal_results, winners, losers, n)
    
    while missing_permutations: # Continue until all permutations are added
        
        #Randomly select missing permutation
        missing_perm_hash = random.choice(list(missing_permutations))
        missing_perm = unhash_permutation(missing_perm_hash, n)
        best_insertion_point = -1
        best_score = -float('inf')

        # 1. Generate Targeted Candidates (using the modified function)
        candidates = generate_completion_candidates(
            "".join(str(x) for x in working_superpermutation),  # Current superperm
            missing_permutations,  # Remaining missing perms
            prodigal_results,       # Prodigal results
            winners,               # Winners
            losers,                # Losers
            n,                     # n
            eput,                  # ePUT
            limbo_list             # Limbo list
        )

        # 2. Evaluate Candidates and Insert (using the enhanced scoring function)
        golden_ratio_points = calculate_golden_ratio_points(len(working_superpermutation), levels=3)
        for candidate_hash in candidates:
            candidate_perm = unhash_permutation(candidate_hash, n)
            candidate_string = "".join(str(x) for x in candidate_perm)

            for i in range(len(working_superpermutation) + 1):
                # Calculate overlap if inserted here.
                overlap_start = calculate_overlap("".join(str(x) for x in working_superpermutation[max(0,i-(n-1)):i]), candidate_string)
                overlap_end = calculate_overlap(candidate_string, "".join(str(x) for x in working_superpermutation[i:i+(n-1)]))
                overlap = max(overlap_start, overlap_end)

                # "Prodigal Disruption":
                prodigal_disruption = 0
                for prodigal_id, prodigal in prodigal_results.items():
                    if prodigal.sequence in "".join(str(x) for x in working_superpermutation):
                        # Check if insertion breaks it.
                        if not (prodigal.sequence in "".join(str(x) for x in working_superpermutation[:i + (n - 1)]) and \
                                prodigal.sequence in "".join(str(x) for x in working_superpermutation[i - (n - 1):])):
                            prodigal_disruption += prodigal.length  # Penalty

                # Golden Ratio Bonus
                golden_ratio_bonus = 0
                for point in golden_ratio_points:
                    distance = abs(i - point)
                    golden_ratio_bonus += math.exp(-distance / (len(working_superpermutation) / 20))

                # Winner/Loser Bonus/Penalty (using Layout Memory)
                k_values = [n - 1, n - 2]
                layout_bonus = 0
                for k in k_values:
                    if i >= k:
                        kmer_end = "".join(str(x) for x in working_superpermutation[i-k:i])
                        kmer_start = candidate_string[:k]
                        layout_bonus += layout_memory.get_layout_score(kmer_end, kmer_start, (len(working_superpermutation)-i) if overlap == 0 else 1)  # Use correct distance
                    kmer_end = candidate_string[-k:]
                    kmer_start = "".join(str(x) for x in working_superpermutation[i:i+k])
                    layout_bonus += layout_memory.get_layout_score(kmer_end, kmer_start, 1)

                # --- Prodigal Bonus ---
                prodigal_bonus = 0
                for p_id, p_data in prodigal_results.items():
                    if candidate_string in p_data.sequence:
                        prodigal_bonus = p_data.length * 200
                        break

                # --- Loser Penalty (Veto) ---
                loser_penalty = 0
                for k in [7, 6]:  # Check 7-mers and 6-mers
                    for j in range(len(candidate_string) - k + 1):
                        kmer = candidate_string[j:j+k]
                        loser_penalty += losers.get(kmer, 0) * 5

                # --- Missing Permutation Bonus (CRUCIAL) ---
                missing_bonus = 0
                if candidate_hash in missing_permutations:
                    missing_bonus = 10000 # Very Large bonus.

                 # Higher-Order Winners/Losers
                higher_order_bonus = 0
                for seq_length in [2, 3]:  # Check sequences of length 2 and 3
                  if len(current_superpermutation) >= (n * seq_length):
                    prev_seq = current_superpermutation[-(n*seq_length):]
                    prev_perms = []
                    for i in range(len(prev_seq) - n + 1):
                        pp = tuple(int(x) for x in prev_seq[i:i+n])
                        if is_valid_permutation(pp, n):
                            prev_perms.append(hash_permutation(pp))
                    if len(prev_perms) >= (seq_length -1):
                        current_seq = tuple(prev_perms[-(seq_length - 1):] + [permutation_hash])
                        current_seq_hash = hash(current_seq)
                        higher_order_bonus += winners.get(current_seq_hash, 0) * 5
                        loser_penalty += losers.get(current_seq_hash, 0) * 5

                score = (overlap * 5) + layout_bonus + golden_ratio_bonus + missing_bonus + prodigal_bonus - loser_penalty - prodigal_disruption + higher_order_bonus

                if score > best_score:
                    best_score = score
                    best_insertion_point = i

        # --- Insertion ---
        if best_insertion_point != -1: #Should always find a spot
            #print(f"Inserting permutation: {unhash_permutation(missing_perm_hash, n)} at position {best_insertion_point}")
            working_superpermutation = working_superpermutation[:best_insertion_point] + list(str(x) for x in missing_perm) + working_superpermutation[best_insertion_point:]
            num_added += 1

            # --- Local Optimization (Swapping) ---
            for j in range(max(0, best_insertion_point - 10), min(len(working_superpermutation) - n, best_insertion_point + n + 10)):
                original_perm = tuple(int(x) for x in working_superpermutation[j:j + n])
                if not is_valid_permutation(original_perm,n):
                    continue
                for k in range(j + 1, min(len(working_superpermutation) - n, best_insertion_point + n * 2 + 10)):
                    compare_perm = tuple(int(x) for x in working_superpermutation[k:k + n])
                    if not is_valid_permutation(compare_perm,n):
                        continue
                    # Calculate overlap before and after swap
                    overlap_before = calculate_overlap("".join(str(x) for x in working_superpermutation[j - n:j]),
                                                     "".join(str(x) for x in original_perm)) + \
                                     calculate_overlap("".join(str(x) for x in original_perm),
                                                     "".join(str(x) for x in working_superpermutation[j + n:j + 2 * n]))

                    overlap_after = calculate_overlap("".join(str(x) for x in working_superpermutation[j - n:j]),
                                                    "".join(str(x) for x in compare_perm)) + \
                                    calculate_overlap("".join(str(x) for x in compare_perm),
                                                    "".join(str(x) for x in working_superpermutation[j + n:j + 2 * n]))

                    if overlap_after > overlap_before:
                        # Perform Swap
                        temp = working_superpermutation[j:j + n]
                        working_superpermutation[j:j + n] = working_superpermutation[k:k + n]
                        working_superpermutation[k:k + n] = temp

            #Update ePUT
            perm_hash = missing_perm_hash
            if perm_hash not in eput: #Should now always be the case.
               eput[perm_hash] = PermutationData(missing_perm, in_sample = False, creation_method="completion")
            eput[perm_hash].used_count = 1
            eput[perm_hash].used_in_final = True
            #Update neighbors in ePUT.  Less important here.
            perm_string = "".join(str(x) for x in missing_perm)
            for k in [n-1, n-2]:
                prefix = "".join(str(x) for x in working_superpermutation[:best_insertion_point + k])
                suffix = "".join(str(x) for x in working_superpermutation[best_insertion_point + len(perm_string)-k:])

                prefix_perms = set()
                suffix_perms = set()
                for i in range(len(prefix) - n + 1):
                    p = tuple(int(x) for x in prefix[i:i+n])
                    if is_valid_permutation(p,n):
                        prefix_perms.add(hash_permutation(p))
                for i in range(len(suffix) - n + 1):
                    p = tuple(int(x) for x in suffix[i:i+n])
                    if is_valid_permutation(p,n):
                        suffix_perms.add(hash_permutation(p))

                for other_perm_hash in prefix_perms:
                    if other_perm_hash != perm_hash:
                        eput[perm_hash].neighbors.add(other_perm_hash)
                        #Add to layout memory:
                        kmer1 = "".join(str(x) for x in unhash_permutation(other_perm_hash, n)[-k:])
                        kmer2 = perm_string[:k]
                        layout_memory.update_distances(kmer1,kmer2, len(prefix) - i - k, "n8_completion")
                for other_perm_hash in suffix_perms:
                    if other_perm_hash != perm_hash:
                        eput[perm_hash].neighbors.add(other_perm_hash)
                        kmer1 = perm_string[-k:]
                        kmer2 = "".join(str(x) for x in unhash_permutation(other_perm_hash, n)[:k])
                        layout_memory.update_distances(kmer1, kmer2, 1, "n8_completion")
            #Update Prodigals:
            new_prodigals = analysis_scripts.find_prodigal_results("".join(working_superpermutation), n, min_length=PRODIGAL_MIN_LENGTH, overlap_threshold=PRODIGAL_OVERLAP_THRESHOLD)
            for prodigal_seq in new_prodigals:
                is_new = True
                for existing_prodigal_id, existing_prodigal in prodigal_results.items():
                    if prodigal_seq in existing_prodigal.sequence:
                        is_new = False
                        break
                if is_new:
                    new_id = len(prodigal_results) + 1
                    prodigal_results[new_id] = ProdigalResult(prodigal_seq, new_id)
                    #print(f"New Prodigal Result found in completion: {prodigal_seq}")

        #Remove from missing
        missing_permutations.remove(missing_perm_hash)

        #Adjust Overlap Threshold and Length Threshold - Dynamic Adjustment
        missing_count = len(missing_permutations)
        if (missing_count % 500 == 0):
            print(f"Missing {missing_count} permutations")

    return "".join(working_superpermutation)
```

### src/unified/embedder/cand_16_construct_superpermutation_py.py

```python
construct_superpermutation.py

# construct_superpermutation.py
import random
import logging
import json
# From our other files:
from utils import calculate_overlap, hash_permutation, unhash_permutation, is_valid_permutation
from analysis_scripts_final import is_prodigal, identify_anti_prodigals, calculate_winners_losers
from laminate_utils import is_compatible

def generate_completion_candidates(current_superpermutation: str, missing_permutations: set[int],
                                    prodigal_results: dict, winners: dict, losers: dict,
                                    n: int, eput: dict, limbo_list: set, anti_laminates:list) -> set[int]:
    """Generates candidate permutations for the completion phase.
    Prioritizes extending the current superpermutation, and those that use missing permuations.
    """
    candidates = set()
    min_overlap = n - 3  # Consider overlaps of n-1, n-2, and n-3

    # 1. Prioritize permutations that *contain* missing permutations as substrings
    missing_perm_strings = set("".join(str(x) for x in unhash_permutation(h, n)) for h in missing_permutations)
    for missing_perm_str in missing_perm_strings:
        # Find existing permutations that could connect to this missing permutation.
        for prodigal_id in prodigal_results:
            prodigal = prodigal_results[prodigal_id]['sequence']
            # Check overlaps with the *entire* prodigal sequence
            overlap_start = calculate_overlap(prodigal, missing_perm_str)
            overlap_end = calculate_overlap(missing_perm_str, prodigal)

            # Generate extensions, but ONLY if there's sufficient overlap with a prodigal
            if overlap_start >= min_overlap:
                candidates.update(generate_permutations_on_demand(prodigal, missing_perm_str, prodigal_results, winners, losers, n, eput, limbo_list, overlap_start, anti_laminates))
            if overlap_end >= min_overlap:
                candidates.update(generate_permutations_on_demand(missing_perm_str, prodigal, prodigal_results, winners, losers, n, eput, limbo_list, overlap_end, anti_laminates))

    # 2. Extend the ends of the current superpermutation (if not enough candidates)
    if len(candidates) < 100:  #  Adjust threshold as needed
        prefix = current_superpermutation[:n - 1]
        suffix = current_superpermutation[-(n - 1):]
        candidates.update(generate_permutations_on_demand(prefix, suffix, prodigal_results, winners, losers, n, eput, limbo_list, min_overlap, anti_laminates))

    # 3. "Bridge" to the missing permutations (if still not enough candidates)
    if len(candidates) < 100: # Adjust threshold
        for missing_perm_hash in missing_permutations:
            missing_perm = unhash_permutation(missing_perm_hash, n)
            missing_perm_string = "".join(str(x) for x in missing_perm)
            for k in [n - 1, n - 2, n-3]: #Iterate over overlap lengths
                prefix = missing_perm_string[:k]
                suffix = missing_perm_string[-k:]
                candidates.update(generate_permutations_on_demand(prefix, suffix, prodigal_results, winners, losers, n, eput, limbo_list, k, anti_laminates))

    # 4. If *still* not enough candidates, use high-ranking "Winners" (very limited)
    if len(candidates) < 100 and winners:  # Adjust threshold
        top_winners = heapq.nlargest(10, winners.items(), key=lambda item: item[1])  # Limit the number
        for winner, _ in top_winners:
            candidates.update(generate_permutations_on_demand(winner[:n - 1], winner[-(n - 1):], prodigal_results, winners, losers, n, eput, limbo_list, n - 2, anti_laminates))


    #Filter, based on eput and limbolist:
    filtered_candidates = set()
    for perm_hash in candidates:
        if perm_hash not in eput and perm_hash not in limbo_list:
            filtered_candidates.add(perm_hash)

    #Return top results:
    scored_candidates = []
    for cand_hash in filtered_candidates:
        score = calculate_score(current_superpermutation, cand_hash, prodigal_results, winners, losers, LayoutMemory(), n, missing_permutations, eput)
        scored_candidates.append((score, cand_hash))

    best_candidates = heapq.nlargest(100, scored_candidates, key=lambda item: item[0])
    final_candidates = set(x[1] for x in best_candidates)
    return final_candidates
    

def generate_permutations_on_demand(prefix: str, suffix: str, prodigal_results: dict, winners: dict, losers: dict,
                                   n: int, eput: dict, limbo_list: set, min_overlap: int, anti_laminates: list) -> set[int]:
    """Generates permutations on demand, extending a given prefix or suffix.
        Prioritizes "Prodigal" extensions, uses "Winners" and "Losers," and
        avoids already-used permutations and the "Limbo List."  Also uses anti-laminates.

    Args:
        prefix (str): The prefix of the current superpermutation.
        suffix (str): The suffix of the current superpermutation.
        prodigal_results (dict): Dictionary of ProdigalResult objects.
        winners (dict): Dictionary of Winner k-mers and their weights.
        losers (dict): Dictionary of Loser k-mers and their weights.
        n (int): The value of n.
        eput (dict): The Enhanced Permutation Universe Tracker.
        limbo_list (set): The set of permutation hashes to avoid.
        min_overlap (int):  The minimum required overlap.
        anti_laminates (list): List of anti-laminate graphs.

    Returns:
        set[int]: A set of *permutation hashes* that are potential extensions.
    """
    valid_permutations = set()

    # Prioritize Prodigal extensions
    for prodigal_id, prodigal_data in prodigal_results.items():
        prodigal_sequence = prodigal_data['sequence']
        if prefix and prodigal_sequence.startswith(prefix[-min_overlap:]):
            for i in range(len(prodigal_sequence) - (n - 1)):
                perm = tuple(int(x) for x in prodigal_sequence[i:i + n])
                if is_valid_permutation(perm, n):
                    valid_permutations.add(hash_permutation(perm))
        if suffix and prodigal_sequence.endswith(suffix[:min_overlap]):
            for i in range(len(prodigal_sequence) - (n - 1)):
                perm = tuple(int(x) for x in prodigal_sequence[i:i + n])
                if is_valid_permutation(perm, n):
                    valid_permutations.add(hash_permutation(perm))

    # Filter based on Limbo List and Anti-Laminates
    filtered_permutations = set()
    for perm_hash in valid_permutations:
        perm = unhash_permutation(perm_hash, n)
        perm_str = "".join(str(x) for x in perm)
        is_in_limbo = False

        # Basic Loser check (using n-1 and n-2 mers)
        for k in [n - 1, n - 2]:
            for i in range(len(perm_str) - k + 1):
                kmer = perm_str[i:i+k]
                if kmer in limbo_list:
                    is_in_limbo = True
                    break
            if is_in_limbo:
                break
        
        valid_perm = True
        if not is_in_limbo:
          for anti_laminate in anti_laminates:
            if not is_compatible(perm, anti_laminate, n, n-1):
                valid_perm = False
                break
            if not is_compatible(perm, anti_laminate, n, n-2):
                valid_perm = False
                break
        if valid_perm:
            filtered_permutations.add(perm_hash)

    return filtered_permutations


def calculate_score(current_superpermutation, permutation_hash, prodigal_results, winners, losers, layout_memory, n, missing_permutations, eput):
    """Calculates the score for adding a permutation.

    The score combines several factors:
    1. Overlap with the current superpermutation.
    2. Bonus for covering a missing permutation.
    3. Layout score (from LayoutMemory).
    4. Prodigal bonus.
    5. Winner/loser bonus/penalty.
    6. Anti-laminate penalty.
    7. Lookahead bonus.
    """

    permutation = unhash_permutation(permutation_hash, n)
    permutation_string = "".join(str(x) for x in permutation)
    overlap = calculate_overlap(current_superpermutation, permutation_string)

    score = overlap * 5  # Base score (adjust weight)

    if permutation_hash in missing_permutations:
        score += 1000  # Bonus for covering missing permutation (adjust weight)

    # Layout score
    k = n - 1
    if current_superpermutation: # Make sure it is not empty
        last_k_minus_1 = tuple(int(digit) for digit in current_superpermutation[-k:])
        kmer2 = tuple(int(digit) for digit in permutation_string[:k])
        score += layout_memory.get_layout_score(last_k_minus_1, kmer2) * 2  # Adjust weight

    # Prodigal bonus (if applicable)
    if str(permutation_hash) in prodigal_results:
        score += 500  # Adjust weight

    # Winner/loser bonus/penalty
    score += winners.get("".join(map(str,permutation)), 0)  # Using joined string for key
    score -= losers.get("".join(map(str,permutation)), 0)

    # Anti-laminate penalty (Not implemented yet)
    # score -= anti_laminate_penalty(permutation, anti_laminates)

    # Lookahead bonus (simplified)
    lookahead_bonus = 0
    for i in range(1, n + 1):
      lookahead_perm = permutation + (i,)
      if is_valid_permutation(lookahead_perm, n + 1):
        lookahead_kmer = "".join(map(str,lookahead_perm[-(n-1):]))
        lookahead_bonus += winners.get(lookahead_kmer,0) # Add winner bonus if its a winner.
    score += lookahead_bonus * 0.5 #Smaller weight.

    return score
```

### src/unified/embedder/cand_17_construct_superpermutation_py_superpermutation_generator_py.py

```python
# construct_superpermutation.py
import random
import logging
import json
from utils import calculate_overlap, hash_permutation, unhash_permutation, kmer_to_int, int_to_kmer, is_valid_permutation
from layout_memory import LayoutMemory
from analysis_scripts_final import is_prodigal, generate_hypothetical_prodigals, generate_permutations

DATA_FILENAME = "superpermutation_data.json"

def construct_superpermutation(n, initial_sequence="", prodigal_results=None, winners=None, losers=None, layout_memory=None, limbo_list=None, eput=None):
    """Constructs a superpermutation using a dynamic prodigal approach.

    This function implements the core logic for building the superpermutation.  It manages the search process,
    candidate generation, scoring, and data persistence.
    """

    if layout_memory is None:
        layout_memory = LayoutMemory()
    if limbo_list is None:
        limbo_list = set()
    if prodigal_results is None:
        prodigal_results = {}
    if winners is None:
        winners = {}
    if losers is None:
        losers = {}
    if eput is None:
        eput = {}

    superpermutation = list(initial_sequence)  # Start with the initial sequence (if provided)

    try:
        with open(DATA_FILENAME, 'r') as f:
            data = json.load(f)
            prodigal_results = data.get("prodigal_results", {})
            winners = data.get("winners", {})
            losers = data.get("losers", {})
            limbo_list = set(data.get("limbo_list", []))  # Load as a list, then convert to set
            layout_memory.memory = data.get("layout_memory", {})
            eput = data.get("eput", {})
    except FileNotFoundError:
        data = {}  # Create a new data dictionary if the file doesn't exist

    all_permutations = generate_permutations(n)  # Generate all permutations once and store them
    missing_permutations = set(hash_permutation(p) for p in all_permutations) - set(eput.keys())  # Initialize missing permutations

    while missing_permutations:  # Continue until all permutations are covered
        best_candidate = None
        best_score = -float('inf')  # Initialize best score to negative infinity

        candidates = generate_candidates(superpermutation, missing_permutations, prodigal_results, winners, losers, layout_memory, n, eput, limbo_list)

        for candidate_hash in candidates:
            candidate_perm = unhash_permutation(candidate_hash, n)  # Convert hash back to permutation tuple
            candidate_str = "".join(str(x) for x in candidate_perm)  # Convert permutation to string

            for i in range(len(superpermutation) + 1):  # Iterate through possible insertion points
                overlap = calculate_overlap("".join(superpermutation[max(0, i - (n - 1)):i]), candidate_str)  # Calculate overlap
                score = calculate_score("".join(superpermutation), candidate_hash, prodigal_results, winners, losers, layout_memory, n, missing_permutations, eput)  # Calculate score

                if score > best_score:  # Update best candidate if current score is better
                    best_score = score
                    best_candidate = candidate_str
                    best_insertion_point = i

        if best_candidate:  # If a best candidate was found
            superpermutation = superpermutation[:best_insertion_point] + list(best_candidate) + superpermutation[best_insertion_point:]  # Insert the best candidate

            # Update ePUT, layout memory, missing_permutations
            candidate_perm = tuple(int(x) for x in best_candidate)
            candidate_hash = hash_permutation(candidate_perm)
            if candidate_hash not in eput: # Check if it is already initialized before initializing
                eput[candidate_hash] = {"used_count": 0, "used_in_final": False, "neighbors": set()} #Initialize properly
            eput[candidate_hash]["used_count"] += 1
            eput[candidate_hash]["used_in_final"] = True

            # ... (Neighbor updating, as in your original code) ...

            layout_memory.add_sequence("".join(superpermutation), n, n-1, "main_loop")  # Add the updated superpermutation to layout memory

            missing_permutations.discard(candidate_hash)  # Remove the covered permutation from missing permutations

            # Prodigal and hypothetical prodigal handling
            if is_prodigal(candidate_perm, all_permutations, n):
                prodigal_results[hash_permutation(candidate_perm)] = True  # Add to prodigal results

            hypothetical_prodigals = generate_hypothetical_prodigals(prodigal_results, winners, losers, n)
            # ... (Use hypothetical_prodigals in candidate generation)

            # Save data after each iteration
            data["prodigal_results"] = prodigal_results
            data["winners"] = winners
            data["losers"] = losers
            data["limbo_list"] = list(limbo_list)  # Convert set to list for JSON serialization
            data["layout_memory"] = layout_memory.memory
            data["eput"] = eput
            with open(DATA_FILENAME, 'w') as f:
                json.dump(data, f)  # Save the data to the JSON file

        else:
            break  # If no suitable candidate is found, exit the loop

    return "".join(superpermutation)
	
	#This part of construct_superpermutation.py contains the main loop of the superpermutation generation algorithm. It handles data loading and saving, iterates through missing permutations, selects the best candidate, updates data structures, and manages prodigal sequences.  It interacts with the generate_candidates and calculate_score functions (which are defined in the next chunk) to perform the core search and evaluation.  It also uses the utility functions from utils.py and the LayoutMemory class.
    
    # construct_superpermutation.py (Continued)

def generate_candidates(superpermutation, missing_permutations, prodigal_results, winners, losers, layout_memory, n, eput, limbo_list):
    """Generates candidate permutations.

    This function combines several strategies to generate promising candidates:
    1. Candidates from missing permutations (prioritized).
    2. Candidates extending known prodigal results.
    3. Candidates connecting to "winners" (good sequences).
    """

    candidates = set()
    k = n - 1  # k-mer length

    # 1. Candidates from missing permutations (prioritized)
    for missing_perm_hash in missing_permutations:
        missing_perm = unhash_permutation(missing_perm_hash, n)

        # Generate candidates by extending missing permutations
        for i in range(1, n + 1):  # Try adding each digit at the end
            candidate_perm = missing_perm + (i,)
            if is_valid_permutation(candidate_perm, n):
                candidate_hash = hash_permutation(candidate_perm)
                if candidate_hash not in eput and candidate_hash not in limbo_list:
                    candidates.add(candidate_hash)

        # Generate candidates by prepending to missing permutations
        for i in range(1, n + 1):  # Try adding each digit at the beginning
            candidate_perm = (i,) + missing_perm
            if is_valid_permutation(candidate_perm, n):
                candidate_hash = hash_permutation(candidate_perm)
                if candidate_hash not in eput and candidate_hash not in limbo_list:
                    candidates.add(candidate_hash)

        #Generate candidates by combining with existing superpermutation
        if superpermutation:
            last_k_minus_1 = tuple(int(digit) for digit in superpermutation[-(k):]) # Last k-1 digits
            for i in range(1, n + 1):
                candidate_perm = last_k_minus_1 + (i,)
                if is_valid_permutation(candidate_perm, n):
                    candidate_hash = hash_permutation(candidate_perm)
                    if candidate_hash not in eput and candidate_hash not in limbo_list:
                        candidates.add(candidate_hash)

    # 2. Candidates extending known prodigal results
    for prodigal_hash in prodigal_results:
        prodigal = unhash_permutation(prodigal_hash, n)
        for i in range(1, n + 1):
            candidate_perm = prodigal + (i,)
            if is_valid_permutation(candidate_perm, n):
                candidate_hash = hash_permutation(candidate_perm)
                if candidate_hash not in eput and candidate_hash not in limbo_list:
                    candidates.add(candidate_hash)


    # 3. Candidates connecting to "winners" (good sequences)
    for winner_hash in winners:
        winner = unhash_permutation(winner_hash, n)
        # ... (Add logic here to generate candidates connecting to winners)

    return candidates


def calculate_score(current_superpermutation, permutation_hash, prodigal_results, winners, losers, layout_memory, n, missing_permutations, eput):
    """Calculates the score for adding a permutation.

    The score combines several factors:
    1. Overlap with the current superpermutation.
    2. Bonus for covering a missing permutation.
    3. Layout score (from LayoutMemory).
    4. Prodigal bonus.
    5. Winner/loser bonus/penalty.
    """

    permutation = unhash_permutation(permutation_hash, n)
    permutation_string = "".join(str(x) for x in permutation)
    overlap = calculate_overlap(current_superpermutation, permutation_string)

    score = overlap * 5  # Base score (adjust weight)

    if permutation_hash in missing_permutations:
        score += 1000  # Bonus for covering missing permutation (adjust weight)

    # Layout score
    k = n - 1
    if current_superpermutation: # Make sure it is not empty
        last_k_minus_1 = tuple(int(digit) for digit in current_superpermutation[-k:])
        kmer2 = tuple(int(digit) for digit in permutation_string[:k])
        score += layout_memory.get_layout_score(last_k_minus_1, kmer2) * 2  # Adjust weight

    # Prodigal bonus (if applicable)
    if permutation_hash in prodigal_results:
        score += 500  # Adjust weight

    # Winner/loser bonus/penalty (if applicable)
    if permutation_hash in winners:
        score += 200 # Adjust weight
    elif permutation_hash in losers:
        score -= 100 # Adjust weight

    return score


# superpermutation_generator.py
import logging
from utils import setup_logging, normalize_sequence, compute_checksum
from construct_superpermutation import construct_superpermutation
from analysis_scripts_final import generate_permutations  # Import generate_permutations
from layout_memory import LayoutMemory

def main():
    setup_logging()
    n = 7  # or 8
    seed = 42
    layout_memory = LayoutMemory()

    all_permutations = generate_permutations(n)  # Generate all permutations ONCE and store them. This is more efficient.

    superpermutation = construct_superpermutation(n, initial_sequence="", prodigal_results={}, winners={}, losers={}, layout_memory=layout_memory, limbo_list=set(), eput={})

    if superpermutation:
        normalized_sequence = normalize_sequence(superpermutation)
        # ... (Verification and output - will be added later)

    else:
        print("Superpermutation generation failed.")

if __name__ == "__main__":
    main()
```

### src/unified/embedder/cand_18_generation_code_n7_dynamic_py.py

```python
# generation_code_n7_dynamic.py: Generates distinct minimal superpermutations for n=7

import itertools
import random
import math
import time
import networkx as nx
from collections import deque, defaultdict
import heapq
#from layout_memory import LayoutMemory  # Assuming layout_memory.py is in the same directory #REMOVED, using in main
#import analysis_scripts  # Import the analysis functions #REMOVED, using in main

# --- Constants ---
N = 7  # The value of n (number of symbols).
PRODIGAL_OVERLAP_THRESHOLD = 0.98  # Initial minimum overlap rate for "Prodigal Results"
PRODIGAL_MIN_LENGTH = 10  # Initial minimum length for "Prodigal Results"
HYPOTHETICAL_PRODIGAL_OVERLAP_THRESHOLD = 0.95
HYPOTHETICAL_PRODIGAL_MIN_LENGTH = 7
HYPOTHETICAL_PRODIGAL_GENERATION_COUNT = 50
WINNER_THRESHOLD = 0.75  # Not directly used in scoring, but for analysis
LOSER_THRESHOLD = 0.25  # Not directly used in scoring, but for analysis
#INITIAL_SAMPLE_SIZE =  Removed for Dynamic
NUM_ITERATIONS = 1000  # Number of iterations.  Adjust as needed.
#SUPER_BATCH_SIZE =  Removed
#BATCH_SIZE =  Removed
LAYOUT_K_VALUES = [N - 1, N - 2]  # k values for Layout Memory
DE_BRUIJN_K_VALUES = [N - 1, N - 2]  # k values for De Bruijn graph generation
#COMPLETION_SAMPLE_SIZE =  Removed
RANDOM_SEED = 42  # For reproducibility

# --- Helper Functions ---

def calculate_overlap(s1: str, s2: str) -> int:
    """Calculates the maximum overlap between two strings."""
    max_overlap = 0
    for i in range(1, min(len(s1), len(s2)) + 1):
        if s1[-i:] == s2[:i]:
            max_overlap = i
    return max_overlap

def generate_permutations(n: int) -> list[tuple[int, ...]]:
    """Generates all permutations of 1 to n."""
    return list(itertools.permutations(range(1, n + 1)))

def calculate_distance(p1: str, p2: str, n: int) -> int:
    """Calculates distance (n-1 - overlap)."""
    return (n - 1) - max(calculate_overlap(p1, p2), calculate_overlap(p2, p1))

def hash_permutation(permutation: tuple) -> int:
    """Hashes a permutation tuple to a unique integer."""
    result = 0
    n = len(permutation)
    for i, val in enumerate(permutation):
        result += val * (n ** (n - 1 - i))
    return result

def unhash_permutation(hash_value: int, n: int) -> tuple:
    """Converts a hash value back to a permutation tuple."""
    permutation = []
    for i in range(n - 1, -1, -1):
        val = hash_value // (n ** i)
        permutation.append(val + 1)  # Adjust to be 1-indexed
        hash_value -= val * (n ** i)
    return tuple(permutation)

def is_valid_permutation(perm: tuple, n: int) -> bool:
    """Checks if a given sequence is a valid permutation."""
    return len(set(perm)) == n and min(perm) == 1 and max(perm) == n

def calculate_golden_ratio_points(length: int, levels: int = 1) -> list[int]:
    """Calculates multiple levels of golden ratio points."""
    phi = (1 + math.sqrt(5)) / 2
    points = []
    for _ in range(levels):
        new_points = []
        if not points:
            new_points = [int(length / phi), int(length - length / phi)]
        else:
            for p in points:
                new_points.extend([int(p / phi), int(p - p / phi)])
                new_points.extend([int((length-p) / phi) + p, int(length - (length-p) / phi)])
        points.extend(new_points)
        points = sorted(list(set(points)))  # Remove duplicates and sort
    return points

def get_kmers(sequence: str, n: int, k: int) -> set[str]:
    """Extracts all k-mers from a sequence, given n and k."""
    kmers = set()
    seq_list = [int(x) for x in sequence]
    for i in range(len(sequence) - n + 1):
        perm = tuple(seq_list[i:i+n])
        if is_valid_permutation(perm, n):
            if i >= k:
                kmer = "".join(str(x) for x in seq_list[i-k:i])
                kmers.add(kmer)
    return kmers

def build_debruijn_graph(n: int, k: int, permutations=None, superpermutation=None) -> nx.DiGraph:
    """Builds a De Bruijn graph of order k for permutations of n symbols."""

    if (permutations is None and superpermutation is None) or (permutations is not None and superpermutation is not None):
        raise ValueError("Must provide either 'permutations' or 'superpermutation', but not both.")

    graph = nx.DiGraph()

    if permutations:
      for perm in permutations:
          perm_str = "".join(str(x) for x in perm)
          for i in range(len(perm_str) - k + 1):
              kmer1 = perm_str[i:i + k - 1]
              kmer2 = perm_str[i + 1:i + k]
              if len(kmer1) == k - 1 and len(kmer2) == k-1: #Should always be true
                graph.add_edge(kmer1, kmer2, weight=calculate_overlap(kmer1, kmer2))
    else: #Use superpermutation
        s_list = [int(x) for x in superpermutation]
        for i in range(len(s_list) - n + 1):
            perm = tuple(s_list[i:i+n])
            if is_valid_permutation(perm, n):
                #Valid permutation.
                if i >= k:
                  kmer1 = "".join(str(x) for x in s_list[i-k:i])
                  kmer2 = "".join(str(x) for x in s_list[i-k+1: i+1])
                  if len(kmer1) == k - 1 and len(kmer2) == k - 1:
                    graph.add_edge(kmer1, kmer2, weight = calculate_overlap(kmer1,kmer2))
    return graph

def add_weights_to_debruijn(graph: nx.DiGraph, winners: dict, losers: dict):
    """Adds 'winner_weight' and 'loser_weight' attributes to the edges of a De Bruijn graph."""

    for u, v, data in graph.edges(data=True):
        kmer = u[1:] + v[-1]  # Reconstruct the k-mer from the edge
        data['winner_weight'] = winners.get(kmer, 0)
        data['loser_weight'] = losers.get(kmer, 0)

def find_cycles(graph: nx.DiGraph) -> list[list[str]]:
    """Finds cycles in the De Bruijn graph using a simple DFS approach."""
    cycles = []
    visited = set()

    def dfs(node, path):
        visited.add(node)
        path.append(node)

        for neighbor in graph.neighbors(node):
            if neighbor == path[0] and len(path) > 1:  # Found a cycle
                cycles.append(path.copy())
            elif neighbor not in visited:
                dfs(neighbor, path)

        path.pop()
        visited.remove(node) #Correctly remove node

    for node in graph.nodes:
        if node not in visited:
            dfs(node, [])
    return cycles

def find_high_weight_paths(graph: nx.DiGraph, start_node: str, length_limit: int) -> list[list[str]]:
    """
    Finds high-weight paths in the De Bruijn graph starting from a given node.
    """
    best_paths = []
    best_score = -float('inf')

    def dfs(current_node, current_path, current_score):
        nonlocal best_score
        nonlocal best_paths

        if len(current_path) > length_limit:
            return

        current_score_adj = current_score
        if len(current_path) > 1:
            current_score_adj = current_score / (len(current_path)-1) #Average

        if current_score_adj > best_score:
            best_score = current_score_adj
            best_paths = [current_path.copy()]  # New best path
        elif current_score_adj == best_score and len(current_path) > 1: #Dont include starting nodes
            best_paths.append(current_path.copy())  # Add to list of best paths

        for neighbor in graph.neighbors(current_node):
            edge_data = graph.get_edge_data(current_node, neighbor)
            new_score = current_score + edge_data.get('weight', 0) + edge_data.get('winner_weight', 0) - edge_data.get('loser_weight', 0)
            dfs(neighbor, current_path + [neighbor], new_score)

    dfs(start_node, [start_node], 0)
    return best_paths
    
    # START SECTION: Data Structures

class PermutationData:
    def __init__(self, permutation: tuple, in_sample: bool = False, creation_method: str = ""):
        """
        Stores data associated with a single permutation.

        Args:
            permutation (tuple): The permutation as a tuple of integers (1-indexed).
            in_sample (bool): True if the permutation was part of the initial sample
                             (always False in the fully dynamic version).
            creation_method (str):  Describes how the permutation was generated
                                   (e.g., "prodigal_extension", "hypothetical_prodigal",
                                    "completion").
        """
        self.hash: int = hash_permutation(permutation)
        self.permutation: tuple = permutation
        self.in_sample: bool = in_sample  # Will likely always be False in this version
        self.used_count: int = 0  # How many times this permutation has been used
        self.prodigal_status: list[int] = []  # List of ProdigalResult IDs it belongs to
        self.creation_method: str = creation_method
        self.batch_ids: list[int] = []  # Removed
        self.used_in_final: bool = False  # True if in the current best superpermutation
        self.neighbors: set[int] = set()  # Set of hashes of neighboring permutations

    def __str__(self) -> str:
        """String representation for debugging."""
        return (f"PermutationData(hash={self.hash}, permutation={self.permutation}, used_count={self.used_count}, "
                f"prodigal_status={self.prodigal_status}, creation_method={self.creation_method})")

    def __repr__(self) -> str:
        return self.__str__()


class ProdigalResult:
    def __init__(self, sequence: str, result_id: int):
        """
        Represents a "Prodigal Result" - a highly efficient subsequence.

        Args:
            sequence (str): The superpermutation sequence (as a string of digits).
            result_id (int): A unique ID for this "Prodigal Result."
        """
        self.id: int = result_id
        self.sequence: str = sequence
        self.length: int = len(sequence)
        self.permutations: set[int] = set()  # Store hashes of permutations
        self.calculate_permutations()  # Calculate on creation
        self.overlap_rate: float = self.calculate_overlap_rate()

    def calculate_permutations(self):
        """Calculates and stores the set of permutations contained in the sequence."""
        n = N  # Use the global N value
        for i in range(len(self.sequence) - n + 1):
            perm = tuple(int(x) for x in self.sequence[i:i + n])
            if is_valid_permutation(perm, n):
                self.permutations.add(hash_permutation(perm))

    def calculate_overlap_rate(self) -> float:
        """Calculates the overlap rate of the sequence."""
        n = N  # Use the global N value
        total_length = sum(len(str(p)) for p in [unhash_permutation(x,n) for x in self.permutations])
        overlap_length = total_length - len(self.sequence)
        max_possible_overlap = (len(self.permutations) - 1) * (n - 1)
        if max_possible_overlap == 0:
            return 0  # Avoid division by zero
        return overlap_length / max_possible_overlap

    def __str__(self) -> str:
        """String representation for debugging."""
        return (f"ProdigalResult(id={self.id}, length={self.length}, "
                f"overlap_rate={self.overlap_rate:.4f}, num_permutations={len(self.permutations)})")

    def __repr__(self) -> str:
        return self.__str__()

# END SECTION: Data Structures

# START SECTION: Initialization and On-Demand Generation
def initialize_data(initial_n7_prodigals: list[str], initial_winners: dict, initial_losers: dict) -> tuple:
    """Initializes the data structures for the algorithm.
        Modified for n=7, no initial superpermutation, loads initial prodigals.
    Args:
        initial_n7_prodigals (list[str]): A list of initial n=7 Prodigal Result strings.
        initial_winners (dict):  Initial Winner k-mers and weights.
        initial_losers (dict): Initial Loser k-mers and weights.

    Returns:
        tuple: A tuple containing the initialized data structures:
               (prodigal_results, winners, losers, meta_hierarchy, limbo_list, eput, next_prodigal_id)
    """
    prodigal_results = {}  # {prodigal_id: ProdigalResult object}
    winners = initial_winners  # {kmer: weight}
    losers = initial_losers  # {kmer: weight}
    meta_hierarchy = {}  # Track strategy effectiveness
    limbo_list = set()  # Set of permutation hashes.
    eput = {}  # The Enhanced Permutation Universe Tracker

    # Add initial n=7 prodigals
    next_prodigal_id = 0
    for prodigal in initial_n7_prodigals:
        prodigal_results[next_prodigal_id] = ProdigalResult(prodigal, next_prodigal_id)
        next_prodigal_id += 1

    return prodigal_results, winners, losers, meta_hierarchy, limbo_list, eput, next_prodigal_id

def generate_permutations_on_demand_hypothetical(current_sequence, kmer, n, k):
    """Generates candidate permutations for hypothetical prodigals."""
    valid_permutations = set()
    all_perms = generate_permutations(n)
    for perm in all_perms:
        perm_str = "".join(str(x) for x in perm)
        if kmer == perm_str[:k] or kmer == perm_str[-k:]:
           valid_permutations.add(hash_permutation(perm))

    filtered_permutations = set()
    for perm_hash in valid_permutations:
        perm_str = "".join(str(x) for x in unhash_permutation(perm_hash, n))
        is_in_limbo = False

        # Basic Loser check (can be expanded)
        for i in range(len(perm_str) - (n-1) + 1):
            kmer7 = perm_str[i:i+(n-1)]
            if kmer7 in losers and losers[kmer7] > 5:  # Threshold for "Loser"
                is_in_limbo = True
                break
        for i in range(len(perm_str) - (n-2) + 1):
            kmer6 = perm_str[i:i+(n-2)]
            if kmer6 in losers and losers[kmer6] > 5:
                is_in_limbo = True;
                break

        if not is_in_limbo:
            filtered_permutations.add(perm_hash)

    return filtered_permutations

def generate_hypothetical_prodigals(prodigal_results, winners, losers, n, num_to_generate=50, min_length=10, max_length=100):
    """Generates hypothetical prodigal results based on existing data.
    Uses De Bruijn graphs, winners, losers
    """
    hypothetical_prodigals = {}
    next_hypothetical_id = 0

    # 1. Build De Bruijn graphs (k=n-1 and k=n-2)
    combined_prodigal_sequence = "".join([p.sequence for p in prodigal_results.values()])
    graph_k6 = analysis_scripts.build_debruijn_graph(n, 6, superpermutation=combined_prodigal_sequence) #n-1
    graph_k5 = analysis_scripts.build_debruijn_graph(n, 5, superpermutation=combined_prodigal_sequence) #n-2
    analysis_scripts.add_weights_to_debruijn(graph_k6, winners, losers)
    analysis_scripts.add_weights_to_debruijn(graph_k5, winners, losers)

    #Create set of all current prodigal permutations.
    current_prodigals = set()
    for p in prodigal_results:
        current_prodigals.update(prodigal_results[p].permutations)

    for _ in range(num_to_generate):
        # 2. Choose a starting point (preferentially a cycle or high-weight path)
        start_kmer = None
        if random.random() < 0.7:  # 70% chance to start from a cycle in larger k graph
            if graph_k6 and graph_k6.edges:
                cycles = list(nx.simple_cycles(graph_k6))
                if (cycles):
                    cycles.sort(key=lambda x: sum(winners.get("".join(str(y) for y in x[i:i+n-1]),0) for i in range(len(x)-n+2)), reverse = True) #Sort based on winners
                    cycle = random.choice(cycles[:5]) if len(cycles) > 5 else random.choice(cycles) #Choose from top cycles
                    start_kmer = cycle[0] #Just take the first node.

        if not start_kmer: #try from smaller k graph
            if graph_k5 and graph_k5.edges:
                cycles = list(nx.simple_cycles(graph_k5))
                if cycles:
                    cycles.sort(key=lambda x: sum(winners.get("".join(str(y) for y in x[i:i+n-2]),0) for i in range(len(x)-n+3)), reverse = True)
                    cycle = random.choice(cycles[:5]) if len(cycles) > 5 else random.choice(cycles)
                    start_kmer = cycle[0] #Just take the first node.

        if not start_kmer: # Fallback to random kmer from winners.
            if winners:
              top_winners = heapq.nlargest(10, winners.items(), key=lambda item: item[1])
              start_kmer = random.choice(top_winners)[0]
            else:
                continue #Skip if we have no data at all.
        
        # 3. Extend the sequence
        current_sequence = start_kmer
        attempts = 0
        while len(current_sequence) < n * max_length and attempts < 100: # Limit length and attempts
            attempts += 1
            
            #Prioritize Overlap n-1
            candidates = generate_permutations_on_demand_hypothetical(current_sequence, "",  n, n-1)  # Use n-1 mers
            if not candidates:
                candidates = generate_permutations_on_demand_hypothetical(current_sequence, "", n, n-2) # Try with n-2
            if not candidates:
                break
            
            #Choose a random candidate
            best_candidate = unhash_permutation(random.choice(list(candidates)), n)
            overlap = calculate_overlap(current_sequence, "".join(str(x) for x in best_candidate))
            if overlap > 0:
                current_sequence += "".join(str(x) for x in best_candidate)[overlap:]
            else: #Should never happen, due to generate_permutations_on_demand_hypothetical
                break


        # 4. Check against criteria and add if valid
        perms_in_sequence = set()
        for i in range(len(current_sequence) - n + 1):
            perm = tuple(int(x) for x in current_sequence[i:i+n])
            if is_valid_permutation(perm, n):
                perms_in_sequence.add(hash_permutation(perm))
        
        if analysis_scripts.is_prodigal(current_sequence, [unhash_permutation(x,n) for x in perms_in_sequence], n, min_length=min_length, overlap_threshold=HYPOTHETICAL_PRODIGAL_OVERLAP_THRESHOLD) and len(perms_in_sequence) > 0:
            hypothetical_prodigals[next_hypothetical_id] = ProdigalResult(current_sequence, next_hypothetical_id)
            next_hypothetical_id += 1

    return hypothetical_prodigals

def generate_permutations_on_demand(prefix: str, suffix: str, prodigal_results: dict, winners: dict, losers: dict,
                                   n: int, eput: dict, limbo_list: set, min_overlap: int,
                                   hypothetical_prodigals: dict = None) -> set[int]:
    """Generates permutations on demand, extending a given prefix or suffix.
       Prioritizes "Prodigal" extensions, uses "Winners" and "Losers," and
       avoids already-used permutations and the "Limbo List."

    Args:
        prefix (str): The prefix of the current superpermutation.
        suffix (str): The suffix of the current superpermutation.
        prodigal_results (dict): Dictionary of ProdigalResult objects.
        winners (dict): Dictionary of Winner k-mers and their weights.
        losers (dict): Dictionary of Loser k-mers and their weights.
        n (int): The value of n.
        eput (dict): The Enhanced Permutation Universe Tracker.
        limbo_list (set): The set of permutation hashes to avoid.
        min_overlap (int):  The minimum required overlap.
        hypothetical_prodigals (dict): Optional dictionary of Hypothetical Prodigals

    Returns:
        set[int]: A set of *permutation hashes* that are potential extensions.
    """
    valid_permutations = set()

    # Prioritize Hypothetical Prodigal extensions
    if hypothetical_prodigals:
        for prodigal_id, prodigal in hypothetical_prodigals.items():
            if prefix and prodigal.sequence.startswith(prefix[-min_overlap:]):
                for i in range(len(prodigal.sequence) - (n - 1)):
                    perm = tuple(int(x) for x in prodigal.sequence[i:i + n])
                    if is_valid_permutation(perm, n) and hash_permutation(perm) not in eput:
                        valid_permutations.add(hash_permutation(perm))
            if suffix and prodigal.sequence.endswith(suffix[:min_overlap]):
                for i in range(len(prodigal.sequence) - (n - 1)):
                    perm = tuple(int(x) for x in prodigal.sequence[i:i + n])
                    if is_valid_permutation(perm, n) and hash_permutation(perm) not in eput:
                        valid_permutations.add(hash_permutation(perm))

    # Prioritize Prodigal extensions
    for prodigal_id, prodigal in prodigal_results.items():
        if prefix and prodigal.sequence.startswith(prefix[-min_overlap:]):
            for i in range(len(prodigal.sequence) - (n - 1)):
                perm = tuple(int(x) for x in prodigal.sequence[i:i + n])
                if is_valid_permutation(perm, n) and hash_permutation(perm) not in eput:
                    valid_permutations.add(hash_permutation(perm))
        if suffix and prodigal.sequence.endswith(suffix[:min_overlap]):
            for i in range(len(prodigal.sequence) - (n - 1)):
                perm = tuple(int(x) for x in prodigal.sequence[i:i + n])
                if is_valid_permutation(perm, n) and hash_permutation(perm) not in eput:
                    valid_permutations.add(hash_permutation(perm))

    # Filter based on Limbo List and create final set of hashes
    filtered_permutations = set()
    for perm_hash in valid_permutations:
        perm_str = "".join(str(x) for x in unhash_permutation(perm_hash, n))
        is_in_limbo = False

        # Basic Loser check (using 6-mers and 5-mers for n=7)
        for k in [n - 1, n - 2]:  # Check n-1-mers and n-2-mers
            for i in range(len(perm_str) - k + 1):
                kmer = perm_str[i:i+k]
                if kmer in limbo_list:
                    is_in_limbo = True
                    break
            if is_in_limbo:
                break

        if not is_in_limbo:
            filtered_permutations.add(perm_hash)

    return filtered_permutations
    
    # START SECTION: Scoring and Construction

def calculate_score(current_superpermutation: str, permutation_hash: int, prodigal_results: dict,
                    winners: dict, losers: dict, layout_memory: LayoutMemory, n: int,
                    golden_ratio_points: list[int], hypothetical_prodigals: dict) -> float:
    """Calculates the score for adding a permutation to the current superpermutation (n=7 version).

    Args:
        current_superpermutation (str): The current superpermutation string.
        permutation_hash (int): The hash of the candidate permutation.
        prodigal_results (dict): Dictionary of ProdigalResult objects.
        winners (dict): Dictionary of Winner k-mers and their weights.
        losers (dict): Dictionary of Loser k-mers and their weights.
        layout_memory (LayoutMemory): The LayoutMemory object.
        n (int): The value of n (should be 7).
        golden_ratio_points (list): List of golden ratio points.
        hypothetical_prodigals (dict): Dictionary of Hypothetical Prodigals

    Returns:
        float: The score for the candidate permutation. Higher is better.
    """
    permutation = unhash_permutation(permutation_hash, n)
    permutation_string = "".join(str(x) for x in permutation)
    overlap = calculate_overlap(current_superpermutation, permutation_string)
    score = overlap * 5  # Base score based on overlap

    # Prodigal Result Bonus (very high)
    prodigal_bonus = 0
    for prodigal_id, prodigal in prodigal_results.items():
        if permutation_string in prodigal.sequence:
            prodigal_bonus += prodigal.length * 100  # Large bonus
            break  # Only check if contained, not full extension
            
    # Hypothetical prodigal bonus
    hypothetical_bonus = 0
    if hypothetical_prodigals:
        for h_prodigal_id, h_prodigal in hypothetical_prodigals.items():
            if permutation_string in h_prodigal.sequence:
                hypothetical_bonus += h_prodigal.length * 25 #Smaller bonus
                break

    # Winner and Loser Bonus/Penalty (using Layout Memory, reduced influence)
    k_values = [n - 1, n - 2]
    layout_bonus = 0
    for k in k_values:
        if len(current_superpermutation) >= k:
            kmer_end = current_superpermutation[-k:]
            kmer_start = permutation_string[:k]
            layout_bonus += layout_memory.get_layout_score(kmer_end, kmer_start, 1)

    # Golden Ratio Bonus (small, dynamic)
    golden_ratio_bonus = 0
    insertion_point = len(current_superpermutation)
    for point in golden_ratio_points:
        distance = abs(insertion_point - point)
        golden_ratio_bonus += math.exp(-distance / (len(current_superpermutation)/20))  # Smaller divisor = tighter

    # Loser Penalty (Veto)
    loser_penalty = 0
    for k in [6, 5]:  # Check 6-mers and 5-mers for n=7
        for i in range(len(permutation_string) - k + 1):
            kmer = permutation_string[i:i+k]
            loser_penalty += losers.get(kmer, 0) * 5

    score += prodigal_bonus + layout_bonus + golden_ratio_bonus + hypothetical_bonus - loser_penalty

    return score

def construct_superpermutation(initial_permutations: list, prodigal_results: dict, winners: dict, losers: dict,
                              layout_memory: LayoutMemory, meta_hierarchy: dict, limbo_list: set, n: int,
                              hypothetical_prodigals: dict) -> tuple[str, set[int]]:
    """Constructs a superpermutation using the dynamic prodigal approach (n=7 version).

    Args:
        initial_permutations (list):  Empty list, will start with longest prodigal.
        prodigal_results (dict): Dictionary of ProdigalResult objects.
        winners (dict): Dictionary of Winner k-mers and their weights.
        losers (dict): Dictionary of Loser k-mers and their weights.
        layout_memory (LayoutMemory): The LayoutMemory Object.
        meta_hierarchy (dict): Dictionary for tracking strategy effectiveness.
        limbo_list (set): Set of permutation hashes to avoid.
        n (int): The value of n.
        hypothetical_prodigals (dict): "Hypothetical Prodigals" to use.

    Returns:
        tuple: (superpermutation string, set of used permutation hashes)
    """

    superpermutation = ""
    used_permutations = set()

    # Initialize with the longest "Prodigal Result"
    best_prodigal_key = max(prodigal_results, key=lambda k: prodigal_results[k].length)
    superpermutation = prodigal_results[best_prodigal_key].sequence
    used_permutations.update(prodigal_results[best_prodigal_key].permutations)

    golden_ratio_points = calculate_golden_ratio_points(len(superpermutation), levels=3)

    while True:  # Continue until no more additions can be made
        best_candidate = None
        best_score = -float('inf')

        # Find frontier k-mers
        prefix = superpermutation[:n - 1]
        suffix = superpermutation[-(n - 1):]

        # Generate candidates (very small set, focused on the frontier)
        candidates = generate_permutations_on_demand(prefix, suffix, prodigal_results, winners, losers, n, used_permutations, limbo_list, n - 1, hypothetical_prodigals)
        if not candidates:
            candidates = generate_permutations_on_demand(prefix, suffix, prodigal_results, winners, losers, n, used_permutations, limbo_list, n - 2, hypothetical_prodigals)
        if not candidates:
            # print("No candidates found. Stopping.") # For Debugging
            break  # No more candidates can be added

        #Deterministic selection from candidates:
        scored_candidates = []
        for candidate_hash in candidates:
            score = calculate_score(superpermutation, candidate_hash, prodigal_results, winners, losers, layout_memory, n, golden_ratio_points, hypothetical_prodigals)
            scored_candidates.append((score, candidate_hash))
        
        best_candidate = None
        if scored_candidates:
            scored_candidates.sort(reverse=True) #Sort by score
            best_candidate = scored_candidates[0][1] #Take top result.

        if best_candidate is not None:
            best_candidate_perm = unhash_permutation(best_candidate, n)
            overlap = calculate_overlap(superpermutation, "".join(str(x) for x in best_candidate_perm))
            superpermutation += "".join(str(x) for x in best_candidate_perm)[overlap:]  # Add to superpermutation
            used_permutations.add(best_candidate)

            # Update golden ratio points
            golden_ratio_points = calculate_golden_ratio_points(len(superpermutation))

            # Prodigal Result Check
            new_prodigals = analysis_scripts.find_prodigal_results(superpermutation, n, min_length=PRODIGAL_MIN_LENGTH, overlap_threshold=PRODIGAL_OVERLAP_THRESHOLD)
            for prodigal_seq in new_prodigals:
                is_new = True
                for existing_prodigal_id, existing_prodigal in prodigal_results.items():
                    if prodigal_seq in existing_prodigal.sequence:
                        is_new = False
                        break
                if is_new:
                    new_id = len(prodigal_results) + 1
                    prodigal_results[new_id] = ProdigalResult(prodigal_seq, new_id)
                    # print(f"New Prodigal Result found: {prodigal_seq}") # For Debugging

            #Update ePUT
            perm_hash = best_candidate
            if perm_hash not in eput: #Should now always be the case
                eput[perm_hash] = PermutationData(best_candidate_perm, in_sample=False, creation_method="dynamic_generation") # All are now dynamically generated
            eput[perm_hash].used_count += 1
            eput[perm_hash].used_in_final = True
            #Update neighbors in ePUT
            perm_string = "".join(str(x) for x in best_candidate_perm)
            for k in [n-1, n-2]:
                prefix = superpermutation[:k]
                suffix = superpermutation[-k:]
                prefix_perms = set()
                suffix_perms = set()
                for i in range(len(prefix) - n + 1):
                    p = tuple(int(x) for x in prefix[i:i+n])
                    if is_valid_permutation(p,n):
                        prefix_perms.add(hash_permutation(p))
                for i in range(len(suffix) - n + 1):
                    p = tuple(int(x) for x in suffix[i:i+n])
                    if is_valid_permutation(p,n):
                        suffix_perms.add(hash_permutation(p))
                for other_perm_hash in prefix_perms:
                    if other_perm_hash != perm_hash:
                        eput[perm_hash].neighbors.add(other_perm_hash)
                        kmer1 = "".join(str(x) for x in unhash_permutation(other_perm_hash, n)[-k:])
                        kmer2 = perm_string[:k]
                        layout_memory.update_distances(kmer1,kmer2, len(superpermutation) - i - len(prefix), "n7_dynamic")
                for other_perm_hash in suffix_perms:
                    if other_perm_hash != perm_hash:
                        eput[perm_hash].neighbors.add(other_perm_hash)
                        kmer1 = perm_string[-k:]
                        kmer2 = "".join(str(x) for x in unhash_permutation(other_perm_hash, n)[:k])
                        layout_memory.update_distances(kmer1, kmer2, 1, "n7_dynamic")

        else:
            break  # If we get here, we are stuck

    return superpermutation, used_permutations

# END SECTION: Superpermutation Construction

# START SECTION: Main Function and Distinctness Check
def is_cyclically_distinct(s1: str, s2: str) -> bool:
    """Checks if two strings are cyclically distinct.
       Returns True if they are distinct, and False if one is a cyclic shift of the other.
    """
    if len(s1) != len(s2):
        return True  # Different lengths, so they must be distinct

    s1s1 = s1 + s1  # Concatenate s1 with itself
    return s2 not in s1s1

def main():
    """
    Main function to execute the Dynamic Prodigal Assembly algorithm for n=7.
    Generates multiple distinct 5906 superpermutations.
    """
    n = 7  # Set n=7
    num_iterations = 1000  # Set a high number; it will likely stop much earlier

    # --- File Paths (IMPORTANT: Adjust these paths as needed) ---
    initial_winners_losers_n7_file = "initial_winners_losers_n7.txt"
    prodigal_results_n7_file = "prodigal_results_n7.txt"  # Will contain the initial 5906 string
    distinct_superpermutations_file = "distinct_superpermutations_n7.txt" # Output file
    layout_memory_file = "layout_memory_n7.pkl"


    # --- Load Initial Data ---
    initial_winners, initial_losers = {}, {}
    try:
        with open(initial_winners_losers_n7_file, "r") as f:
            for line in f:
                kmer, w_type, weight = line.strip().split(",")
                if w_type == "winner":
                    initial_winners[kmer] = int(weight)
                else:
                    initial_losers[kmer] = int(weight)
    except FileNotFoundError:
        print(f"Initial Winners/Losers file ({initial_winners_losers_n7_file}) not found. Starting with empty.")


    initial_n7_prodigals = []
    try:
        with open(prodigal_results_n7_file, "r") as f:
            initial_n7_prodigals = [line.strip() for line in f]
    except FileNotFoundError:
        print(f"Initial n=7 Prodigal Results file ({prodigal_results_n7_file}) not found.  Starting with empty.")
        #In this case, we will create an empty file
        with open(prodigal_results_n7_file, "w") as f:
            pass


    # --- Initialize Data Structures ---
    prodigal_results, winners, losers, meta_hierarchy, limbo_list, eput, next_prodigal_id = initialize_data(
        "", initial_n7_prodigals, initial_winners, initial_losers
    )  #  Empty string, as we *build* the superpermutation from scratch
    
    #Create Initial Laminate
    laminates = []
    for prodigal in initial_n7_prodigals:
        laminates.append(analysis_scripts.create_laminate(prodigal, n, n-1))
        laminates.append(analysis_scripts.create_laminate(prodigal, n, n-2))
    
    #Load layout memory
    layout_memory = LayoutMemory()
    try:
        layout_memory.load_from_file(layout_memory_file)
        print("Loaded LayoutMemory from file.")
    except FileNotFoundError:
        print("No existing LayoutMemory file found. Starting with a new one.")

    # Load existing distinct superpermutations
    existing_superpermutations = set()
    try:
        with open(distinct_superpermutations_file, "r") as f:
            for line in f:
                existing_superpermutations.add(line.strip())
    except FileNotFoundError:
        print(f"No existing distinct superpermutations file found.")

    distinct_count = 0
    
    for iteration in range(num_iterations):
        print(f"Starting iteration {iteration + 1}...")
        start_time = time.time()

        # 1. Generate Hypothetical Prodigals (now done *before* construction)
        hypothetical_prodigals = analysis_scripts.generate_hypothetical_prodigals(prodigal_results, winners, losers, n)
        print(f"  Generated {len(hypothetical_prodigals)} hypothetical prodigals.")

        # 2. Construct Superpermutation (using the dynamic approach)
        superpermutation, used_permutations = construct_superpermutation([], prodigal_results, winners, losers, layout_memory, meta_hierarchy, limbo_list, n, hypothetical_prodigals)
        print(f"  Superpermutation length: {len(superpermutation)}")

        # 3. Update Data
        analysis_results = analysis_scripts.analyze_superpermutation(superpermutation, n)  # Use analysis script
        print(f"  Valid: {analysis_results['validity']}")
        print(f"  Overlap Distribution: {analysis_results['overlap_distribution']}")

        # Check for 5906 length and distinctness
        if analysis_results['validity'] and len(superpermutation) == 5906:
            is_distinct = True
            for existing_sp in existing_superpermutations:
                if is_cyclically_distinct(superpermutation, existing_sp) == False:
                    is_distinct = False
                    break

            if is_distinct:
                print("  Found a *distinct* 5906 superpermutation!")
                existing_superpermutations.add(superpermutation)
                distinct_count += 1
                with open(distinct_superpermutations_file, "a") as f:
                    f.write(superpermutation + "\n")

                #Create and add new laminate
                new_lam_1 = analysis_scripts.create_laminate(superpermutation, n, n-1)
                new_lam_2 = analysis_scripts.create_laminate(superpermutation, n, n-2)
                laminates.append(new_lam_1)
                laminates.append(new_lam_2)
            else:
                print("  Found a 5906 superpermutation, but it's a duplicate.")
        else:
            print(" Run did not produce a valid minimal superpermutation.")

        # Find and add new prodigal results. Stricter criteria.
        new_prodigals = analysis_scripts.find_prodigal_results(superpermutation, n, min_length=PRODIGAL_MIN_LENGTH, overlap_threshold=PRODIGAL_OVERLAP_THRESHOLD)
        for prodigal_seq in new_prodigals:
            is_new = True
            for existing_prodigal_id, existing_prodigal in prodigal_results.items():
                if prodigal_seq in existing_prodigal.sequence:
                    is_new = False
                    break
            if is_new:
                prodigal_results[next_prodigal_id] = ProdigalResult(prodigal_seq, next_prodigal_id)
                next_prodigal_id += 1

        #Update ePUT
        s_tuple = tuple(int(x) for x in superpermutation)
        for i in range(len(s_tuple) - n + 1):
            perm = s_tuple[i:i+n]
            if is_valid_permutation(perm, n):
                perm_hash = hash_permutation(perm)
                if perm_hash not in eput:
                    eput[perm_hash] = PermutationData(perm, in_sample=False, creation_method="dynamic_generation")
                eput[perm_hash].used_count += 1
                eput[perm_hash].used_in_final = True # All are used.
                #Update neighbors
                if i > 0:
                    prev_perm = s_tuple[i-1:i-1+n]
                    if is_valid_permutation(prev_perm, n):
                        eput[perm_hash].neighbors.add(hash_permutation(prev_perm))
                if i < len(s_tuple) - n:
                    next_perm = s_tuple[i+1:i+1+n]
                    if is_valid_permutation(next_perm, n):
                        eput[perm_hash].neighbors.add(hash_permutation(next_perm))

        # Update "Winners" and "Losers" (using the new superpermutation, and k=6 and k=7)
        new_winners6, new_losers6 = analysis_scripts.calculate_winners_losers([superpermutation], n, k=6)
        new_winners7, new_losers7 = analysis_scripts.calculate_winners_losers([superpermutation], n, k=7)

        for kmer, weight in new_winners6.items():
            winners[kmer] = winners.get(kmer, 0) + weight
        for kmer, weight in new_losers6.items():
            losers[kmer] = losers.get(kmer, 0) + weight
            limbo_list.add(kmer)  # Add to limbo list
        for kmer, weight in new_winners7.items():
            winners[kmer] = winners.get(kmer, 0) + weight
        for kmer, weight in new_losers7.items():
            losers[kmer] = losers.get(kmer, 0) + weight
            limbo_list.add(kmer)
        
        # Update Layout Memory
        layout_memory.add_sequence(superpermutation, n, 6, f"run_{iteration}")
        layout_memory.add_sequence(superpermutation, n, 5, f"run_{iteration}")

        # Update "Meta-Hierarchy" (simplified for this example)
        meta_hierarchy.setdefault("run_lengths", []).append(len(superpermutation))
        meta_hierarchy.setdefault("prodigal_counts", []).append(len(prodigal_results))

        end_time = time.time()
        print(f"  Iteration {iteration + 1} completed in {end_time - start_time:.2f} seconds.")
        print(f"  Distinct 5906 superpermutations found so far: {distinct_count}")

        # Check stopping criteria
        if len(superpermutation) == 5906:
            # Check for early stopping based on lack of new distinct solutions
            is_new_solution = True
            for existing_sp in existing_superpermutations:
                if not is_cyclically_distinct(superpermutation, existing_sp):
                    is_new_solution = False
                    break

            if not is_new_solution:
                no_new_solutions_count = meta_hierarchy.get("no_new_solutions_count", 0) + 1
                meta_hierarchy["no_new_solutions_count"] = no_new_solutions_count
                if no_new_solutions_count >= 20:
                    print("No new distinct 5906 solutions found in 20 iterations. Stopping.")
                    break
            else:
                meta_hierarchy["no_new_solutions_count"] = 0  # Reset counter

if __name__ == "__main__":
    random.seed(RANDOM_SEED)  # Set the random seed for reproducibility
    main()
```

### src/unified/embedder/cand_19_generation_code_n7_dynamic_2_0_py.py

```python
import itertools
import random
import math
import time
import networkx as nx
from collections import deque, defaultdict
import heapq
import analysis_scripts # Now we need to import it here
from layout_memory import LayoutMemory #And here


# --- Constants ---
N = 7  # Set n=7
PRODIGAL_OVERLAP_THRESHOLD = 0.98
PRODIGAL_MIN_LENGTH = 10  # Reduced for n=7
HYPOTHETICAL_PRODIGAL_OVERLAP_THRESHOLD = 0.95
HYPOTHETICAL_PRODIGAL_MIN_LENGTH = 7
HYPOTHETICAL_PRODIGAL_GENERATION_COUNT = 50 #Number of hypos to make
MEGA_HYPOTHETICAL_GENERATION_COUNT = 20 #Number of mega hypos
WINNER_THRESHOLD = 0.75
LOSER_THRESHOLD = 0.25
NUM_ITERATIONS = 10000  #  Adjust as needed.  This is now for testing *finding variations*.
LAYOUT_K_VALUES = [N - 1, N - 2]
DE_BRUIJN_K_VALUES = [N - 1, N - 2]
RANDOM_SEED = 42  #  Start with a seed.  Change this for each run.


# --- Helper Functions ---

def calculate_overlap(s1: str, s2: str) -> int:
    """Calculates the maximum overlap between two strings."""
    max_overlap = 0
    for i in range(1, min(len(s1), len(s2)) + 1):
        if s1[-i:] == s2[:i]:
            max_overlap = i
    return max_overlap

def generate_permutations(n: int) -> list[tuple[int, ...]]:
    """Generates all permutations of 1 to n."""
    return list(itertools.permutations(range(1, n + 1)))

def calculate_distance(p1: str, p2: str, n: int) -> int:
    """Calculates distance (n-1 - overlap)."""
    return (n - 1) - max(calculate_overlap(p1, p2), calculate_overlap(p2, p1))

def hash_permutation(permutation: tuple) -> int:
    """Hashes a permutation tuple to a unique integer."""
    result = 0
    n = len(permutation)
    for i, val in enumerate(permutation):
        result += val * (n ** (n - 1 - i))
    return result

def unhash_permutation(hash_value: int, n: int) -> tuple:
    """Converts a hash value back to a permutation tuple."""
    permutation = []
    for i in range(n - 1, -1, -1):
        val = hash_value // (n ** i)
        permutation.append(val + 1)  # Adjust to be 1-indexed
        hash_value -= val * (n ** i)
    return tuple(permutation)

def is_valid_permutation(perm: tuple, n: int) -> bool:
    """Checks if a given sequence is a valid permutation."""
    return len(set(perm)) == n and min(perm) == 1 and max(perm) == n

def calculate_golden_ratio_points(length: int, levels: int = 1) -> list[int]:
    """Calculates multiple levels of golden ratio points."""
    phi = (1 + math.sqrt(5)) / 2
    points = []
    for _ in range(levels):
        new_points = []
        if not points:
            new_points = [int(length / phi), int(length - length / phi)]
        else:
            for p in points:
                new_points.extend([int(p / phi), int(p - p / phi)])
                new_points.extend([int((length-p) / phi) + p, int(length - (length-p) / phi)])
        points.extend(new_points)
        points = sorted(list(set(points)))  # Remove duplicates and sort
    return points

def get_kmers(sequence: str, n: int, k: int) -> set[str]:
    """Extracts all k-mers from a sequence, given n and k."""
    kmers = set()
    seq_list = [int(x) for x in sequence]
    for i in range(len(sequence) - n + 1):
        perm = tuple(seq_list[i:i+n])
        if is_valid_permutation(perm, n):
            if i >= k:
                kmer = "".join(str(x) for x in seq_list[i-k:i])
                kmers.add(kmer)
    return kmers

def is_cyclically_distinct(s1: str, s2: str) -> bool:
    """Checks if two strings are cyclically distinct."""
    if len(s1) != len(s2):
        return True  # Different lengths, so they must be distinct
    s1s1 = s1 + s1  # Concatenate s1 with itself
    return s2 not in s1s1
    
    # START SECTION: Data Structures

class PermutationData:
    def __init__(self, permutation: tuple, creation_method: str = ""):
        """
        Stores data associated with a single permutation.

        Args:
            permutation (tuple): The permutation as a tuple of integers (1-indexed).
            creation_method (str):  Describes how the permutation was generated
                                   (e.g., "prodigal_extension", "hypothetical_prodigal").
        """
        self.hash: int = hash_permutation(permutation)
        self.permutation: tuple = permutation
        self.in_sample: bool = False #Always false
        self.used_count: int = 0  # How many times this permutation has been used
        self.prodigal_status: list[int] = []  # List of ProdigalResult IDs it belongs to
        self.creation_method: str = creation_method
        self.batch_ids: list[int] = []  # Removed
        self.used_in_final: bool = False  # True if in the current best superpermutation
        self.neighbors: set[int] = set()  # Set of hashes of neighboring permutations

    def __str__(self) -> str:
        """String representation for debugging."""
        return (f"PermutationData(hash={self.hash}, permutation={self.permutation}, used_count={self.used_count}, "
                f"prodigal_status={self.prodigal_status}, creation_method={self.creation_method})")

    def __repr__(self) -> str:
        return self.__str__()


class ProdigalResult:
    def __init__(self, sequence: str, result_id: int):
        """
        Represents a "Prodigal Result" - a highly efficient subsequence.

        Args:
            sequence (str): The superpermutation sequence (as a string of digits).
            result_id (int): A unique ID for this "Prodigal Result."
        """
        self.id: int = result_id
        self.sequence: str = sequence
        self.length: int = len(sequence)
        self.permutations: set[int] = set()  # Store hashes of permutations
        self.calculate_permutations()  # Calculate on creation
        self.overlap_rate: float = self.calculate_overlap_rate()

    def calculate_permutations(self):
        """Calculates and stores the set of permutations contained in the sequence."""
        n = N  # Use the global N value
        for i in range(len(self.sequence) - n + 1):
            perm = tuple(int(x) for x in self.sequence[i:i + n])
            if is_valid_permutation(perm, n):
                self.permutations.add(hash_permutation(perm))

    def calculate_overlap_rate(self) -> float:
        """Calculates the overlap rate of the sequence."""
        n = N  # Use the global N value
        total_length = sum(len(str(p)) for p in [unhash_permutation(x,n) for x in self.permutations])
        overlap_length = total_length - len(self.sequence)
        max_possible_overlap = (len(self.permutations) - 1) * (n - 1)
        if max_possible_overlap == 0:
            return 0  # Avoid division by zero
        return overlap_length / max_possible_overlap

    def __str__(self) -> str:
        """String representation for debugging."""
        return (f"ProdigalResult(id={self.id}, length={self.length}, "
                f"overlap_rate={self.overlap_rate:.4f}, num_permutations={len(self.permutations)})")

    def __repr__(self) -> str:
        return self.__str__()

# END SECTION: Data Structures

# START SECTION: Initialization and On-Demand Generation
def initialize_data(initial_n7_prodigals: list[str], initial_winners: dict, initial_losers: dict) -> tuple:
    """Initializes the data structures for the algorithm.
        Modified for n=7, takes initial prodigals as a list of strings.
    Args:
        initial_n7_prodigals (list[str]): A list of initial n=7 Prodigal Result strings.
        initial_winners (dict):  Initial Winner k-mers and weights.
        initial_losers (dict): Initial Loser k-mers and weights.

    Returns:
        tuple: (prodigal_results, winners, losers, meta_hierarchy, limbo_list, eput, next_prodigal_id)
    """
    prodigal_results = {}  # {prodigal_id: ProdigalResult object}
    winners = initial_winners  # {kmer: weight}
    losers = initial_losers  # {kmer: weight}
    meta_hierarchy = {}  # Track strategy effectiveness
    limbo_list = set()  # Set of permutation hashes.
    eput = {}  # The Enhanced Permutation Universe Tracker

    # Add initial n=7 prodigals
    next_prodigal_id = 0
    for prodigal in initial_n7_prodigals:
        prodigal_results[next_prodigal_id] = ProdigalResult(prodigal, next_prodigal_id)
        next_prodigal_id += 1

    return prodigal_results, winners, losers, meta_hierarchy, limbo_list, eput, next_prodigal_id

def generate_permutations_on_demand(prefix: str, suffix: str, prodigal_results: dict, winners: dict, losers: dict,
                                   n: int, eput: dict, limbo_list: set, min_overlap: int,
                                   hypothetical_prodigals: dict = None, laminate_graphs: list = None) -> set[int]:
    """Generates permutations on demand, extending a given prefix or suffix.
       Prioritizes "Prodigal" extensions, uses "Winners" and "Losers," and
       avoids already-used permutations and the "Limbo List."  Also uses laminate.

    Args:
        prefix (str): The prefix of the current superpermutation.
        suffix (str): The suffix of the current superpermutation.
        prodigal_results (dict): Dictionary of ProdigalResult objects.
        winners (dict): Dictionary of Winner k-mers and their weights.
        losers (dict): Dictionary of Loser k-mers and their weights.
        n (int): The value of n.
        eput (dict): The Enhanced Permutation Universe Tracker.
        limbo_list (set): The set of permutation hashes to avoid.
        min_overlap (int):  The minimum required overlap.
        hypothetical_prodigals (dict): Optional dictionary of Hypothetical Prodigals.
        laminate_graphs(list): List of laminate graphs.

    Returns:
        set[int]: A set of *permutation hashes* that are potential extensions.
    """
    valid_permutations = set()

    # Prioritize Hypothetical Prodigal extensions
    if hypothetical_prodigals:
        for prodigal_id, prodigal in hypothetical_prodigals.items():
            if prefix and prodigal.sequence.startswith(prefix[-min_overlap:]):
                for i in range(len(prodigal.sequence) - (n - 1)):
                    perm = tuple(int(x) for x in prodigal.sequence[i:i + n])
                    if analysis_scripts.is_valid_permutation(perm, n) and hash_permutation(perm) not in eput:
                        valid_permutations.add(hash_permutation(perm))
            if suffix and prodigal.sequence.endswith(suffix[:min_overlap]):
                for i in range(len(prodigal.sequence) - (n - 1)):
                    perm = tuple(int(x) for x in prodigal.sequence[i:i + n])
                    if analysis_scripts.is_valid_permutation(perm, n) and hash_permutation(perm) not in eput:
                        valid_permutations.add(hash_permutation(perm))

    # Prioritize Prodigal extensions
    for prodigal_id, prodigal in prodigal_results.items():
        if prefix and prodigal.sequence.startswith(prefix[-min_overlap:]):
            for i in range(len(prodigal.sequence) - (n - 1)):
                perm = tuple(int(x) for x in prodigal.sequence[i:i + n])
                if analysis_scripts.is_valid_permutation(perm, n) and hash_permutation(perm) not in eput:
                    valid_permutations.add(hash_permutation(perm))
        if suffix and prodigal.sequence.endswith(suffix[:min_overlap]):
            for i in range(len(prodigal.sequence) - (n - 1)):
                perm = tuple(int(x) for x in prodigal.sequence[i:i + n])
                if analysis_scripts.is_valid_permutation(perm, n) and hash_permutation(perm) not in eput:
                    valid_permutations.add(hash_permutation(perm))
    
    # Filter based on Limbo List, and laminate
    filtered_permutations = set()
    for perm_hash in valid_permutations:
        perm = unhash_permutation(perm_hash, n)
        perm_str = "".join(str(x) for x in perm)
        is_in_limbo = False

        # Basic Loser check (using 5-mers and 6-mers for n=7)
        for k in [6, 5]:
            for i in range(len(perm_str) - k + 1):
                kmer = perm_str[i:i+k]
                if kmer in limbo_list:
                    is_in_limbo = True
                    break
            if is_in_limbo:
                break

        if not is_in_limbo:
            #Laminate Check
            valid_perm = False
            for laminate in laminate_graphs:
                if analysis_scripts.is_compatible(perm, laminate, n, n-1):
                    valid_perm = True
                    break
                elif analysis_scripts.is_compatible(perm, laminate, n, n-2):
                    valid_perm = True
                    break
            if valid_perm:
                filtered_permutations.add(perm_hash)


    return filtered_permutations
    
    # START SECTION: Scoring and Construction

def calculate_score(current_superpermutation: str, permutation_hash: int, prodigal_results: dict,
                    winners: dict, losers: dict, layout_memory: LayoutMemory, n: int,
                    golden_ratio_points: list[int], hypothetical_prodigals: dict = None) -> float:
    """Calculates the score for adding a permutation to the current superpermutation (n=7 version).

    Args:
        current_superpermutation (str): The current superpermutation string.
        permutation_hash (int): The hash of the candidate permutation.
        prodigal_results (dict): Dictionary of ProdigalResult objects.
        winners (dict): Dictionary of Winner k-mers and their weights (from WL_PUT).
        losers (dict): Dictionary of Loser k-mers and their weights (from WL_PUT).
        layout_memory (LayoutMemory): The LayoutMemory object.
        n (int): The value of n (should be 7).
        golden_ratio_points (list): List of golden ratio points.
        hypothetical_prodigals (dict): Dictionary of Hypothetical Prodigals.

    Returns:
        float: The score for the candidate permutation. Higher is better.
    """
    permutation = unhash_permutation(permutation_hash, n)
    permutation_string = "".join(str(x) for x in permutation)
    overlap = calculate_overlap(current_superpermutation, permutation_string)
    score = overlap * 5  # Base score based on overlap

    # Prodigal Result Bonus (very high)
    prodigal_bonus = 0
    for prodigal_id, prodigal in prodigal_results.items():
        if permutation_string in prodigal.sequence:
            prodigal_bonus += prodigal.length * 100  # Large bonus
            break  # Only check if contained, not full extension
            
    #Hypothetical Bonus
    hypothetical_bonus = 0
    if hypothetical_prodigals:
        for h_prodigal_id, h_prodigal in hypothetical_prodigals.items():
            if permutation_string in h_prodigal.sequence:
                hypothetical_bonus +=  h_prodigal.length * 25

    # Winner and Loser Bonus/Penalty (using Layout Memory)
    k_values = [n - 1, n - 2]
    layout_bonus = 0
    for k in k_values:
        if len(current_superpermutation) >= k:
            kmer_end = current_superpermutation[-k:]
            kmer_start = permutation_string[:k]
            layout_bonus += layout_memory.get_layout_score(kmer_end, kmer_start, 1) # Check for adjacency

    # Golden Ratio Bonus (small, dynamic)
    golden_ratio_bonus = 0
    insertion_point = len(current_superpermutation)
    for point in golden_ratio_points:
        distance = abs(insertion_point - point)
        golden_ratio_bonus += math.exp(-distance / (len(current_superpermutation)/20))  # Smaller divisor = tighter to golden ratio

    # Loser Penalty (Veto)
    loser_penalty = 0
    for k in [6, 5]:  # Check 6-mers and 5-mers for n=7
        for i in range(len(permutation_string) - k + 1):
            kmer = permutation_string[i:i+k]
            loser_penalty += losers.get(kmer, 0) * 5

    # Higher-Order Winners/Losers
    higher_order_bonus = 0
    for seq_length in [2, 3]:  # Check sequences of length 2 and 3
      if len(current_superpermutation) >= (n * seq_length):
        prev_seq = current_superpermutation[-(n*seq_length):]
        prev_perms = []
        for i in range(len(prev_seq) - n + 1):
            pp = tuple(int(x) for x in prev_seq[i:i+n])
            if is_valid_permutation(pp, n):
                prev_perms.append(hash_permutation(pp))
        if len(prev_perms) >= (seq_length -1):
            current_seq = tuple(prev_perms[-(seq_length - 1):] + [permutation_hash])
            current_seq_hash = hash(current_seq)
            higher_order_bonus += winners.get(current_seq_hash, 0) * 5
            loser_penalty += losers.get(current_seq_hash, 0) * 5


    score += prodigal_bonus + layout_bonus + golden_ratio_bonus + hypothetical_bonus - loser_penalty + higher_order_bonus

    return score

def construct_superpermutation(initial_permutations: list, prodigal_results: dict, winners: dict, losers: dict,
                              layout_memory: LayoutMemory, meta_hierarchy: dict, limbo_list: set, n: int,
                              hypothetical_prodigals: dict) -> tuple[str, set[int]]:
    """Constructs a superpermutation using the dynamic prodigal approach (n=7 version).

    Args:
        initial_permutations (list): Initially empty, will start with longest prodigal.
        prodigal_results (dict): Dictionary of ProdigalResult objects.
        winners (dict): Dictionary of Winner k-mers and their weights.
        losers (dict): Dictionary of Loser k-mers and their weights.
        layout_memory (LayoutMemory): The LayoutMemory object.
        meta_hierarchy (dict): Dictionary for tracking strategy effectiveness.
        limbo_list (set): Set of permutation hashes to avoid.
        n (int): The value of n.
        hypothetical_prodigals (dict): "Hypothetical Prodigals" to use.

    Returns:
        tuple: (superpermutation string, set of used permutation hashes)
    """

    superpermutation = ""
    used_permutations = set()

    # Initialize with the longest "Prodigal Result" if possible
    if prodigal_results:
        best_prodigal_key = max(prodigal_results, key=lambda k: prodigal_results[k].length)
        superpermutation = prodigal_results[best_prodigal_key].sequence
        used_permutations.update(prodigal_results[best_prodigal_key].permutations)
    else:
        # Fallback to a valid permutation if no prodigals.
        superpermutation = "".join(str(x) for x in range(1, n + 1))
        used_permutations.add(hash_permutation(tuple(range(1, n + 1))))

    golden_ratio_points = calculate_golden_ratio_points(len(superpermutation), levels=3)
    laminates = [analysis_scripts.create_laminate(prodigal_results[key].sequence, n, k) for key in prodigal_results for k in [n-1,n-2]]

    while True:  # Continue until no more additions can be made, or 5906 is hit
        best_candidate = None
        best_score = -float('inf')
        best_overlap = 0

        # Find frontier k-mers
        prefix = superpermutation[:n - 1]
        suffix = superpermutation[-(n - 1):]

        # Generate candidates (very small set, focused on the frontier)
        candidates = generate_permutations_on_demand(prefix, suffix, prodigal_results, winners, losers, n, used_permutations, limbo_list, n - 1, hypothetical_prodigals, laminates)
        if not candidates:
            candidates = generate_permutations_on_demand(prefix, suffix, prodigal_results, winners, losers, n, used_permutations, limbo_list, n - 2, hypothetical_prodigals)
        if not candidates:
            #print("No candidates found. Stopping.") #Keep for debugging
            break  # No more candidates can be added

        #Deterministic selection from candidates:
        scored_candidates = []
        for candidate_hash in candidates:
            score = calculate_score(superpermutation, candidate_hash, prodigal_results, winners, losers, layout_memory, n, golden_ratio_points, hypothetical_prodigals)
            scored_candidates.append((score, candidate_hash))
        
        best_candidate = None
        if scored_candidates:
            scored_candidates.sort(reverse=True, key=lambda item: item[0]) #Sort by score, DESCENDING
            best_candidate = scored_candidates[0][1] #Get the hash

        if best_candidate is not None:
            best_candidate_perm = unhash_permutation(best_candidate, n)
            overlap = calculate_overlap(superpermutation, "".join(str(x) for x in best_candidate_perm))
            superpermutation += "".join(str(x) for x in best_candidate_perm)[overlap:]  # Add to superpermutation
            used_permutations.add(best_candidate)

            # Update golden ratio points
            golden_ratio_points = calculate_golden_ratio_points(len(superpermutation))

            # Prodigal Result Check
            new_prodigals = analysis_scripts.find_prodigal_results(superpermutation, n, min_length=PRODIGAL_MIN_LENGTH, overlap_threshold=PRODIGAL_OVERLAP_THRESHOLD) # Use analysis script
            for prodigal_seq in new_prodigals:
                is_new = True
                for existing_prodigal_id, existing_prodigal in prodigal_results.items():
                    if prodigal_seq in existing_prodigal.sequence:
                        is_new = False
                        break
                if is_new:
                    new_id = len(prodigal_results) + 1
                    prodigal_results[new_id] = ProdigalResult(prodigal_seq, new_id)
                    #Create and add new laminates:
                    laminates.append(analysis_scripts.create_laminate(prodigal_seq, n, n-1))
                    laminates.append(analysis_scripts.create_laminate(prodigal_seq, n, n-2))
                    #print(f"New Prodigal Result found: {prodigal_seq}")

            # Update ePUT
            perm_hash = best_candidate
            if perm_hash not in eput:
                eput[perm_hash] = PermutationData(best_candidate_perm, in_sample=False, creation_method="dynamic_generation")
            eput[perm_hash].used_count += 1
            eput[perm_hash].used_in_final = True
            #Update neighbors in ePUT (using consistent k values)
            perm_string = "".join(str(x) for x in best_candidate_perm)
            for k in [n - 1, n - 2]:
                prefix = superpermutation[:k]
                suffix = superpermutation[-k:]
                prefix_perms = set()
                suffix_perms = set()
                for i in range(len(prefix) - n + 1):
                    p = tuple(int(x) for x in prefix[i:i+n])
                    if is_valid_permutation(p,n):
                        prefix_perms.add(hash_permutation(p))
                for i in range(len(suffix) - n + 1):
                    p = tuple(int(x) for x in suffix[i:i+n])
                    if is_valid_permutation(p,n):
                        suffix_perms.add(hash_permutation(p))

                for other_perm_hash in prefix_perms:
                    if other_perm_hash != perm_hash:
                        eput[perm_hash].neighbors.add(other_perm_hash)
                        kmer1 = "".join(str(x) for x in unhash_permutation(other_perm_hash, n)[-k:])
                        kmer2 = perm_string[:k]
                        layout_memory.update_distances(kmer1,kmer2, len(superpermutation) - i - len(prefix), "n7_dynamic")

                for other_perm_hash in suffix_perms:
                    if other_perm_hash != perm_hash:
                        eput[perm_hash].neighbors.add(other_perm_hash)
                        kmer1 = perm_string[-k:]
                        kmer2 = "".join(str(x) for x in unhash_permutation(other_perm_hash, n)[:k])
                        layout_memory.update_distances(kmer1, kmer2, 1, "n7_dynamic")

        else:
            break  # If we get here, we are stuck
        
        if len(superpermutation) >= 5906:
            break #We stop when we hit the target.

    return superpermutation, used_permutations

# END SECTION: Superpermutation Construction

# START SECTION: Main Function

def main():
    """
    Main function to execute the Dynamic Prodigal Assembly algorithm for n=7.
    Generates multiple distinct 5906 superpermutations.
    """
    n = 7  #  Set n=7
    num_iterations = 1000  # Set a high number; the algorithm will likely stop earlier

    # --- File Paths (IMPORTANT: Adjust these paths as needed) ---
    initial_winners_losers_n7_file = "initial_winners_losers_n7.txt"
    prodigal_results_n7_file = "prodigal_results_n7.txt"
    distinct_superpermutations_file = "distinct_superpermutations_n7.txt"
    layout_memory_file = "layout_memory_n7.pkl"

    # --- Load Initial Data ---
    initial_winners, initial_losers = {}, {}
    try:
        with open(initial_winners_losers_n7_file, "r") as f:
            for line in f:
                kmer, w_type, weight = line.strip().split(",")
                if w_type == "winner":
                    initial_winners[kmer] = int(weight)
                else:
                    initial_losers[kmer] = int(weight)
    except FileNotFoundError:
        print(f"Initial Winners/Losers file ({initial_winners_losers_n7_file}) not found. Starting with empty.")


    initial_n7_prodigals = []
    try:
        with open(prodigal_results_n7_file, "r") as f:
            initial_n7_prodigals = [line.strip() for line in f]
    except FileNotFoundError:
        print(f"Initial n=7 Prodigal Results file ({prodigal_results_n7_file}) not found.  Starting with empty.")
        #In this case, we will create an empty file
        with open(prodigal_results_n7_file, "w") as f:
            pass

    # --- Initialize Data Structures ---
    prodigal_results, winners, losers, meta_hierarchy, limbo_list, eput, next_prodigal_id = initialize_data(
        "", initial_n7_prodigals, initial_winners, initial_losers
    )  # Start with an EMPTY string for n=7

    # --- Load Existing Distinct Superpermutations ---
    existing_superpermutations = set()
    try:
        with open(distinct_superpermutations_file, "r") as f:
            for line in f:
                existing_superpermutations.add(line.strip())
    except FileNotFoundError:
        print(f"No existing distinct superpermutations file found.")
        #Create File
        with open(distinct_superpermutations_file, "w") as f:
            pass

    # --- Create Initial Laminate (from initial prodigals) ---
    laminates = []
    for prodigal in initial_n7_prodigals:
        laminates.append(analysis_scripts.create_laminate(prodigal, n, n - 1))
        laminates.append(analysis_scripts.create_laminate(prodigal, n, n - 2))
        
    # Load Layout memory
    layout_memory = LayoutMemory()
    try:
        layout_memory.load_from_file(layout_memory_file)
        print("Loaded LayoutMemory from file.")
    except FileNotFoundError:
        print("No existing LayoutMemory file found. Starting with a new one.")


    distinct_count = 0
    run_count = 0
    # --- Main Iterative Loop ---
    while True: #Continue until we reach the iteration limit, OR no new variations found
        run_count += 1
        print(f"Starting run {run_count}...")
        start_time = time.time()

        # 1. Generate Hypothetical Prodigals (now done *before* construction)
        hypothetical_prodigals = analysis_scripts.generate_hypothetical_prodigals(prodigal_results, winners, losers, n)
        print(f"  Generated {len(hypothetical_prodigals)} hypothetical prodigals.")
        
        #Add to the prodigals list
        for h_id, h_prodigal in hypothetical_prodigals.items():
            prodigal_results[next_prodigal_id] = h_prodigal
            next_prodigal_id += 1

        # 2. Construct Superpermutation (using the dynamic approach, starting from EMPTY)
        superpermutation, used_permutations = construct_superpermutation([], prodigal_results, winners, losers, layout_memory, meta_hierarchy, limbo_list, n, hypothetical_prodigals = hypothetical_prodigals)
        print(f"  Superpermutation length: {len(superpermutation)}")

        # 3. Update Data and check for validity
        analysis_results = analysis_scripts.analyze_superpermutation(superpermutation, n)  # Use analysis script
        print(f"  Valid: {analysis_results['validity']}")
        print(f"  Overlap Distribution: {analysis_results['overlap_distribution']}")

        # Check for 5906 length and distinctness
        if analysis_results['validity'] and len(superpermutation) == 5906:
            is_distinct = True
            for existing_sp in existing_superpermutations:
                if not analysis_scripts.is_cyclically_distinct(superpermutation, existing_sp):
                    is_distinct = False
                    break

            if is_distinct:
                print("  Found a *distinct* 5906 superpermutation!")
                existing_superpermutations.add(superpermutation)
                distinct_count += 1
                with open(distinct_superpermutations_file, "a") as f:  # Append to the file
                    f.write(superpermutation + "\n")
                #Create and add new laminates
                new_lam_1 = analysis_scripts.create_laminate(superpermutation, n, n-1)
                new_lam_2 = analysis_scripts.create_laminate(superpermutation, n, n-2)
                laminates.append(new_lam_1)
                laminates.append(new_lam_2)
            else:
                print("  Found a 5906 superpermutation, but it's a duplicate.")
        else:
            print("  Run did not produce a valid minimal superpermutation.")


        # Find and add new prodigal results. Stricter criteria.
        new_prodigals = analysis_scripts.find_prodigal_results(superpermutation, n, min_length=PRODIGAL_MIN_LENGTH, overlap_threshold=PRODIGAL_OVERLAP_THRESHOLD)  # Adjusted
        for prodigal_seq in new_prodigals:
            is_new = True
            for existing_prodigal_id, existing_prodigal in prodigal_results.items():
                if prodigal_seq in existing_prodigal.sequence:
                    is_new = False
                    break
            if is_new:
                prodigal_results[next_prodigal_id] = ProdigalResult(prodigal_seq, next_prodigal_id)
                next_prodigal_id += 1
                # Create and add a laminate for the new prodigal
                laminates.append(analysis_scripts.create_laminate(prodigal_seq, n, n-1))
                laminates.append(analysis_scripts.create_laminate(prodigal_seq, n, n-2))

        # Update "Winners" and "Losers" (using the new superpermutation, and k=6 and k=5)
        new_winners6, new_losers6 = analysis_scripts.calculate_winners_losers([superpermutation], n, k=6)
        new_winners5, new_losers5 = analysis_scripts.calculate_winners_losers([superpermutation], n, k=5)

        for kmer, weight in new_winners6.items():
            winners[kmer] = winners.get(kmer, 0) + weight
        for kmer, weight in new_losers6.items():
            losers[kmer] = losers.get(kmer, 0) + weight
            limbo_list.add(kmer) #Add to the limbo list
        for kmer, weight in new_winners5.items():
            winners[kmer] = winners.get(kmer, 0) + weight
        for kmer, weight in new_losers5.items():
            losers[kmer] = losers.get(kmer, 0) + weight
            limbo_list.add(kmer) #Add to the limbo list

        # Higher-Order Winners/Losers
        new_seq_winners, new_seq_losers = analysis_scripts.calculate_sequence_winners_losers([superpermutation], n)
        for seq_hash, weight in new_seq_winners.items():
            if seq_hash in winners:
                winners[seq_hash] += weight
            else:
                winners[seq_hash] = weight
        for seq_hash, weight in new_seq_losers.items():
            if seq_hash in losers:
                losers[seq_hash] += weight
            else:
                losers[seq_hash] = weight

        # Update ePUT (add all *used* permutations)
        s_tuple = tuple(int(x) for x in superpermutation)
        for i in range(len(s_tuple) - n + 1):
            perm = s_tuple[i:i+n]
            if is_valid_permutation(perm, n):
                perm_hash = hash_permutation(perm)
                if perm_hash not in eput:
                    eput[perm_hash] = PermutationData(perm, in_sample=False, creation_method="dynamic_generation")
                eput[perm_hash].used_count += 1
                eput[perm_hash].used_in_final = True #All are used
                #Update neighbors in ePUT.  This is less important for n=7
                if i > 0:
                    prev_perm = s_tuple[i-1:i-1+n]
                    if is_valid_permutation(prev_perm, n):
                        eput[perm_hash].neighbors.add(hash_permutation(prev_perm))
                if i < len(s_tuple) - n:
                    next_perm = s_tuple[i+1:i+1+n]
                    if is_valid_permutation(next_perm, n):
                        eput[perm_hash].neighbors.add(hash_permutation(next_perm))

        # Update Layout Memory
        layout_memory.add_sequence(superpermutation, n, 6, f"run_{run_count}")
        layout_memory.add_sequence(superpermutation, n, 5, f"run_{run_count}")

        # Update "Meta-Hierarchy" (simplified for this example)
        meta_hierarchy.setdefault("run_lengths", []).append(len(superpermutation))
        meta_hierarchy.setdefault("prodigal_counts", []).append(len(prodigal_results))
        #Calculate hypothetical success rate:
        total_hypotheticals = len(hypothetical_prodigals)
        successful_hypotheticals = 0
        for h_id, h_prodigal in hypothetical_prodigals.items():
            if h_prodigal.sequence in superpermutation:
                successful_hypotheticals += 1
        success_rate = (successful_hypotheticals / total_hypotheticals) if total_hypotheticals > 0 else 0
        meta_hierarchy.setdefault("hypothetical_success_rate",[]).append(success_rate)


        end_time = time.time()
        print(f"  Run {run_count} completed in {end_time - start_time:.2f} seconds.")
        print(f"  Distinct 5906 superpermutations found so far: {distinct_count}")

        # Check stopping criteria
        if len(superpermutation) == 5906:
            # Check for early stopping based on lack of new distinct solutions
            is_new_solution = True
            for existing_sp in existing_superpermutations:
                if not analysis_scripts.is_cyclically_distinct(superpermutation, existing_sp):
                    is_new_solution = False
                    break

            if not is_new_solution:
                meta_hierarchy["no_new_solutions_count"] = meta_hierarchy.get("no_new_solutions_count",0) + 1
                if meta_hierarchy["no_new_solutions_count"] >= 20:
                    print("No new distinct 5906 solutions found in 20 iterations. Stopping.")
                    break
            else:
                meta_hierarchy["no_new_solutions_count"] = 0  # Reset counter

        if run_count >= 100:
            print("Reached 100 runs. Stopping.")
            break

    layout_memory.save_to_file(layout_memory_file)


if __name__ == "__main__":
    random.seed(RANDOM_SEED)
    main()
```

### src/unified/embedder/cand_1_knowledge_base_integration_py.py

```python
import requests
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

class KnowledgeBaseIntegrator:
    def __init__(self, config):
        self.config = config
        self.api_keys = self.load_api_keys()
        self.embeddings = {}

    def load_api_keys(self):
        # Placeholder implementation
        return {"openstax": self.config.get("openstax_api_key")}

    def query_openstax(self, query):
        # Placeholder implementation
        api_key = self.api_keys.get("openstax")
        # Implement actual API call here
        return f"OpenStax result for query: {query}"

    def query_other_free_resource(self, query):
        # Placeholder implementation
        return f"Other free resource result for query: {query}"

    def create_embedding(self, text):
        # Placeholder implementation
        return np.random.rand(100)  # Return a random 100-dimensional vector

    def find_relevant_info(self, query, threshold=0.7):
        query_embedding = self.create_embedding(query)
        relevant_info = []
        for source, embeddings in self.embeddings.items():
            similarities = cosine_similarity([query_embedding], embeddings)[0]
            relevant_indices = np.where(similarities > threshold)[0]
            relevant_info.extend([source[i] for i in relevant_indices])
        return relevant_info

    def update_knowledge_base(self):
        # Placeholder implementation
        openstax_data = self.query_openstax("superpermutations")
        other_data = self.query_other_free_resource("superpermutations")
        self.embeddings["openstax"] = [self.create_embedding(openstax_data)]
        self.embeddings["other"] = [self.create_embedding(other_data)]


```

### src/unified/embedder/cand_20_generation_code_n7_dynamic_final_py.py

```python
import itertools
import random
import math
import time
import networkx as nx
from collections import deque, defaultdict
import heapq
import analysis_scripts
from layout_memory import LayoutMemory

# --- Constants ---
N = 7  # The value of n (number of symbols).
PRODIGAL_OVERLAP_THRESHOLD = 0.98  # Initial minimum overlap rate for "Prodigal Results"
PRODIGAL_MIN_LENGTH = 10 #Reduced
HYPOTHETICAL_PRODIGAL_OVERLAP_THRESHOLD = 0.90 #Reduced
HYPOTHETICAL_PRODIGAL_MIN_LENGTH = 7
HYPOTHETICAL_PRODIGAL_GENERATION_COUNT = 50 # Number of Hypothetical Prodigals to Generate
WINNER_THRESHOLD = 0.75  # Not directly used in scoring
LOSER_THRESHOLD = 0.25  # Not directly used in scoring
NUM_ITERATIONS = 10000  #  Set a large number; it will likely stop earlier
LAYOUT_K_VALUES = [N - 1, N - 2]  # k values for Layout Memory
DE_BRUIJN_K_VALUES = [N - 1, N - 2]  # k values for De Bruijn graph generation
RANDOM_SEED = 42  # For reproducibility.  CHANGE THIS FOR EACH RUN.

# --- Helper Functions --- (These are identical to the n=8 versions)
def calculate_overlap(s1: str, s2: str) -> int:
    """Calculates the maximum overlap between two strings."""
    max_overlap = 0
    for i in range(1, min(len(s1), len(s2)) + 1):
        if s1[-i:] == s2[:i]:
            max_overlap = i
    return max_overlap

def generate_permutations(n: int) -> list[tuple[int, ...]]:
    """Generates all permutations of 1 to n."""
    return list(itertools.permutations(range(1, n + 1)))

def calculate_distance(p1: str, p2: str, n: int) -> int:
    """Calculates distance (n-1 - overlap)."""
    return (n - 1) - max(calculate_overlap(p1, p2), calculate_overlap(p2, p1))

def hash_permutation(permutation: tuple) -> int:
    """Hashes a permutation tuple to a unique integer."""
    result = 0
    n = len(permutation)
    for i, val in enumerate(permutation):
        result += val * (n ** (n - 1 - i))
    return result

def unhash_permutation(hash_value: int, n: int) -> tuple:
    """Converts a hash value back to a permutation tuple."""
    permutation = []
    for i in range(n - 1, -1, -1):
        val = hash_value // (n ** i)
        permutation.append(val + 1)  # Adjust to be 1-indexed
        hash_value -= val * (n ** i)
    return tuple(permutation)

def is_valid_permutation(perm: tuple, n: int) -> bool:
    """Checks if a given sequence is a valid permutation."""
    return len(set(perm)) == n and min(perm) == 1 and max(perm) == n

def calculate_golden_ratio_points(length: int, levels: int = 1) -> list[int]:
    """Calculates multiple levels of golden ratio points."""
    phi = (1 + math.sqrt(5)) / 2
    points = []
    for _ in range(levels):
        new_points = []
        if not points:
            new_points = [int(length / phi), int(length - length / phi)]
        else:
            for p in points:
                new_points.extend([int(p / phi), int(p - p / phi)])
                new_points.extend([int((length-p) / phi) + p, int(length - (length-p) / phi)])
        points.extend(new_points)
        points = sorted(list(set(points)))  # Remove duplicates and sort
    return points

def get_kmers(sequence: str, n: int, k: int) -> set[str]:
    """Extracts all k-mers from a sequence, given n and k."""
    kmers = set()
    seq_list = [int(x) for x in sequence]
    for i in range(len(sequence) - n + 1):
        perm = tuple(seq_list[i:i+n])
        if is_valid_permutation(perm, n):
            if i >= k:
                kmer = "".join(str(x) for x in seq_list[i-k:i])
                kmers.add(kmer)
    return kmers
# --- Data Structures --- (Same as n=8 version)

class PermutationData:
    def __init__(self, permutation: tuple, in_sample: bool = False, creation_method: str = ""):
        """Stores data associated with a single permutation."""
        self.hash: int = hash_permutation(permutation)
        self.permutation: tuple = permutation
        self.in_sample: bool = in_sample  # Will always be False
        self.used_count: int = 0
        self.prodigal_status: list[int] = []
        self.creation_method: str = creation_method
        self.batch_ids: list[int] = [] #Not used
        self.used_in_final: bool = False
        self.neighbors: set[int] = set()

    def __str__(self) -> str:
        return (f"PermutationData(hash={self.hash}, permutation={self.permutation}, used_count={self.used_count}, "
                f"prodigal_status={self.prodigal_status}, creation_method={self.creation_method})")

    def __repr__(self) -> str:
        return self.__str__()

class ProdigalResult:
    def __init__(self, sequence: str, result_id: int):
        """Represents a 'Prodigal Result'."""
        self.id: int = result_id
        self.sequence: str = sequence
        self.length: int = len(sequence)
        self.permutations: set[int] = set()  # Store hashes
        self.calculate_permutations()
        self.overlap_rate: float = self.calculate_overlap_rate()

    def calculate_permutations(self):
        """Calculates and stores the set of permutations."""
        n = N
        for i in range(len(self.sequence) - n + 1):
            perm = tuple(int(x) for x in self.sequence[i:i + n])
            if is_valid_permutation(perm, n):
                self.permutations.add(hash_permutation(perm))

    def calculate_overlap_rate(self) -> float:
        """Calculates the overlap rate."""
        n = N
        total_length = sum(len(str(p)) for p in [unhash_permutation(x,n) for x in self.permutations])
        overlap_length = total_length - len(self.sequence)
        max_possible_overlap = (len(self.permutations) - 1) * (n - 1)
        if max_possible_overlap == 0:
            return 0
        return overlap_length / max_possible_overlap

    def __str__(self) -> str:
        return (f"ProdigalResult(id={self.id}, length={self.length}, "
                f"overlap_rate={self.overlap_rate:.4f}, num_permutations={len(self.permutations)})")

    def __repr__(self) -> str:
        return self.__str__()
# --- Initialization Function ---
def initialize_data(initial_n7_prodigals: list[str], initial_winners: dict, initial_losers: dict) -> tuple:
    """Initializes the data structures for the algorithm.
        Modified for n=7, no initial superpermutation, loads initial prodigals.
    Args:
        initial_n7_prodigals (list[str]): A list of initial n=7 Prodigal Result strings.
        initial_winners (dict):  Initial Winner k-mers and weights.
        initial_losers (dict): Initial Loser k-mers and weights.

    Returns:
        tuple: (prodigal_results, winners, losers, meta_hierarchy, limbo_list, eput, next_prodigal_id)
    """
    prodigal_results = {}  # {prodigal_id: ProdigalResult object}
    winners = initial_winners  # {kmer: weight}
    losers = initial_losers  # {kmer: weight}
    meta_hierarchy = {}  # Track strategy effectiveness
    limbo_list = set()  # Set of permutation hashes.
    eput = {}  # The Enhanced Permutation Universe Tracker

    # Add initial n=7 prodigals
    next_prodigal_id = 0
    for prodigal in initial_n7_prodigals:
        prodigal_results[next_prodigal_id] = ProdigalResult(prodigal, next_prodigal_id)
        next_prodigal_id += 1

    return prodigal_results, winners, losers, meta_hierarchy, limbo_list, eput, next_prodigal_id

# --- On-Demand Permutation Generation ---
def generate_permutations_on_demand(prefix: str, suffix: str, prodigal_results: dict, winners: dict, losers: dict,
                                   n: int, eput: dict, limbo_list: set, min_overlap: int,
                                   hypothetical_prodigals: dict = None, laminate_graphs: list = None) -> set[int]:
    """Generates permutations on demand, extending a given prefix or suffix.
       Prioritizes "Prodigal" extensions, uses "Winners" and "Losers," and
       avoids already-used permutations and the "Limbo List."  Also uses laminates.

    Args:
        prefix (str): The prefix of the current superpermutation.
        suffix (str): The suffix of the current superpermutation.
        prodigal_results (dict): Dictionary of ProdigalResult objects.
        winners (dict): Dictionary of Winner k-mers and their weights.
        losers (dict): Dictionary of Loser k-mers and their weights.
        n (int): The value of n.
        eput (dict): The Enhanced Permutation Universe Tracker.
        limbo_list (set): The set of permutation hashes to avoid.
        min_overlap (int):  The minimum required overlap.
        hypothetical_prodigals (dict): Optional dictionary of Hypothetical Prodigals
        laminate_graphs (list): List of laminate graphs.

    Returns:
        set[int]: A set of *permutation hashes* that are potential extensions.
    """
    valid_permutations = set()

    # Prioritize Hypothetical Prodigal extensions
    if hypothetical_prodigals:
        for prodigal_id, prodigal in hypothetical_prodigals.items():
            if prefix and prodigal.sequence.startswith(prefix[-min_overlap:]):
                for i in range(len(prodigal.sequence) - (n - 1)):
                    perm = tuple(int(x) for x in prodigal.sequence[i:i + n])
                    if analysis_scripts.is_valid_permutation(perm, n) and hash_permutation(perm) not in eput:
                         valid_permutations.add(hash_permutation(perm))
            if suffix and prodigal.sequence.endswith(suffix[:min_overlap]):
                for i in range(len(prodigal.sequence) - (n - 1)):
                    perm = tuple(int(x) for x in prodigal.sequence[i:i + n])
                    if analysis_scripts.is_valid_permutation(perm, n) and hash_permutation(perm) not in eput:
                        valid_permutations.add(hash_permutation(perm))

    # Prioritize Prodigal extensions
    for prodigal_id, prodigal in prodigal_results.items():
        if prefix and prodigal.sequence.startswith(prefix[-min_overlap:]):
            for i in range(len(prodigal.sequence) - (n - 1)):
                perm = tuple(int(x) for x in prodigal.sequence[i:i + n])
                if analysis_scripts.is_valid_permutation(perm, n) and hash_permutation(perm) not in eput:
                    valid_permutations.add(hash_permutation(perm))
        if suffix and prodigal.sequence.endswith(suffix[:min_overlap]):
            for i in range(len(prodigal.sequence) - (n - 1)):
                perm = tuple(int(x) for x in prodigal.sequence[i:i + n])
                if analysis_scripts.is_valid_permutation(perm, n) and hash_permutation(perm) not in eput:
                    valid_permutations.add(hash_permutation(perm))


    # Filter based on Limbo List and Laminate
    filtered_permutations = set()
    for perm_hash in valid_permutations:
        perm = unhash_permutation(perm_hash, n)
        perm_str = "".join(str(x) for x in perm)
        is_in_limbo = False

        # Basic Loser check (using 5-mers and 6-mers for n=7)
        for k in [6, 5]:  # Check 6-mers and 5-mers
            for i in range(len(perm_str) - k + 1):
                kmer = perm_str[i:i+k]
                if kmer in limbo_list:
                    is_in_limbo = True
                    break
            if is_in_limbo:
                break
        
        if not is_in_limbo:
            valid_perm = False
            if laminate_graphs:
                for laminate in laminate_graphs:
                    if analysis_scripts.is_compatible(perm, laminate, n, n - 1):
                        valid_perm = True
                        break
                    elif analysis_scripts.is_compatible(perm, laminate, n, n - 2):
                        valid_perm = True
                        break
            else: #If no laminates, pass.
                valid_perm = True
            if valid_perm:
                filtered_permutations.add(perm_hash)

    return filtered_permutations

# --- Scoring Function ---
def calculate_score(current_superpermutation: str, permutation_hash: int, prodigal_results: dict,
                    winners: dict, losers: dict, layout_memory: LayoutMemory, n: int,
                    golden_ratio_points: list[int], hypothetical_prodigals:dict) -> float:
    """Calculates the score for adding a permutation (n=7 version)."""
    permutation = unhash_permutation(permutation_hash, n)
    permutation_string = "".join(str(x) for x in permutation)
    overlap = calculate_overlap(current_superpermutation, permutation_string)
    score = overlap * 5  # Base score based on overlap

    # Prodigal Result Bonus (very high)
    prodigal_bonus = 0
    for prodigal_id, prodigal in prodigal_results.items():
        if permutation_string in prodigal.sequence:
            prodigal_bonus += prodigal.length * 100  # Large bonus
            break

    #Hypothetical bonus
    hypothetical_bonus = 0
    if hypothetical_prodigals:
        for h_prodigal_id, h_prodigal in hypothetical_prodigals.items():
            if permutation_string in h_prodigal.sequence:
                hypothetical_bonus +=  h_prodigal.length * 25

    # Winner and Loser Bonus/Penalty (using Layout Memory)
    k_values = [n - 1, n - 2]
    layout_bonus = 0
    for k in k_values:
        if len(current_superpermutation) >= k:
            kmer_end = current_superpermutation[-k:]
            kmer_start = permutation_string[:k]
            layout_bonus += layout_memory.get_layout_score(kmer_end, kmer_start, 1)

    # Golden Ratio Bonus (small, dynamic)
    golden_ratio_bonus = 0
    insertion_point = len(current_superpermutation)
    for point in golden_ratio_points:
        distance = abs(insertion_point - point)
        golden_ratio_bonus += math.exp(-distance / (len(current_superpermutation)/20))  # Smaller divisor = tighter

    # Loser Penalty (Veto)
    loser_penalty = 0
    for k in [6, 5]:  # Check 6-mers and 5-mers for n=7
        for i in range(len(permutation_string) - k + 1):
            kmer = permutation_string[i:i+k]
            loser_penalty += losers.get(kmer, 0) * 5 # Penalty for losers

    # Higher-Order Winners/Losers
    higher_order_bonus = 0
    for seq_length in [2, 3]:  # Check sequences of length 2 and 3
      if len(current_superpermutation) >= (n * seq_length):
        prev_seq = current_superpermutation[-(n*seq_length):]
        prev_perms = []
        for i in range(len(prev_seq) - n + 1):
            pp = tuple(int(x) for x in prev_seq[i:i+n])
            if is_valid_permutation(pp, n):
                prev_perms.append(hash_permutation(pp))
        if len(prev_perms) >= (seq_length -1):
            current_seq = tuple(prev_perms[-(seq_length - 1):] + [permutation_hash])
            current_seq_hash = hash(current_seq)
            higher_order_bonus += winners.get(current_seq_hash, 0) * 5
            loser_penalty += losers.get(current_seq_hash, 0) * 5

    score += prodigal_bonus + layout_bonus + golden_ratio_bonus + hypothetical_bonus- loser_penalty + higher_order_bonus

    return score
# --- Main Construction Function ---

def construct_superpermutation(initial_permutations: list, prodigal_results: dict, winners: dict, losers: dict,
                              layout_memory: LayoutMemory, meta_hierarchy: dict, limbo_list: set, n: int,
                              hypothetical_prodigals: dict) -> tuple[str, set[int]]:
    """Constructs a superpermutation using the dynamic prodigal approach (n=7 version).

    Args:
        initial_permutations (list):  Empty list
        prodigal_results (dict): Dictionary of ProdigalResult objects.
        winners (dict): Dictionary of Winner k-mers and their weights.
        losers (dict): Dictionary of Loser k-mers and their weights.
        layout_memory (LayoutMemory): The LayoutMemory object.
        meta_hierarchy (dict): Dictionary for tracking strategy effectiveness.
        limbo_list (set): Set of permutation hashes to avoid.
        n (int): The value of n.
        hypothetical_prodigals (dict): "Hypothetical Prodigals" to use.

    Returns:
        tuple: (superpermutation string, set of used permutation hashes)
    """

    superpermutation = ""
    used_permutations = set()

    # Initialize with the longest "Prodigal Result" (should be the initial 5906)
    if prodigal_results:
        best_prodigal_key = max(prodigal_results, key=lambda k: prodigal_results[k].length)
        superpermutation = prodigal_results[best_prodigal_key].sequence
        used_permutations.update(prodigal_results[best_prodigal_key].permutations)
    else:
        #Should never reach here.
        superpermutation = "1234567" #Bare minimum to start.
        used_permutations.add(hash_permutation((1,2,3,4,5,6,7)))

    golden_ratio_points = calculate_golden_ratio_points(len(superpermutation), levels=3)
    #Create Laminates
    laminates = [analysis_scripts.create_laminate(prodigal_results[key].sequence, n, k) for key in prodigal_results for k in LAYOUT_K_VALUES]


    while True:  # Continue until no more additions can be made or 5906 is hit
        best_candidate = None
        best_score = -float('inf')
        best_candidate_string = ""

        # Find frontier k-mers
        prefix = superpermutation[:n - 1]
        suffix = superpermutation[-(n - 1):]

        # Generate candidates (very small set, focused on the frontier)
        candidates = generate_permutations_on_demand(prefix, suffix, prodigal_results, winners, losers, n, used_permutations, limbo_list, n - 1, hypothetical_prodigals, laminates)
        if not candidates:
            candidates = generate_permutations_on_demand(prefix, suffix, prodigal_results, winners, losers, n, used_permutations, limbo_list, n - 2, hypothetical_prodigals, laminates)
        if not candidates:
            # print("No candidates found. Stopping.")  # Keep for debugging
            break  # No more candidates can be added
        #Deterministic selection from candidates:
        scored_candidates = []
        for candidate_hash in candidates:
            score = calculate_score(superpermutation, candidate_hash, prodigal_results, winners, losers, layout_memory, n, golden_ratio_points, hypothetical_prodigals)
            scored_candidates.append((score, candidate_hash))
        
        best_candidate = None
        if scored_candidates:
            scored_candidates.sort(reverse=True, key=lambda item: item[0]) #Sort by score, DESCENDING
            best_candidate = scored_candidates[0][1] #Get the hash

        if best_candidate is not None:
            best_candidate_perm = unhash_permutation(best_candidate, n)
            overlap = calculate_overlap(superpermutation, "".join(str(x) for x in best_candidate_perm))
            superpermutation += "".join(str(x) for x in best_candidate_perm)[overlap:]  # Add to superpermutation
            used_permutations.add(best_candidate)

            # Update golden ratio points
            golden_ratio_points = calculate_golden_ratio_points(len(superpermutation))

            # Prodigal Result Check
            new_prodigals = analysis_scripts.find_prodigal_results(superpermutation, n, min_length=PRODIGAL_MIN_LENGTH, overlap_threshold=PRODIGAL_OVERLAP_THRESHOLD)  #Use current thresholds
            for prodigal_seq in new_prodigals:
                is_new = True
                for existing_prodigal_id, existing_prodigal in prodigal_results.items():
                    if prodigal_seq in existing_prodigal.sequence:
                        is_new = False
                        break
                if is_new:
                    new_id = len(prodigal_results) + 1
                    prodigal_results[new_id] = ProdigalResult(prodigal_seq, new_id)
                    # print(f"New Prodigal Result found: {prodigal_seq}")  # Keep for debugging
                    #Create and add laminates:
                    laminates.append(analysis_scripts.create_laminate(prodigal_seq, n, n-1))
                    laminates.append(analysis_scripts.create_laminate(prodigal_seq, n, n-2))

            # Update ePUT
            perm_hash = best_candidate
            perm_string = "".join(str(x) for x in best_candidate_perm)
            if perm_hash not in eput:  # Should always be true here
                eput[perm_hash] = PermutationData(best_candidate_perm, in_sample=False, creation_method="dynamic_generation")
            eput[perm_hash].used_count += 1
            eput[perm_hash].used_in_final = True
            # Update neighbors in ePUT (using consistent k values)
            for k in [n - 1, n - 2]:
                prefix = superpermutation[:k]
                suffix = superpermutation[-k:]
                prefix_perms = set()
                suffix_perms = set()
                for i in range(len(prefix) - n + 1):
                    p = tuple(int(x) for x in prefix[i:i+n])
                    if is_valid_permutation(p,n):
                        prefix_perms.add(hash_permutation(p))
                for i in range(len(suffix) - n + 1):
                    p = tuple(int(x) for x in suffix[i:i+n])
                    if is_valid_permutation(p,n):
                        suffix_perms.add(hash_permutation(p))

                for other_perm_hash in prefix_perms:
                    if other_perm_hash != perm_hash:
                        eput[perm_hash].neighbors.add(other_perm_hash)
                        # Add to layout memory if not exist
                        kmer1 = "".join(str(x) for x in unhash_permutation(other_perm_hash, n)[-k:])
                        kmer2 = perm_string[:k]
                        layout_memory.update_distances(kmer1,kmer2, len(superpermutation) - i - len(prefix), "n7_dynamic")
                for other_perm_hash in suffix_perms:
                    if other_perm_hash != perm_hash:
                        eput[perm_hash].neighbors.add(other_perm_hash)
                        kmer1 = perm_string[-k:]
                        kmer2 = "".join(str(x) for x in unhash_permutation(other_perm_hash, n)[:k])
                        layout_memory.update_distances(kmer1, kmer2, 1, "n7_dynamic")
        else:
            break  # If we get here, we are stuck

        if len(superpermutation) >= 5906: #If we reach the target, break.
            break

    return superpermutation, used_permutations

# --- Main Function ---
def main():
    """
    Main function to execute the Dynamic Prodigal Assembly algorithm for n=7.
    Generates multiple distinct 5906 superpermutations.
    """
    # File Paths
    initial_winners_losers_n7_file = "initial_winners_losers_n7.txt"
    prodigal_results_n7_file = "prodigal_results_n7.txt"
    distinct_superpermutations_file = "distinct_superpermutations_n7.txt"
    layout_memory_file = "layout_memory_n7.pkl"

    # Load Initial Data
    initial_winners, initial_losers = {}, {}
    try:
        with open(initial_winners_losers_n7_file, "r") as f:
            for line in f:
                kmer, w_type, weight = line.strip().split(",")
                if w_type == "winner":
                    initial_winners[kmer] = int(weight)
                else:
                    initial_losers[kmer] = int(weight)
    except FileNotFoundError:
        print("Initial Winners/Losers file not found. Starting with empty.")

    initial_n7_prodigals = []
    try:
        with open(prodigal_results_n7_file, "r") as f:
            initial_n7_prodigals = [line.strip() for line in f]
    except FileNotFoundError:
        print("Initial n=7 Prodigal Results file not found.  Starting with empty.")
        #In this case, we will create an empty file
        with open(prodigal_results_n7_file, "w") as f:
            pass
    # Load existing distinct superpermutations
    existing_superpermutations = set()
    try:
        with open(distinct_superpermutations_file, "r") as f:
            for line in f:
                existing_superpermutations.add(line.strip())
    except FileNotFoundError:
        print("No existing distinct superpermutations file found.")
        #Create File
        with open(distinct_superpermutations_file, "w") as f:
            pass

    # Initialize data structures
    prodigal_results, winners, losers, meta_hierarchy, limbo_list, eput, next_prodigal_id = initialize_data(
        "", initial_n7_prodigals, initial_winners, initial_losers
    )  # Start with an EMPTY string

    #Create Initial Laminate
    laminates = []
    for prodigal in initial_n7_prodigals:
        laminates.append(analysis_scripts.create_laminate(prodigal, N, N-1))
        laminates.append(analysis_scripts.create_laminate(prodigal, N, N-2))
    #If no initial, create blank laminate
    if not laminates:
        laminates = [nx.DiGraph()]
    
    # Load Layout memory
    layout_memory = LayoutMemory()
    try:
        layout_memory.load_from_file(layout_memory_file)
        print("Loaded LayoutMemory from file.")
    except FileNotFoundError:
        print("No existing LayoutMemory file found. Starting with a new one.")

    distinct_count = len(existing_superpermutations)
    run_count = 0

    # --- Main Iterative Loop ---
    while True: # Keep generating until a stopping criterion is met
        run_count += 1
        print(f"Starting run {run_count}...")
        start_time = time.time()

        # Set a new random seed for each run
        random.seed(RANDOM_SEED + run_count)  # Use a different seed each run

        # 1. Generate Hypothetical Prodigals
        hypothetical_prodigals = analysis_scripts.generate_hypothetical_prodigals(prodigal_results, winners, losers, N)
        print(f"  Generated {len(hypothetical_prodigals)} hypothetical prodigals.")

        # 2. Construct Superpermutation (Dynamic, Prodigal-Focused)
        superpermutation, used_permutations = construct_superpermutation([], prodigal_results, winners, losers, layout_memory, meta_hierarchy, limbo_list, N, hypothetical_prodigals)
        print(f"  Superpermutation length: {len(superpermutation)}")

        # 3. Analysis and Updates
        analysis_results = analysis_scripts.analyze_superpermutation(superpermutation, N)
        print(f"  Valid: {analysis_results['validity']}")
        print(f"  Overlap Distribution: {analysis_results['overlap_distribution']}")
        
        # Check for 5906 and distinctness
        if analysis_results['validity'] and len(superpermutation) == 5906:
            is_distinct = True
            for existing_sp in existing_superpermutations:
                if not analysis_scripts.is_cyclically_distinct(superpermutation, existing_sp):
                    is_distinct = False
                    break

            if is_distinct:
                print("  Found a *distinct* 5906 superpermutation!")
                distinct_count += 1
                existing_superpermutations.add(superpermutation)
                with open(distinct_superpermutations_file, "a") as f:  # Append
                    f.write(superpermutation + "\n")
                #Create and add laminates
                new_lam_1 = analysis_scripts.create_laminate(superpermutation, N, N-1)
                new_lam_2 = analysis_scripts.create_laminate(superpermutation, N, N-2)
                laminates.append(new_lam_1)
                laminates.append(new_lam_2)
            else:
                print("  Found a 5906 superpermutation, but it's a duplicate.")
        else:
            print("  Run did not produce a valid minimal superpermutation.")
            
        # Update "Winners" and "Losers" (using the new superpermutation)
        new_winners, new_losers = analysis_scripts.calculate_winners_losers([superpermutation], N, k=N-1)
        new_winners2, new_losers2 = analysis_scripts.calculate_winners_losers([superpermutation], N, k=N-2)

        for kmer, weight in new_winners.items():
            winners[kmer] = winners.get(kmer, 0) + weight
        for kmer, weight in new_losers.items():
            losers[kmer] = losers.get(kmer, 0) + weight
        for kmer, weight in new_winners2.items():
            winners[kmer] = winners.get(kmer, 0) + weight
        for kmer, weight in new_losers2.items():
            losers[kmer] = losers.get(kmer, 0) + weight

        # Higher-Order Winners/Losers
        new_seq_winners, new_seq_losers = analysis_scripts.calculate_sequence_winners_losers([superpermutation], N)
        for seq_hash, weight in new_seq_winners.items():
            if seq_hash in winners:
                winners[seq_hash] += weight
            else:
                winners[seq_hash] = weight
        for seq_hash, weight in new_seq_losers.items():
            if seq_hash in losers:
                losers[seq_hash] += weight
            else:
                losers[seq_hash] = weight

        # Add new losers to limbo list
        for kmer, weight in losers.items():
            limbo_list.add(kmer)  # Add to limbo list

        # Find and add new prodigal results (stricter criteria).
        new_prodigals = analysis_scripts.find_prodigal_results(superpermutation, N, min_length=PRODIGAL_MIN_LENGTH, overlap_threshold=PRODIGAL_OVERLAP_THRESHOLD)
        for prodigal_seq in new_prodigals:
            is_new = True
            for existing_prodigal_id, existing_prodigal in prodigal_results.items():
                if prodigal_seq in existing_prodigal.sequence:
                    is_new = False
                    break
            if is_new:
                prodigal_results[next_prodigal_id] = ProdigalResult(prodigal_seq, next_prodigal_id)
                next_prodigal_id += 1
                #Create and add laminate
                laminates.append(analysis_scripts.create_laminate(prodigal_seq, N, N-1))
                laminates.append(analysis_scripts.create_laminate(prodigal_seq, N, N-2))


        # Update ePUT (add all *used* permutations)
        s_tuple = tuple(int(x) for x in superpermutation)
        for i in range(len(s_tuple) - n + 1):
            perm = s_tuple[i:i+n]
            if is_valid_permutation(perm, n):
                perm_hash = hash_permutation(perm)
                if perm_hash not in eput:
                    eput[perm_hash] = PermutationData(perm, in_sample=False, creation_method="dynamic_generation") # Dynamic
                eput[perm_hash].used_count += 1
                eput[perm_hash].used_in_final = True  # Mark as used in this iteration's superpermutation
                # Update neighbors in ePUT (using consistent k values)
                if i > 0:
                    prev_perm = s_tuple[i-1:i-1+n]
                    if is_valid_permutation(prev_perm, n):
                         eput[perm_hash].neighbors.add(hash_permutation(prev_perm))
                if i < len(s_tuple) - n:
                    next_perm = s_tuple[i+1:i+1+n]
                    if is_valid_permutation(next_perm, n):
                        eput[perm_hash].neighbors.add(hash_permutation(next_perm))

        # Update Layout Memory
        layout_memory.add_sequence(superpermutation, N, N-1, f"run_{run_count}")  # Add the new superpermutation
        layout_memory.add_sequence(superpermutation, N, N-2, f"run_{run_count}")

        # Update "Meta-Hierarchy" (simplified for this example)
        meta_hierarchy.setdefault("run_lengths", []).append(len(superpermutation))
        meta_hierarchy.setdefault("prodigal_counts", []).append(len(prodigal_results))
        total_hypotheticals = len(hypothetical_prodigals)
        successful_hypotheticals = 0
        for h_id, h_prodigal in hypothetical_prodigals.items():
            if h_prodigal.sequence in superpermutation:
               successful_hypotheticals += 1
        success_rate = ( successful_hypotheticals / total_hypotheticals) if total_hypotheticals > 0 else 0
        meta_hierarchy.setdefault("hypothetical_success_rate",[]).append(success_rate)


        end_time = time.time()
        print(f"  Run {run_count} completed in {end_time - start_time:.2f} seconds.")
        print(f"  Distinct 5906 superpermutations found so far: {distinct_count}")


        #Stopping Criteria
        if len(superpermutation) == 5906:
            # Check for early stopping based on lack of new distinct solutions
            is_new_solution = True
            for existing_sp in existing_superpermutations:
                if not analysis_scripts.is_cyclically_distinct(superpermutation, existing_sp):
                    is_new_solution = False
                    break

            if not is_new_solution:
                meta_hierarchy["no_new_solutions_count"] = meta_hierarchy.get("no_new_solutions_count",0) + 1
                if meta_hierarchy["no_new_solutions_count"] >= 20:
                    print("No new distinct 5906 solutions found in 20 iterations. Stopping.")
                    break
            else:
                meta_hierarchy["no_new_solutions_count"] = 0  # Reset counter
        if run_count >= 100:
            break


    layout_memory.save_to_file(layout_memory_file)

if __name__ == "__main__":
    random.seed(RANDOM_SEED)
    main()
```

### src/unified/embedder/cand_21_generation_code_n8_dynamic_py.py

```python
import itertools
import random
import math
import time
import networkx as nx
from collections import deque, defaultdict
import heapq

# --- Constants ---
N = 8  # The value of n (number of symbols).
PRODIGAL_OVERLAP_THRESHOLD = 0.98  # Initial minimum overlap rate for "Prodigal Results"
PRODIGAL_MIN_LENGTH = 50  # Initial minimum length for "Prodigal Results"
HYPOTHETICAL_PRODIGAL_OVERLAP_THRESHOLD = 0.95 # Initial minimum overlap for Hypothetical
HYPOTHETICAL_PRODIGAL_MIN_LENGTH = 20 # Initial minimum length for Hypothetical
HYPOTHETICAL_PRODIGAL_GENERATION_COUNT = 50 # Number of Hypothetical Prodigals to Generate
MEGA_HYPOTHETICAL_GENERATION_COUNT = 20 # Number of Mega-Hypotheticals to generate.
WINNER_THRESHOLD = 0.75 # What percentage of top winners to use.  Not directly used.
LOSER_THRESHOLD = 0.25 # What percentage of top losers to use.
NUM_ITERATIONS = 20  # Number of iterations for the main loop.
LAYOUT_K_VALUES = [N - 1, N - 2]  # k values for Layout Memory
DE_BRUIJN_K_VALUES = [N-1, N-2] # k values for De Bruijn graphs
RANDOM_SEED = 42  # For reproducibility
DE_ANCHOR_FREQUENCY = 0 # How often to temporarily remove the n=7 "Prodigal"
DE_ANCHOR_DURATION = 2 # How many iterations to keep the n=7 "Prodigal" removed
HIGH_ORDER_WINNER_LOSER_LENGTH = 2 # Length of sequences for higher order winner/loser

# --- Helper Functions ---

def calculate_overlap(s1: str, s2: str) -> int:
    """Calculates the maximum overlap between two strings."""
    max_overlap = 0
    for i in range(1, min(len(s1), len(s2)) + 1):
        if s1[-i:] == s2[:i]:
            max_overlap = i
    return max_overlap

def generate_permutations(n: int) -> list[tuple[int, ...]]:
    """Generates all permutations of 1 to n."""
    return list(itertools.permutations(range(1, n + 1)))

def calculate_distance(p1: str, p2: str, n: int) -> int:
    """Calculates distance (n-1 - overlap)."""
    return (n - 1) - max(calculate_overlap(p1, p2), calculate_overlap(p2, p1))

def hash_permutation(permutation: tuple) -> int:
    """Hashes a permutation tuple to a unique integer."""
    result = 0
    n = len(permutation)
    for i, val in enumerate(permutation):
        result += val * (n ** (n - 1 - i))
    return result

def unhash_permutation(hash_value: int, n: int) -> tuple:
    """Converts a hash value back to a permutation tuple."""
    permutation = []
    for i in range(n - 1, -1, -1):
        val = hash_value // (n ** i)
        permutation.append(val + 1)  # Adjust to be 1-indexed
        hash_value -= val * (n ** i)
    return tuple(permutation)

def is_valid_permutation(perm: tuple, n: int) -> bool:
    """Checks if a given sequence is a valid permutation."""
    return len(set(perm)) == n and min(perm) == 1 and max(perm) == n

def calculate_golden_ratio_points(length: int, levels: int = 1) -> list[int]:
    """Calculates multiple levels of golden ratio points."""
    phi = (1 + math.sqrt(5)) / 2
    points = []
    for _ in range(levels):
        new_points = []
        if not points:
            new_points = [int(length / phi), int(length - length / phi)]
        else:
            for p in points:
                new_points.extend([int(p / phi), int(p - p / phi)])
                new_points.extend([int((length-p) / phi) + p, int(length - (length-p) / phi)])
        points.extend(new_points)
        points = sorted(list(set(points)))  # Remove duplicates and sort
    return points

def get_kmers(sequence: str, n: int, k: int) -> set[str]:
    """Extracts all k-mers from a sequence, given n and k."""
    kmers = set()
    seq_list = [int(x) for x in sequence]
    for i in range(len(sequence) - n + 1):
        perm = tuple(seq_list[i:i+n])
        if is_valid_permutation(perm, n):
            if i >= k:
                kmer = "".join(str(x) for x in seq_list[i-k:i])
                kmers.add(kmer)
    return kmers
    
    # START SECTION: Data Structures

class PermutationData:
    def __init__(self, permutation: tuple, creation_method: str = ""):
        """
        Stores data associated with a single permutation.

        Args:
            permutation (tuple): The permutation as a tuple of integers (1-indexed).
            creation_method (str):  Describes how the permutation was generated
                                   (e.g., "prodigal_extension", "hypothetical_prodigal",
                                    "completion", "n7_generation").
        """
        self.hash: int = hash_permutation(permutation)
        self.permutation: tuple = permutation
        self.in_sample: bool = False # Always false
        self.used_count: int = 0  # How many times this permutation has been used
        self.prodigal_status: list[int] = []  # List of ProdigalResult IDs it belongs to
        self.creation_method: str = creation_method
        self.batch_ids: list[int] = []  # Removed
        self.used_in_final: bool = False  # True if in the current best superpermutation
        self.neighbors: set[int] = set()  # Set of hashes of neighboring permutations

    def __str__(self) -> str:
        """String representation for debugging."""
        return (f"PermutationData(hash={self.hash}, permutation={self.permutation}, used_count={self.used_count}, "
                f"prodigal_status={self.prodigal_status}, creation_method={self.creation_method})")

    def __repr__(self) -> str:
        return self.__str__()


class ProdigalResult:
    def __init__(self, sequence: str, result_id: int):
        """
        Represents a "Prodigal Result" - a highly efficient subsequence.

        Args:
            sequence (str): The superpermutation sequence (as a string of digits).
            result_id (int): A unique ID for this "Prodigal Result."
        """
        self.id: int = result_id
        self.sequence: str = sequence
        self.length: int = len(sequence)
        self.permutations: set[int] = set()  # Store hashes of permutations
        self.calculate_permutations()  # Calculate on creation
        self.overlap_rate: float = self.calculate_overlap_rate()

    def calculate_permutations(self):
        """Calculates and stores the set of permutations contained in the sequence."""
        n = N  # Use the global N value
        for i in range(len(self.sequence) - n + 1):
            perm = tuple(int(x) for x in self.sequence[i:i + n])
            if is_valid_permutation(perm, n):
                self.permutations.add(hash_permutation(perm))

    def calculate_overlap_rate(self) -> float:
        """Calculates the overlap rate of the sequence."""
        n = N  # Use the global N value
        total_length = sum(len(str(p)) for p in [unhash_permutation(x,n) for x in self.permutations])
        overlap_length = total_length - len(self.sequence)
        max_possible_overlap = (len(self.permutations) - 1) * (n - 1)
        if max_possible_overlap == 0:
            return 0  # Avoid division by zero
        return overlap_length / max_possible_overlap

    def __str__(self) -> str:
        """String representation for debugging."""
        return (f"ProdigalResult(id={self.id}, length={self.length}, "
                f"overlap_rate={self.overlap_rate:.4f}, num_permutations={len(self.permutations)})")

    def __repr__(self) -> str:
        return self.__str__()

# END SECTION: Data Structures

# START SECTION: Initialization and On-Demand Generation
def initialize_data(initial_n7_superpermutation:str, initial_n8_prodigals: list[str],
                    initial_winners: dict, initial_losers: dict) -> tuple:
    """Initializes the data structures for the algorithm.

    Args:
        initial_n7_superpermutation: String of the n=7 superpermutation.
        initial_n8_prodigals: A list of initial n=8 Prodigal Result strings.
        initial_winners: A dictionary of initial Winner k-mers and weights.
        initial_losers: A dictionary of initial Loser k-mers and weights.

    Returns:
        tuple: (prodigal_results, winners, losers, meta_hierarchy, limbo_list, eput, next_prodigal_id)
    """
    prodigal_results = {}  # {prodigal_id: ProdigalResult object}
    winners = initial_winners  # {kmer: weight}
    losers = initial_losers  # {kmer: weight}
    meta_hierarchy = {}  # Track strategy effectiveness
    limbo_list = set()  # Set of permutation hashes.
    eput = {}  # The Enhanced Permutation Universe Tracker

    # Add the n=7 superpermutation as a Prodigal Result
    prodigal_results[0] = ProdigalResult(initial_n7_superpermutation, 0)
    next_prodigal_id = 1

    # Add initial n=8 prodigals
    for prodigal in initial_n8_prodigals:
        prodigal_results[next_prodigal_id] = ProdigalResult(prodigal, next_prodigal_id)
        next_prodigal_id += 1

    return prodigal_results, winners, losers, meta_hierarchy, limbo_list, eput, next_prodigal_id

def generate_permutations_on_demand(prefix: str, suffix: str, prodigal_results: dict, winners: dict, losers: dict,
                                   n: int, eput: dict, limbo_list: set, min_overlap: int,
                                   hypothetical_prodigals: dict = None) -> set[int]:
    """Generates permutations on demand, extending a given prefix or suffix.
       Prioritizes "Prodigal" extensions, uses "Winners" and "Losers," and
       avoids already-used permutations and the "Limbo List."

    Args:
        prefix (str): The prefix of the current superpermutation.
        suffix (str): The suffix of the current superpermutation.
        prodigal_results (dict): Dictionary of ProdigalResult objects.
        winners (dict): Dictionary of Winner k-mers and their weights (from WL_PUT).
        losers (dict): Dictionary of Loser k-mers and their weights (from WL_PUT).
        n (int): The value of n.
        eput (dict): The Enhanced Permutation Universe Tracker.
        limbo_list (set): The set of permutation hashes to avoid.
        min_overlap (int):  The minimum required overlap.
        hypothetical_prodigals (dict): Optional dictionary of Hypothetical Prodigals

    Returns:
        set[int]: A set of *permutation hashes* that are potential extensions.
    """
    valid_permutations = set()

    # Prioritize Hypothetical Prodigal extensions
    if hypothetical_prodigals:
        for prodigal_id, prodigal in hypothetical_prodigals.items():
            if prefix and prodigal.sequence.startswith(prefix[-min_overlap:]):
                for i in range(len(prodigal.sequence) - (n - 1)):
                    perm = tuple(int(x) for x in prodigal.sequence[i:i + n])
                    if is_valid_permutation(perm, n) and hash_permutation(perm) not in eput:
                        valid_permutations.add(hash_permutation(perm))
            if suffix and prodigal.sequence.endswith(suffix[:min_overlap]):
                for i in range(len(prodigal.sequence) - (n - 1)):
                    perm = tuple(int(x) for x in prodigal.sequence[i:i + n])
                    if is_valid_permutation(perm, n) and hash_permutation(perm) not in eput:
                        valid_permutations.add(hash_permutation(perm))

    # Prioritize Prodigal extensions
    for prodigal_id, prodigal in prodigal_results.items():
        if prefix and prodigal.sequence.startswith(prefix[-min_overlap:]):
            for i in range(len(prodigal.sequence) - (n - 1)):
                perm = tuple(int(x) for x in prodigal.sequence[i:i + n])
                if is_valid_permutation(perm, n) and hash_permutation(perm) not in eput:
                    valid_permutations.add(hash_permutation(perm))
        if suffix and prodigal.sequence.endswith(suffix[:min_overlap]):
            for i in range(len(prodigal.sequence) - (n - 1)):
                perm = tuple(int(x) for x in prodigal.sequence[i:i + n])
                if is_valid_permutation(perm, n) and hash_permutation(perm) not in eput:
                    valid_permutations.add(hash_permutation(perm))

    # Filter based on Limbo List and create final set of hashes
    filtered_permutations = set()
    for perm_hash in valid_permutations:
        perm_str = "".join(str(x) for x in unhash_permutation(perm_hash, n))
        is_in_limbo = False

        # Basic Loser check (using 7-mers and 6-mers for n=8)
        for k in [7, 6]:  # Check 7-mers and 6-mers
            for i in range(len(perm_str) - k + 1):
                kmer = perm_str[i:i+k]
                if kmer in limbo_list:
                    is_in_limbo = True
                    break
            if is_in_limbo:
                break

        if not is_in_limbo:
            filtered_permutations.add(perm_hash)

    return filtered_permutations
    
    # START SECTION: Scoring and Construction
def calculate_score(current_superpermutation: str, permutation_hash: int, prodigal_results: dict,
                    winners: dict, losers: dict, layout_memory: LayoutMemory, n: int,
                    golden_ratio_points: list[int], hypothetical_prodigals: dict = None) -> float:
    """Calculates the score for adding a permutation to the current superpermutation.

    Args:
        current_superpermutation (str): The current superpermutation string.
        permutation_hash (int): The hash of the candidate permutation.
        prodigal_results (dict): Dictionary of ProdigalResult objects.
        winners (dict): Dictionary of Winner k-mers and their weights (from WL_PUT).
        losers (dict): Dictionary of Loser k-mers and their weights (from WL_PUT).
        layout_memory (LayoutMemory): The LayoutMemory object.
        n (int): The value of n.
        golden_ratio_points (list): List of golden ratio points.
        hypothetical_prodigals(dict): The dictionary of Hypothetical Prodigals

    Returns:
        float: The score for the candidate permutation. Higher is better.
    """
    permutation = unhash_permutation(permutation_hash, n)
    permutation_string = "".join(str(x) for x in permutation)
    overlap = calculate_overlap(current_superpermutation, permutation_string)
    score = overlap * 5  # Base score based on overlap

    # Prodigal Result Bonus (very high)
    prodigal_bonus = 0
    for prodigal_id, prodigal in prodigal_results.items():
        if permutation_string in prodigal.sequence:
            prodigal_bonus += prodigal.length * 100  # Large bonus
            break  # Only check if contained, not full extension

    # --- Hypothetical Prodigal Bonus ---
    hypothetical_bonus = 0
    if hypothetical_prodigals:
        for h_prodigal_id, h_prodigal in hypothetical_prodigals.items():
            if permutation_string in h_prodigal.sequence:
                hypothetical_bonus += h_prodigal.length * 25 #Smaller bonus

    # Winner and Loser Bonus/Penalty (using Layout Memory, reduced influence)
    k_values = [n - 1, n - 2]
    layout_bonus = 0
    for k in k_values:
        if len(current_superpermutation) >= k:
            kmer_end = current_superpermutation[-k:]
            kmer_start = permutation_string[:k]
            layout_bonus += layout_memory.get_layout_score(kmer_end, kmer_start, 1) # Check for adjacency

    # Golden Ratio Bonus (small, dynamic)
    golden_ratio_bonus = 0
    insertion_point = len(current_superpermutation)
    for point in golden_ratio_points:
        distance = abs(insertion_point - point)
        golden_ratio_bonus += math.exp(-distance / (len(current_superpermutation)/20))  # Smaller divisor = tighter to golden ratio

    # Loser Penalty (Veto)
    loser_penalty = 0
    for k in [7, 6]:  # Check 7-mers and 6-mers for n=8
        for i in range(len(permutation_string) - k + 1):
            kmer = permutation_string[i:i+k]
            loser_penalty += losers.get(kmer, 0) * 5 # Penalty for losers

    # Higher-Order Winners/Losers (sequences of permutations)
    higher_order_bonus = 0
    for seq_length in [2, 3]:  # Check sequences of length 2 and 3
      if len(current_superpermutation) >= (n * seq_length):
        prev_seq = current_superpermutation[-(n*seq_length):]
        prev_perms = []
        for i in range(len(prev_seq) - n + 1):
            pp = tuple(int(x) for x in prev_seq[i:i+n])
            if is_valid_permutation(pp, n):
                prev_perms.append(hash_permutation(pp))
        if len(prev_perms) >= (seq_length -1):
            current_seq = tuple(prev_perms[-(seq_length - 1):] + [permutation_hash])
            current_seq_hash = hash(current_seq)
            higher_order_bonus += winners.get(current_seq_hash, 0) * 5 #* (sequence_length-1)  # Bonus for winner sequences
            loser_penalty += losers.get(current_seq_hash, 0) * 5

    score += prodigal_bonus + layout_bonus + golden_ratio_bonus + hypothetical_bonus + higher_order_bonus - loser_penalty

    return score
def construct_superpermutation(initial_permutations: list, prodigal_results: dict, winners: dict, losers: dict,
                              layout_memory: LayoutMemory, meta_hierarchy: dict, limbo_list: set, n: int,
                              hypothetical_prodigals: dict) -> tuple[str, set[int]]:
    """Constructs a superpermutation using the dynamic prodigal approach."""

    superpermutation = ""
    used_permutations = set()

    # Initialize with the longest "Prodigal Result" if possible
    if prodigal_results:
        best_prodigal_key = max(prodigal_results, key=lambda k: prodigal_results[k].length)
        superpermutation = prodigal_results[best_prodigal_key].sequence
        used_permutations.update(prodigal_results[best_prodigal_key].permutations)
    else:
        # Fallback (should not normally happen with n=8)
        # In a true emergency, generate a single valid permutation
        superpermutation = "".join(str(x) for x in range(1, n + 1))
        used_permutations.add(hash_permutation(tuple(range(1, n + 1))))


    golden_ratio_points = calculate_golden_ratio_points(len(superpermutation), levels=3)

    while True:  # Continue until no more additions can be made
        best_candidate = None
        best_score = -float('inf')
        best_candidate_string = ""

        # Find frontier k-mers
        prefix = superpermutation[:n - 1]
        suffix = superpermutation[-(n - 1):]

        # Generate candidates (very small set, focused on the frontier)
        candidates = generate_permutations_on_demand(prefix, suffix, prodigal_results, winners, losers, n, used_permutations, limbo_list, n - 1, hypothetical_prodigals)
        if not candidates:
            candidates = generate_permutations_on_demand(prefix, suffix, prodigal_results, winners, losers, n, used_permutations, limbo_list, n - 2, hypothetical_prodigals)
        if not candidates:
            # print("No candidates found. Stopping.")  # Keep for debugging
            break  # No more candidates can be added

        #Deterministic selection from candidates:
        scored_candidates = []
        for candidate_hash in candidates:
            score = calculate_score(superpermutation, candidate_hash, prodigal_results, winners, losers, layout_memory, n, golden_ratio_points, hypothetical_prodigals)
            scored_candidates.append((score, candidate_hash))
        
        best_candidate = None
        if scored_candidates:
            scored_candidates.sort(reverse=True, key=lambda item: item[0]) #Sort by score, DESCENDING
            best_candidate = scored_candidates[0][1] #Get the hash

        if best_candidate is not None:
            best_candidate_perm = unhash_permutation(best_candidate, n)
            overlap = calculate_overlap(superpermutation, "".join(str(x) for x in best_candidate_perm))
            superpermutation += "".join(str(x) for x in best_candidate_perm)[overlap:]  # Add to superpermutation
            used_permutations.add(best_candidate)

            # Update golden ratio points
            golden_ratio_points = calculate_golden_ratio_points(len(superpermutation))

            # Prodigal Result Check
            new_prodigals = analysis_scripts.find_prodigal_results(superpermutation, n, min_length=PRODIGAL_MIN_LENGTH, overlap_threshold=PRODIGAL_OVERLAP_THRESHOLD)  # Use analysis script.
            for prodigal_seq in new_prodigals:
                is_new = True
                for existing_prodigal_id, existing_prodigal in prodigal_results.items():
                    if prodigal_seq in existing_prodigal.sequence:
                        is_new = False
                        break
                if is_new:
                    new_id = len(prodigal_results) + 1
                    prodigal_results[new_id] = ProdigalResult(prodigal_seq, new_id)
                    #print(f"New Prodigal Result found: {prodigal_seq}")

            # Update ePUT
            perm_hash = best_candidate
            perm_string = "".join(str(x) for x in best_candidate_perm)
            if perm_hash not in eput:  # Should always be the case, but check anyway
                eput[perm_hash] = PermutationData(best_candidate_perm, in_sample=False, creation_method="dynamic_generation")
            eput[perm_hash].used_count += 1
            eput[perm_hash].used_in_final = True  # Mark as used in this iteration's superpermutation
            # Update neighbors in ePUT (using consistent k values)
            for k in [n - 1, n - 2]:
                prefix = superpermutation[:k]
                suffix = superpermutation[-k:]
                prefix_perms = set()
                suffix_perms = set()
                for i in range(len(prefix) - n + 1):
                    p = tuple(int(x) for x in prefix[i:i+n])
                    if is_valid_permutation(p,n):
                        prefix_perms.add(hash_permutation(p))
                for i in range(len(suffix) - n + 1):
                    p = tuple(int(x) for x in suffix[i:i+n])
                    if is_valid_permutation(p,n):
                        suffix_perms.add(hash_permutation(p))

                for other_perm_hash in prefix_perms:
                    if other_perm_hash != perm_hash:
                        eput[perm_hash].neighbors.add(other_perm_hash)
                        # Add to layout memory if not exist
                        kmer1 = "".join(str(x) for x in unhash_permutation(other_perm_hash, n)[-k:])
                        kmer2 = perm_string[:k]
                        layout_memory.update_distances(kmer1,kmer2, len(superpermutation) - i - len(prefix), "n8_dynamic") #Add source to layout memory
                for other_perm_hash in suffix_perms:
                    if other_perm_hash != perm_hash:
                        eput[perm_hash].neighbors.add(other_perm_hash)
                        kmer1 = perm_string[-k:]
                        kmer2 = "".join(str(x) for x in unhash_permutation(other_perm_hash, n)[:k])
                        layout_memory.update_distances(kmer1, kmer2, 1, "n8_dynamic") #Add source to layout memory.

        else:
            break  # If we get here, we are stuck.

    return superpermutation, used_permutations
    
    # START SECTION: Main Function
def main():
    """
    Main function to execute the Dynamic Prodigal Assembly algorithm for n=8.
    """
    n = 8
    num_iterations = 20  #  Adjust as needed.

    # --- File Paths (IMPORTANT: Adjust these paths as needed) ---
    initial_winners_losers_n7_file = "initial_winners_losers_n7.txt"
    prodigal_results_n7_file = "prodigal_results_n7.txt"
    prodigal_results_n8_file = "prodigal_results_n8.txt"
    winners_losers_data_n8_file = "winners_losers_data_n8.txt"
    best_superpermutation_file = "superpermutation_n8_best_dynamic.txt"  # Store the best result
    layout_memory_file = "layout_memory_n8.pkl" # Using pickle for objects.
    distinct_n7_file = "distinct_superpermutations_n7.txt"

    # --- Load Initial Data ---
    initial_winners, initial_losers = {}, {}
    try:
        with open(initial_winners_losers_n7_file, "r") as f:
            for line in f:
                kmer, w_type, weight = line.strip().split(",")
                if w_type == "winner":
                    initial_winners[kmer] = int(weight)
                else:
                    initial_losers[kmer] = int(weight)
    except FileNotFoundError:
        print(f"Initial Winners/Losers file ({initial_winners_losers_n7_file}) not found. Starting with empty.")

    initial_n7_prodigals = []
    try:
        with open(prodigal_results_n7_file, "r") as f:
            #initial_n7_prodigals = [line.strip() for line in f] #Original
            # MODIFICATION: Load *all* distinct n=7 superpermutations
            for line in f:
                initial_n7_prodigals.append(line.strip())

    except FileNotFoundError:
        print(f"Initial n=7 Prodigal Results file ({prodigal_results_n7_file}) not found.  Starting with empty.")


    initial_n8_prodigals = []
    try:
        with open(prodigal_results_n8_file, "r") as f:
            initial_n8_prodigals = [line.strip() for line in f]
    except FileNotFoundError:
        print(f"Initial n=8 Prodigal Results file ({prodigal_results_n8_file}) not found.  Starting with empty.")
    

    initial_n8_winners, initial_n8_losers = {}, {}
    try:
        with open(winners_losers_data_n8_file, 'r') as f:
            for line in f:
                kmer, w_type, weight = line.strip().split(",")
                if w_type == 'winner':
                    initial_n8_winners[kmer] = int(weight)
                elif w_type == 'loser':
                    initial_n8_losers[kmer] = int(weight)
    except FileNotFoundError:
        print("Initial n=8 Winners/Losers data not found. Starting with empty.")


    # Combine for initial list, but keep n=7 separate for laminate creation.
    initial_prodigals = initial_n8_prodigals

    # --- Initialize Data Structures ---
    prodigal_results, winners, losers, meta_hierarchy, limbo_list, eput, next_prodigal_id = initialize_data(
        "", initial_prodigals, initial_winners, initial_losers
    )  # Start with EMPTY string

    #Add initial n=8 winners/losers
    for kmer, weight in initial_n8_winners.items():
        winners[kmer] = winners.get(kmer, 0) + weight
    for kmer, weight in initial_n8_losers.items():
        losers[kmer] = losers.get(kmer, 0) + weight

    # --- Load Best Superpermutation (if it exists) ---
    best_superpermutation = ""
    try:
        with open(best_superpermutation_file, "r") as f:
            best_superpermutation = f.read().strip()
            print(f"Loaded initial best superpermutation of length: {len(best_superpermutation)}")
    except FileNotFoundError:
        print("No existing best superpermutation file found. Starting fresh.")

    # --- Create Initial Laminate (from n=7 AND n=8 Prodigals) ---
    laminates = []
    # Create laminates from the distinct n=7 superpermutations
    for n7_sp in initial_n7_prodigals:
      laminates.append(analysis_scripts.create_laminate(n7_sp, 7, 6))
      laminates.append(analysis_scripts.create_laminate(n7_sp, 7, 5))
    # Create laminates from the initial n=8 prodigals
    for n8_prodigal_seq in initial_n8_prodigals:
        laminates.append(analysis_scripts.create_laminate(n8_prodigal_seq, 8, 7))
        laminates.append(analysis_scripts.create_laminate(n8_prodigal_seq, 8, 6))


    #Load layout memory
    layout_memory = LayoutMemory()
    try:
        layout_memory.load_from_file(layout_memory_file)
        print("Loaded LayoutMemory from file.")
    except FileNotFoundError:
        print("No existing LayoutMemory file found. Starting with a new one.")

    # --- Main Iterative Loop ---
    for iteration in range(NUM_ITERATIONS):
        print(f"Starting iteration {iteration + 1}...")
        start_time = time.time()

        # 1. Generate Hypothetical Prodigals (now done *before* construction)
        hypothetical_prodigals = analysis_scripts.generate_hypothetical_prodigals(prodigal_results, winners, losers, n)
        print(f"  Generated {len(hypothetical_prodigals)} hypothetical prodigals.")
        
        #Add to the prodigals list
        for h_id, h_prodigal in hypothetical_prodigals.items():
            prodigal_results[next_prodigal_id] = h_prodigal
            next_prodigal_id += 1

        # 2. Construct Superpermutation (using the dynamic approach)
        superpermutation, used_permutations = construct_superpermutation([], prodigal_results, winners, losers, layout_memory, meta_hierarchy, limbo_list, n, hypothetical_prodigals)
        print(f"  Superpermutation length: {len(superpermutation)}")

        # 3. Update Data
        analysis_results = analysis_scripts.analyze_superpermutation(superpermutation, n)  # Use analysis script
        print(f"  Valid: {analysis_results['validity']}") #Will likely be False until completion
        print(f"  Overlap Distribution: {analysis_results['overlap_distribution']}")

        # Update best superpermutation (if this one is shorter)
        if len(superpermutation) < len(best_superpermutation) or not best_superpermutation:
            best_superpermutation = superpermutation

        # Find and add new prodigal results.  Use stricter criteria.
        new_prodigals = analysis_scripts.find_prodigal_results(superpermutation, n, min_length=PRODIGAL_MIN_LENGTH, overlap_threshold=PRODIGAL_OVERLAP_THRESHOLD)
        for prodigal_seq in new_prodigals:
            is_new = True
            for existing_prodigal_id, existing_prodigal in prodigal_results.items():
                if prodigal_seq in existing_prodigal.sequence:
                    is_new = False
                    break
            if is_new:
                prodigal_results[next_prodigal_id] = ProdigalResult(prodigal_seq, next_prodigal_id)
                next_prodigal_id += 1
                # Create and add a laminate for the new prodigal
                laminates.append(analysis_scripts.create_laminate(prodigal_seq, n, n-1))
                laminates.append(analysis_scripts.create_laminate(prodigal_seq, n, n-2))


        # Update "Winners" and "Losers" (using the new superpermutation, and k=7 and k=8)
        new_winners7, new_losers7 = analysis_scripts.calculate_winners_losers([superpermutation], n, k=7)
        new_winners8, new_losers8 = analysis_scripts.calculate_winners_losers([superpermutation], n, k=8)

        for kmer, weight in new_winners7.items():
            winners[kmer] = winners.get(kmer, 0) + weight
        for kmer, weight in new_losers7.items():
            losers[kmer] = losers.get(kmer, 0) + weight
            if losers[kmer] > 5: # Add to limbo list with a threshold.
                limbo_list.add(kmer)
        for kmer, weight in new_winners8.items():
            winners[kmer] = winners.get(kmer, 0) + weight
        for kmer, weight in new_losers8.items():
            losers[kmer] = losers.get(kmer, 0) + weight
            if losers[kmer] > 5:
                limbo_list.add(kmer) # Add to limbo list

        #Higher Order Winners and Losers
        new_seq_winners, new_seq_losers = analysis_scripts.calculate_sequence_winners_losers([superpermutation],n)
        for seq_hash, weight in new_seq_winners.items():
            if seq_hash in winners:
                winners[seq_hash] += weight
            else:
                winners[seq_hash] = weight
        for seq_hash, weight in new_seq_losers.items():
            if seq_hash in losers:
                losers[seq_hash] += weight
            else:
                losers[seq_hash] = weight
                #Don't add to limbo list, too restrictive

        # Update ePUT (add all *used* permutations)
        s_tuple = tuple(int(x) for x in superpermutation)
        for i in range(len(s_tuple) - n + 1):
            perm = s_tuple[i:i+n]
            if is_valid_permutation(perm, n):
                perm_hash = hash_permutation(perm)
                if perm_hash not in eput:
                    eput[perm_hash] = PermutationData(perm, in_sample=False, creation_method="dynamic_generation") # All are now dynamically generated
                eput[perm_hash].used_count += 1
                eput[perm_hash].used_in_final = True  # Mark as used in this iteration
                # Update neighbors in ePUT (using consistent k values)
                if i > 0:
                    prev_perm = s_tuple[i-1:i-1+n]
                    if is_valid_permutation(prev_perm, n):
                         eput[perm_hash].neighbors.add(hash_permutation(prev_perm))
                if i < len(s_tuple) - n:
                    next_perm = s_tuple[i+1:i+1+n]
                    if is_valid_permutation(next_perm, n):
                        eput[perm_hash].neighbors.add(hash_permutation(next_perm))
        # Update Layout Memory
        layout_memory.add_sequence(superpermutation, n, 7, f"run_{iteration}")  # Add the new superpermutation
        layout_memory.add_sequence(superpermutation, n, 6, f"run_{iteration}")

        # Update "Meta-Hierarchy" (simplified for this example)
        meta_hierarchy.setdefault("run_lengths", []).append(len(superpermutation))
        meta_hierarchy.setdefault("prodigal_counts", []).append(len(prodigal_results))
        #Calculate hypothetical success rate:
        total_hypotheticals = len(hypothetical_prodigals)
        successful_hypotheticals = 0
        for h_id, h_prodigal in hypothetical_prodigals.items():
            if h_prodigal.sequence in superpermutation:
                successful_hypotheticals += 1
        success_rate = (successful_hypotheticals / total_hypotheticals) if total_hypotheticals > 0 else 0
        meta_hierarchy.setdefault("hypothetical_success_rate",[]).append(success_rate)

        # --- Dynamic Parameter Adjustments (Example) ---
        if len(meta_hierarchy["prodigal_counts"]) > 2 and meta_hierarchy["prodigal_counts"][-1] <= meta_hierarchy["prodigal_counts"][-2]:
            # Prodigal discovery rate is slowing down
            PRODIGAL_OVERLAP_THRESHOLD = max(0.90, PRODIGAL_OVERLAP_THRESHOLD - 0.01)  # Decrease threshold, but not below 0.90
            PRODIGAL_MIN_LENGTH = max(20, PRODIGAL_MIN_LENGTH - 5) # Reduce Min Length
            print(f"  Adjusted Prodigal Criteria: Overlap Threshold = {PRODIGAL_OVERLAP_THRESHOLD}, Min Length = {PRODIGAL_MIN_LENGTH}")

        # Print Result and Time
        end_time = time.time()
        print(f"  Iteration {iteration + 1} completed in {end_time - start_time:.2f} seconds.")


    print(f"Final Superpermutation Length (Best): {len(best_superpermutation)}")

    # --- Save Results (IMPORTANT) ---
    with open(best_superpermutation_file, "w") as f:
        f.write(best_superpermutation)
    with open(winners_losers_data_n8_file, "w") as f:
        for kmer, weight in winners.items():
            f.write(f"{kmer},winner,{weight}\n")
        for kmer, weight in losers.items():
            f.write(f"{kmer},loser,{weight}\n")  # Loser weights should always be positive
    with open(prodigal_results_n8_file, "w") as f:
        for prodigal_id, prodigal in prodigal_results.items():
            f.write(f"{prodigal.sequence}\n")
    layout_memory.save_to_file(layout_memory_file)

if __name__ == "__main__":
    random.seed(RANDOM_SEED)  # Set the random seed for reproducibility
    main()
```

### src/unified/embedder/cand_25_Hash_testsuite_py.py

```python
import time
import random
import matplotlib.pyplot as plt
import numpy as np
from typing import Dict, List, Any, Tuple

class HashTableBenchmark:
    """Benchmarking suite for hash table implementations."""
    
    def __init__(self):
        self.results = {}
        
    def run_standard_tests(self, hash_table, traditional_dict=None, n_operations=100000):
        """
        Run standard benchmark tests on hash tables.
        
        Args:
            hash_table: Hash table implementation to test
            traditional_dict: Optional traditional dict for comparison
            n_operations: Number of operations to perform
        
        Returns:
            Dictionary of benchmark results
        """
        print("Running standard distribution test...")
        std_results = self._test_standard_distribution(hash_table, traditional_dict, n_operations)
        
        print("Running skewed distribution test...")
        skew_results = self._test_skewed_distribution(hash_table, traditional_dict, n_operations)
        
        print("Running collision resistance test...")
        collision_results = self._test_collision_resistance(hash_table, traditional_dict, n_operations // 10)
        
        print("Running sequential access test...")
        sequential_results = self._test_sequential_access(hash_table, traditional_dict, n_operations)
        
        print("Running load factor scaling test...")
        load_factor_results = self._test_load_factor_scaling(hash_table, traditional_dict)
        
        # Collect all results
        results = {
            'standard': std_results,
            'skewed': skew_results,
            'collision': collision_results,
            'sequential': sequential_results,
            'load_factor': load_factor_results
        }
        
        self.results = results
        return results
        
    def _test_standard_distribution(self, hash_table, traditional_dict, n_operations):
        """Test with standard uniform distribution of keys."""
        # Generate random key-value pairs
        keys = [random.randint(0, n_operations * 10) for _ in range(n_operations)]
        values = [f"value_{i}" for i in range(n_operations)]
        
        # Test MDHG hash table
        hash_table_times = self._measure_operations(hash_table, keys, values)
        
        # Test traditional dict if provided
        dict_times = None
        if traditional_dict is not None:
            dict_times = self._measure_operations(traditional_dict, keys, values)
        
        return {
            'hash_table_times': hash_table_times,
            'dict_times': dict_times,
            'keys': keys,
            'values': values
        }
    
    def _test_skewed_distribution(self, hash_table, traditional_dict, n_operations):
        """Test with skewed distribution (some keys more frequent than others)."""
        # Generate skewed key distribution (80% of operations use 20% of key space)
        hot_keys = [random.randint(0, n_operations // 5) for _ in range(n_operations // 5)]
        
        keys = []
        for _ in range(n_operations):
            if random.random() < 0.8:
                # Use a hot key
                keys.append(random.choice(hot_keys))
            else:
                # Use a random key
                keys.append(random.randint(0, n_operations * 10))
        
        values = [f"value_{i}" for i in range(n_operations)]
        
        # Test MDHG hash table
        hash_table_times = self._measure_operations(hash_table, keys, values)
        
        # Test traditional dict if provided
        dict_times = None
        if traditional_dict is not None:
            dict_times = self._measure_operations(traditional_dict, keys, values)
        
        return {
            'hash_table_times': hash_table_times,
            'dict_times': dict_times,
            'hot_keys': hot_keys,
            'keys': keys,
            'values': values
        }
    
    def _test_collision_resistance(self, hash_table, traditional_dict, n_operations):
        """Test resistance to hash collisions."""
        # Generate keys designed to cause collisions
        # For example, strings with similar patterns
        keys = [f"collision{i % 100}-{i}" for i in range(n_operations)]
        values = [f"value_{i}" for i in range(n_operations)]
        
        # Test MDHG hash table
        hash_table_times = self._measure_operations(hash_table, keys, values)
        
        # Test traditional dict if provided
        dict_times = None
        if traditional_dict is not None:
            dict_times = self._measure_operations(traditional_dict, keys, values)
        
        return {
            'hash_table_times': hash_table_times,
            'dict_times': dict_times,
            'keys': keys,
            'values': values
        }
    
    def _test_sequential_access(self, hash_table, traditional_dict, n_operations):
        """Test with sequential access patterns."""
        # First insert some data
        keys = [i for i in range(n_operations // 10)]
        values = [f"value_{i}" for i in range(n_operations // 10)]
        
        for i in range(len(keys)):
            hash_table.put(keys[i], values[i])
            if traditional_dict is not None:
                traditional_dict[keys[i]] = values[i]
        
        # Generate sequential access patterns (e.g., iterating through keys in sequence)
        sequential_keys = []
        for _ in range(n_operations):
            # Create sequences of 5-10 sequential keys
            start = random.randint(0, len(keys) - 10)
            length = random.randint(5, 10)
            sequential_keys.extend(keys[start:start + length])
        
        # Test MDHG hash table (gets only)
        start_time = time.time()
        for key in sequential_keys:
            hash_table.get(key)
        hash_table_time = time.time() - start_time
        
        # Test traditional dict if provided
        dict_time = None
        if traditional_dict is not None:
            start_time = time.time()
            for key in sequential_keys:
                traditional_dict.get(key)
            dict_time = time.time() - start_time
        
        return {
            'hash_table_time': hash_table_time,
            'dict_time': dict_time,
            'sequential_keys': sequential_keys
        }
    
    def _test_load_factor_scaling(self, hash_table, traditional_dict):
        """Test how performance scales with increasing load factor."""
        load_factors = [0.1, 0.3, 0.5, 0.7, 0.9]
        base_capacity = 1000
        
        hash_table_results = []
        dict_results = []
        
        for load_factor in load_factors:
            # Calculate number of elements to insert
            n_elements = int(base_capacity * load_factor)
            
            # Create new hash table with fixed capacity
            test_hash_table = MDHGHashTable(capacity=base_capacity)
            test_dict = {} if traditional_dict is not None else None
            
            # Insert elements
            keys = [i for i in range(n_elements)]
            values = [f"value_{i}" for i in range(n_elements)]
            
            for i in range(n_elements):
                test_hash_table.put(keys[i], values[i])
                if test_dict is not None:
                    test_dict[keys[i]] = values[i]
            
            # Measure get performance
            test_keys = random.sample(keys, min(100, n_elements))
            
            start_time = time.time()
            for key in test_keys:
                test_hash_table.get(key)
            hash_table_time = time.time() - start_time
            
            dict_time = None
            if test_dict is not None:
                start_time = time.time()
                for key in test_keys:
                    test_dict.get(key)
                dict_time = time.time() - start_time
            
            hash_table_results.append(hash_table_time)
            dict_results.append(dict_time)
        
        return {
            'load_factors': load_factors,
            'hash_table_times': hash_table_results,
            'dict_times': dict_results
        }
    
    def _measure_operations(self, container, keys, values):
        """
        Measure time for put, get, and remove operations.
        
        Args:
            container: Hash table or dict to test
            keys: List of keys to use
            values: List of values to use
            
        Returns:
            Dictionary of operation times
        """
        n_operations = len(keys)
        
        # Measure put performance
        start_time = time.time()
        for i in range(n_operations):
            if hasattr(container, 'put'):
                container.put(keys[i], values[i])
            else:
                container[keys[i]] = values[i]
        put_time = time.time() - start_time
        
        # Measure get performance
        start_time = time.time()
        for i in range(n_operations):
            if hasattr(container, 'get'):
                container.get(keys[i])
            else:
                container.get(keys[i])
        get_time = time.time() - start_time
        
        # Measure remove performance (sample)
        sample_size = min(1000, n_operations)
        remove_keys = random.sample(keys, sample_size)
        
        start_time = time.time()
        for key in remove_keys:
            if hasattr(container, 'remove'):
                container.remove(key)
            else:
                container.pop(key, None)
        remove_time = time.time() - start_time
        
        # Scale remove time to estimate full removal time
        remove_time = remove_time * (n_operations / sample_size)
        
        return {
            'put': put_time,
            'get': get_time,
            'remove': remove_time,
            'total': put_time + get_time + remove_time
        }
    
    def plot_results(self):
        """Plot benchmark results."""
        if not self.results:
            print("No results to plot. Run benchmarks first.")
            return
        
        # Create figure with subplots
        fig, axs = plt.subplots(2, 2, figsize=(15, 12))
        
        # Plot standard distribution results
        self._plot_operation_times(axs[0, 0], 'Standard Distribution')
        
        # Plot skewed distribution results
        self._plot_operation_times(axs[0, 1], 'Skewed Distribution')
        
        # Plot collision resistance results
        self._plot_operation_times(axs[1, 0], 'Collision Resistance')
        
        # Plot load factor scaling
        self._plot_load_factor_scaling(axs[1, 1])
        
        # Adjust layout and show
        plt.tight_layout()
        plt.show()
    
    def _plot_operation_times(self, ax, title):
        """Plot operation times for a specific test."""
        test_key = title.lower().replace(' ', '_')
        if test_key not in self.results:
            return
        
        result = self.results[test_key]
        
        if 'hash_table_times' not in result or 'dict_times' not in result:
            return
        
        hash_times = result['hash_table_times']
        dict_times = result['dict_times']
        
        if not hash_times or not dict_times:
            return
        
        operations = ['put', 'get', 'remove', 'total']
        hash_values = [hash_times[op] for op in operations]
        dict_values = [dict_times[op] for op in operations]
        
        x = np.arange(len(operations))
        width = 0.35
        
        ax.bar(x - width/2, hash_values, width, label='MDHG Hash')
        ax.bar(x + width/2, dict_values, width, label='Dict')
        
        ax.set_title(title)
        ax.set_xticks(x)
        ax.set_xticklabels(operations)
        ax.set_ylabel('Time (seconds)')
        ax.legend()
        
        # Add speedup annotations
        for i, (hash_val, dict_val) in enumerate(zip(hash_values, dict_values)):
            speedup = dict_val / hash_val if hash_val > 0 else 0
            ax.annotate(f'{speedup:.2f}x', 
                       (x[i], max(hash_val, dict_val) * 1.05),
                       ha='center')
    
    def _plot_load_factor_scaling(self, ax):
        """Plot performance scaling with load factor."""
        if 'load_factor' not in self.results:
            return
        
        result = self.results['load_factor']
        
        load_factors = result['load_factors']
        hash_times = result['hash_table_times']
        dict_times = result['dict_times']
        
        if not hash_times or not dict_times:
            return
        
        ax.plot(load_factors, hash_times, 'o-', label='MDHG Hash')
        ax.plot(load_factors, dict_times, 's-', label='Dict')
        
        ax.set_title('Performance vs. Load Factor')
        ax.set_xlabel('Load Factor')
        ax.set_ylabel('Time (seconds)')
        ax.legend()
        
        # Add speedup annotations
        for i, (hash_val, dict_val) in enumerate(zip(hash_times, dict_times)):
            speedup = dict_val / hash_val if hash_val > 0 else 0
            ax.annotate(f'{speedup:.2f}x', 
                       (load_factors[i], max(hash_val, dict_val) * 1.05),
                       ha='center')
    
    def print_summary(self):
        """Print a summary of benchmark results."""
        if not self.results:
            print("No results to summarize. Run benchmarks first.")
            return
        
        print("\n" + "="*50)
        print("BENCHMARK SUMMARY")
        print("="*50)
        
        # Print standard distribution results
        self._print_test_summary('Standard Distribution', 'standard')
        
        # Print skewed distribution results
        self._print_test_summary('Skewed Distribution', 'skewed')
        
        # Print collision resistance results
        self._print_test_summary('Collision Resistance', 'collision')
        
        # Print sequential access results
        if 'sequential' in self.results:
            result = self.results['sequential']
            hash_time = result['hash_table_time']
            dict_time = result['dict_time']
            
            print("\n" + "-"*50)
            print(f"Sequential Access Test")
            print("-"*50)
            print(f"MDHG Hash: {hash_time:.6f} seconds")
            if dict_time:
                print(f"Dict:      {dict_time:.6f} seconds")
                speedup = dict_time / hash_time if hash_time > 0 else 0
                print(f"Speedup:   {speedup:.2f}x")
        
        # Print load factor scaling results
        if 'load_factor' in self.results:
            result = self.results['load_factor']
            load_factors = result['load_factors']
            hash_times = result['hash_table_times']
            dict_times = result['dict_times']
            
            print("\n" + "-"*50)
            print(f"Load Factor Scaling Test")
            print("-"*50)
            print(f"{'Load Factor':<15} {'MDHG Hash':<15} {'Dict':<15} {'Speedup':<10}")
            print("-"*55)
            
            for i, load_factor in enumerate(load_factors):
                hash_time = hash_times[i]
                dict_time = dict_times[i] if dict_times else None
                
                if dict_time:
                    speedup = dict_time / hash_time if hash_time > 0 else 0
                    print(f"{load_factor:<15.2f} {hash_time:<15.6f} {dict_time:<15.6f} {speedup:<10.2f}x")
                else:
                    print(f"{load_factor:<15.2f} {hash_time:<15.6f} {'N/A':<15} {'N/A':<10}")
    
    def _print_test_summary(self, title, test_key):
        """Print summary for a specific test."""
        if test_key not in self.results:
            return
        
        result = self.results[test_key]
        
        if 'hash_table_times' not in result or 'dict_times' not in result:
            return
        
        hash_times = result['hash_table_times']
        dict_times = result['dict_times']
        
        if not hash_times or not dict_times:
            return
        
        print("\n" + "-"*50)
        print(f"{title} Test")
        print("-"*50)
        print(f"{'Operation':<10} {'MDHG Hash':<15} {'Dict':<15} {'Speedup':<10}")
        print("-"*50)
        
        operations = ['put', 'get', 'remove', 'total']
        for op in operations:
            hash_time = hash_times[op]
            dict_time = dict_times[op]
            speedup = dict_time / hash_time if hash_time > 0 else 0
            
            print(f"{op:<10} {hash_time:<15.6f} {dict_time:<15.6f} {speedup:<10.2f}x")

# Run the benchmarks
def run_benchmarks():
    """Run benchmarks and display results."""
    # Create hash table and traditional dict
    mdhg_hash = MDHGHashTable(capacity=10000, dimensions=3)
    traditional_dict = {}
    
    # Create benchmark
    benchmark = HashTableBenchmark()
    
    # Run standard tests
    results = benchmark.run_standard_tests(mdhg_hash, traditional_dict, n_operations=10000)
    
    # Print summary
    benchmark.print_summary()
    
    # Plot results
    benchmark.plot_results()
    
    return results, benchmark

if __name__ == "__main__":
    results, benchmark = run_benchmarks()
```

### src/unified/embedder/cand_26_idea_tracker_py.py

```python
# idea_tracker.py

# This is a simplified, text-based representation of the idea tracker.
# In a real implementation, you might use a more structured format (e.g., JSON)
# or a dedicated project management tool.

ideas = {
    "idea_1": {
        "description": "Use the golden ratio to guide the combination of prodigal results.",
        "status": "Implemented",
        "relevant_files": ["construct_superpermutation.py"],
        "notes": "Implemented in the 'prodigal_combination' strategy.",
    },
    "idea_2": {
        "description": "Develop a 'mutation' strategy for generating hypothetical superpermutations.",
        "status": "Implemented",
        "relevant_files": ["construct_superpermutation.py"],
        "notes": "Implemented in v4.4",
    },
    "idea_3": {
        "description": "Explore a De Bruijn graph-guided generation strategy.",
        "status": "Partially Implemented",
        "relevant_files": ["construct_superpermutation.py", "graph_utils.py"],
        "notes": "Basic implementation exists, but needs further refinement.",
    },
    "idea_4": {
        "description": "Investigate the relationship between optimal segment length and the golden ratio.",
        "status": "Ongoing",
        "relevant_files": ["analysis_scripts_final.py", "formulas.py"],
        "notes": "Exploration of formulas in progress.  V14 shows promise.",
    },
    "idea_5": {
        "description": "Develop a 'Calculation Suite' for formula exploration and refinement.",
        "status": "Implemented",
        "relevant_files": ["formulas.py", "formula_evaluator.py", "data_manager.py"],
        "notes": "Initial version implemented in v4.5.",
    },
    "idea_6": {
        "description": "Apply the Principle of Least Action to the superpermutation problem.",
        "status": "Ongoing",
        "relevant_files": ["analysis_scripts_final.py", "formulas.py"],
        "notes": "Developing 'action' function formulations.",
    },
    "idea_7": {
      "description": "Explore higher-order relationships between permutations (beyond k-mers).",
      "status": "Partially Implemented",
      "relevant_files": ["analysis_scripts_final.py"],
      "notes": "MegaWinners/MegaLosers implemented.  Consider longer sequences and motifs."
    },
    "idea_8": {
      "description": "Use 'conceptual space' and dimensionality to inform algorithm design.",
      "status": "Exploratory",
       "relevant_files": [],
       "notes": "Relates to the transition at n=6 and the potential need for higher-dimensional representations."
    },
    "idea_9":{
        "description": "Bouncing Batch Methodology",
        "status": "Implemented",
        "relevant_files": ["superpermutation_generator.py", "construct_superpermutation.py"],
        "notes": "Core methodology implemented in v4.1"
    },
    "idea_10":{
        "description": "Create and use Anti-Laminates",
        "status": "Implemented",
        "relevant_files": ["laminate_utils.py", "analysis_scripts_final.py", "construct_superpermutation.py"],
        "notes": "Implemented in v4.1"
    },
    "idea_11":{
        "description": "Create and use constraint Laminates",
        "status": "Implemented",
        "relevant_files": ["laminate_utils.py", "analysis_scripts_final.py", "construct_superpermutation.py"],
        "notes": "Implemented in v4.2"
    },
    "idea_12":{
        "description": "Laminate Bouncing",
        "status": "Implemented",
        "relevant_files":["laminate_utils.py", "n6_searcher.py"],
        "notes": "Implemented in v4.3, refined for n=6 search tool."
    },
    "idea_13":{
        "description": "Dynamic Strategy Selection",
        "status": "Implemented",
        "relevant_files":["superpermutation_generator.py"],
        "notes": "Implemented using laminate density and connectivity."
    }

}

def list_ideas(status=None):
    """Lists the ideas, optionally filtering by status."""
    for idea_id, details in ideas.items():
        if status is None or details["status"] == status:
            print(f"  ID: {idea_id}")
            print(f"    Description: {details['description']}")
            print(f"    Status: {details['status']}")
            print(f"    Relevant Files: {', '.join(details['relevant_files'])}")
            print(f"    Notes: {details['notes']}")
            print("-" * 20)

def get_idea_details(idea_id):
  """Gets details for a specific idea."""
  if idea_id in ideas:
    details = ideas[idea_id]
    print(f"  ID: {idea_id}")
    print(f"    Description: {details['description']}")
    print(f"    Status: {details['status']}")
    print(f"    Relevant Files: {', '.join(details['relevant_files'])}")
    print(f"    Notes: {details['notes']}")
  else:
    print("Idea Not Found")

# Example usage:
if __name__ == '__main__':
  print("All Ideas:")
  list_ideas()
  print("Implemented:")
  list_ideas("Implemented")
  print("\nDetails for idea_4:")
  get_idea_details("idea_4")
```

### src/unified/embedder/cand_27_laminate_utils_py.py

```python
laminate_utils.py

# laminate_utils.py
import networkx as nx
from utils import is_valid_permutation, hash_permutation, unhash_permutation

def create_laminate(sequence, n, k):
    """Creates a laminate graph from a sequence."""
    graph = nx.DiGraph()
    seq_list = [int(x) for x in sequence]
    for i in range(len(seq_list) - n + 1):
        perm = tuple(seq_list[i:i + n])
        if is_valid_permutation(perm, n):
            if i >= k:
                kmer1 = "".join(str(x) for x in seq_list[i - k:i])
                kmer2 = "".join(str(x) for x in seq_list[i - k + 1:i + 1])
                if len(kmer1) == k and len(kmer2) == k:
                    graph.add_edge(kmer1, kmer2)
    return graph

def is_compatible(permutation, laminate_graph, n, k):
    """Checks if a permutation is compatible with a laminate graph."""
    perm_str = "".join(str(x) for x in permutation)
    for i in range(len(perm_str) - k + 1):
        kmer1 = perm_str[i:i + k - 1]
        kmer2 = perm_str[i + 1:i + k]
        if len(kmer1) == k - 1 and len(kmer2) == k - 1:
            if not laminate_graph.has_edge(kmer1, kmer2):
                return False
    return True

def create_anti_laminate(anti_prodigals, n, k):
    """Creates an anti-laminate graph from a set of anti-prodigals.

    Args:
        anti_prodigals (set): A set of anti-prodigal k-mer strings.
        n (int): The value of n.
        k (int): The k-mer length used for the anti-laminate.

    Returns:
        networkx.DiGraph: The anti-laminate graph.
    """

    anti_laminate = nx.DiGraph()

    # Create a complete graph with all possible (n-1)-mers as nodes and edges
    for perm in generate_permutations(n):
        perm_str = "".join(map(str, perm))
        for i in range(len(perm_str) - (k - 1) + 1):
            kmer1 = perm_str[i:i + k - 1]
            kmer2 = perm_str[i + 1:i + k]
            if len(kmer1) == k-1 and len(kmer2) == k-1:
                anti_laminate.add_edge(kmer1, kmer2)


    # Remove edges that correspond to anti-prodigal k-mers
    for anti_prodigal in anti_prodigals:
        if len(anti_prodigal) == k:  # Ensure the anti-prodigal is a k-mer
            prefix = anti_prodigal[:-1]
            suffix = anti_prodigal[1:]
            if anti_laminate.has_edge(prefix, suffix):
                anti_laminate.remove_edge(prefix, suffix)

    return anti_laminate
```

### src/unified/embedder/cand_29_mdhg_hash__1__py.py

```python
# mdhg_hash.py
# Multi-Dimensional Hamiltonian Golden Ratio Hash Table

from collections import defaultdict, deque, Counter
import math, time, random
from typing import Any, Dict, List, Tuple

class MDHGHashTable:
    def __init__(self, capacity=1024, dimensions=3):
        self.PHI = (1 + 5 ** 0.5) / 2
        self.capacity = capacity
        self.dimensions = dimensions
        self.size = 0
        self.buildings = self._init_buildings()
        self.location_map = {}
        self.stats = {
            'puts': 0, 'gets': 0, 'hits': 0, 'misses': 0, 'collisions': 0
        }

    def _init_buildings(self):
        count = max(1, int(math.log(self.capacity, self.PHI)))
        return {f"B{i}": {'velocity': [None] * (self.capacity // count)} for i in range(count)}

    def put(self, key: Any, value: Any):
        self.stats['puts'] += 1
        b_id = self._hash_to_building(key)
        b = self.buildings[b_id]
        idx = self._hash_to_velocity_index(key, b_id)
        if b['velocity'][idx] is None:
            b['velocity'][idx] = (key, value)
            self.location_map[key] = (b_id, idx)
            self.size += 1

    def get(self, key: Any) -> Any:
        self.stats['gets'] += 1
        loc = self.location_map.get(key)
        if loc:
            b_id, idx = loc
            entry = self.buildings[b_id]['velocity'][idx]
            if entry and entry[0] == key:
                self.stats['hits'] += 1
                return entry[1]
        self.stats['misses'] += 1
        return None

    def remove(self, key: Any) -> bool:
        loc = self.location_map.get(key)
        if loc:
            b_id, idx = loc
            if self.buildings[b_id]['velocity'][idx][0] == key:
                self.buildings[b_id]['velocity'][idx] = None
                del self.location_map[key]
                self.size -= 1
                return True
        return False

    def _hash_to_building(self, key) -> str:
        return f"B{hash(key) % len(self.buildings)}"

    def _hash_to_velocity_index(self, key, b_id) -> int:
        return abs(hash(str(key))) % len(self.buildings[b_id]['velocity'])

    def get_stats(self) -> Dict:
        return self.stats

```

### src/unified/embedder/cand_2_analysis_scripts_final_py.py

```python
import itertools
import math
import networkx as nx
from collections import deque, defaultdict
import heapq
import random  #Needed for Hypothetical Prodigal Generation


def calculate_overlap(s1: str, s2: str) -> int:
    """Calculates the maximum overlap between two strings.

    Args:
        s1 (str): The first string.
        s2 (str): The second string.

    Returns:
        int: The length of the maximum overlap. Returns 0 if there is no overlap.
    """
    max_overlap = 0
    for i in range(1, min(len(s1), len(s2)) + 1):
        if s1[-i:] == s2[:i]:
            max_overlap = i
    return max_overlap


def hash_permutation(permutation: tuple) -> int:
    """Hashes a permutation tuple to a unique integer.

    Args:
        permutation: The permutation tuple (e.g., (1, 2, 3, 4, 5, 6, 7, 8)).

    Returns:
        A unique integer hash value.
    """
    result = 0
    n = len(permutation)
    for i, val in enumerate(permutation):
        result += val * (n ** (n - 1 - i))
    return result


def unhash_permutation(hash_value: int, n: int) -> tuple:
    """Converts a hash value back to a permutation tuple.

    Args:
        hash_value: The integer hash value.
        n: The value of n.

    Returns:
        The corresponding permutation tuple.
    """
    permutation = []
    for i in range(n - 1, -1, -1):
        val = hash_value // (n ** i)
        permutation.append(val + 1)  # Adjust to be 1-indexed
        hash_value -= val * (n ** i)
    return tuple(permutation)


def is_valid_permutation(perm: tuple, n: int) -> bool:
    """Checks if a given sequence is a valid permutation.

    Args:
        perm: The sequence (tuple of integers).
        n: The value of n.

    Returns:
        True if the sequence is a valid permutation, False otherwise.
    """
    return len(set(perm)) == n and min(perm) == 1 and max(perm) == n


def get_kmers(sequence: str, n: int, k: int) -> set[str]:
    """Extracts all k-mers from a sequence of digits, ensuring they form valid permutations.

    Args:
        sequence: The input sequence (string of digits).
        n: The value of n.
        k: The length of the k-mers to extract.

    Returns:
        A set of k-mer strings.
    """
    kmers = set()
    seq_list = [int(x) for x in sequence]  # Ensure sequence is treated as digits
    for i in range(len(sequence) - n + 1):
        perm = tuple(seq_list[i:i + n])
        if is_valid_permutation(perm, n):
            if i >= k:
                kmer = "".join(str(x) for x in seq_list[i - k:i])
                kmers.add(kmer)
    return kmers


def calculate_golden_ratio_points(length: int, levels: int = 1) -> list[int]:
    """Calculates multiple levels of golden ratio points within a sequence.

    Args:
        length (int): The total length of the sequence.
        levels (int): The number of recursive divisions to perform.

    Returns:
        list: A sorted list of unique golden ratio points (integers).
    """
    phi = (1 + math.sqrt(5)) / 2
    points = []
    for _ in range(levels):
        new_points = []
        if not points:
            new_points = [int(length / phi), int(length - length / phi)]
        else:
            for p in points:
                new_points.extend([int(p / phi), int(p - p / phi)])
                new_points.extend([int((length - p) / phi) + p, int(length - (length - p) / phi)])
        points.extend(new_points)
        points = sorted(list(set(points)))  # Remove duplicates and sort
    return points


def calculate_fixed_segments(length: int, segment_size: int, overlap_size: int) -> list[tuple[int, int]]:
    """Calculates start and end indices for fixed-size segments with overlap.

    Args:
        length (int): The total length of the sequence.
        segment_size (int): The desired size of each segment.
        overlap_size (int): The desired overlap between adjacent segments.

    Returns:
        list: A list of tuples, where each tuple contains the (start, end) indices of a segment.
    """
    segments = []
    start = 0
    while start < length:
        end = min(start + segment_size, length)
        segments.append((start, end))
        start += segment_size - overlap_size
    return segments

def generate_permutations(n: int) -> list[tuple[int, ...]]:
    """Generates all permutations of 1 to n.  Used for creating complete sets.

    Args:
        n (int): The number of symbols.

    Returns:
        list[tuple[int, ...]]: A list of all permutations as tuples.
    """
    return list(itertools.permutations(range(1, n + 1)))

# --- Superpermutation Analysis ---

def analyze_superpermutation(superpermutation: str, n: int) -> dict:
    """Analyzes a given superpermutation and provides statistics.

    Args:
        superpermutation (str): The superpermutation string.
        n (int): The value of n (number of symbols).

    Returns:
        dict: A dictionary containing:
            - length: The length of the superpermutation.
            - validity: True if valid, False otherwise.
            - missing_permutations: A list of missing permutations (empty if valid).
            - overlap_distribution: A dictionary showing counts of each overlap length.
            - average_overlap: The average overlap between consecutive permutations.
    """
    permutations = set(itertools.permutations(range(1, n + 1)))
    s_tuple = tuple(int(x) for x in superpermutation)
    found_permutations = set()
    missing_permutations = []
    overlap_distribution = {}

    total_overlap = 0

    for i in range(len(s_tuple) - n + 1):
        perm = s_tuple[i:i+n]
        if is_valid_permutation(perm, n):  # Valid
            found_permutations.add(tuple(perm))  # Use tuples for set
            if i > 0:
                overlap = calculate_overlap("".join(str(x) for x in s_tuple[i-n:i]), "".join(str(x) for x in perm))
                total_overlap += overlap
                overlap_distribution[overlap] = overlap_distribution.get(overlap, 0) + 1

    missing_permutations = list(permutations - found_permutations)
    validity = len(missing_permutations) == 0
    average_overlap = total_overlap / (len(found_permutations) - 1) if len(found_permutations) > 1 else 0

    return {
        "length": len(superpermutation),
        "validity": validity,
        "missing_permutations": missing_permutations,
        "overlap_distribution": overlap_distribution,
        "average_overlap": average_overlap,
    }

# --- Prodigal Result Identification ---

def is_prodigal(sequence: str, permutations_in_sequence: list[tuple[int]], n: int, min_length: int, overlap_threshold: float) -> bool:
    """Checks if a sequence is a 'Prodigal Result'.

    Args:
        sequence (str): The sequence of digits to check.
        permutations_in_sequence (list):  A list of *tuples* representing the
                                         permutations contained within the sequence.
        n (int): The value of n.
        min_length (int): The minimum number of permutations for a "Prodigal Result."
        overlap_threshold (float): The minimum overlap rate (0 to 1).

    Returns:
        bool: True if the sequence is a "Prodigal Result," False otherwise.
    """
    if len(permutations_in_sequence) < min_length:
        return False

    total_length = sum(len(str(p)) for p in permutations_in_sequence)
    overlap_length = total_length - len(sequence)
    max_possible_overlap = (len(permutations_in_sequence) - 1) * (n - 1)

    if max_possible_overlap == 0:
        return False
    return (overlap_length / max_possible_overlap) >= overlap_threshold

def find_prodigal_results(superpermutation: str, n: int, min_length: int = None, overlap_threshold: float = 0.98) -> list[str]:
    """Identifies and returns a list of prodigal results within a given superpermutation.

    Args:
        superpermutation (str): The superpermutation string.
        n (int): The value of n.
        min_length (int): Minimum number of permutations in a prodigal result.
                          Defaults to n-1.  This can be dynamically adjusted.
        overlap_threshold (float): Minimum overlap percentage. Defaults to 0.98,
                                   but can be dynamically adjusted.

    Returns:
        list: A list of "Prodigal Result" sequences (strings).
    """
    prodigal_results = []
    superperm_list = [int(x) for x in superpermutation]  # Convert to list of integers
    if min_length is None:
        min_length = n-1

    for length in range(n * min_length, len(superpermutation) + 1):  # Iterate through possible lengths
        for i in range(len(superpermutation) - length + 1):  # Iterate through starting positions
            subsequence = tuple(superperm_list[i:i+length])  # Get the subsequence as a tuple
            permutations_in_subsequence = set()
            for j in range(len(subsequence) - n + 1):
                perm = subsequence[j:j+n]
                if is_valid_permutation(perm, n):  # Valid permutation
                    permutations_in_subsequence.add(tuple(perm))  # Use tuples for set membership

            if is_prodigal(subsequence, list(permutations_in_subsequence), n, min_length, overlap_threshold):
                prodigal_results.append("".join(str(x) for x in subsequence))  # Store as string
    return prodigal_results

    # --- Winner/Loser Calculation ---

def calculate_winners_losers(superpermutations: list[str], n: int, k: int) -> tuple[dict, dict]:
    """Calculates "Winner" and "Loser" k-mer weights from a list of superpermutations.

    Args:
        superpermutations (list): A list of superpermutation strings.
        n (int): The value of n.
        k (int): The length of the k-mers to analyze.

    Returns:
        tuple: (winners, losers), where:
            - winners: A dictionary of {kmer: weight} for winning k-mers.
            - losers: A dictionary of {kmer: weight} for losing k-mers.
    """

    all_kmers = {}  # {kmer: total_count}
    for superpermutation in superpermutations:
        s_tuple = tuple(int(x) for x in superpermutation)
        for i in range(len(s_tuple) - n + 1):
            perm = s_tuple[i:i + n]
            if is_valid_permutation(perm, n):  # Valid
                if i > 0:
                    kmer = "".join(str(x) for x in s_tuple[i - k:i])
                    all_kmers[kmer] = all_kmers.get(kmer, 0) + 1

    # Divide into "shorter" and "longer" groups based on median length
    lengths = [len(s) for s in superpermutations]
    lengths.sort()
    median_length = lengths[len(lengths) // 2]
    shorter_superpermutations = [s for s in superpermutations if len(s) <= median_length]
    longer_superpermutations = [s for s in superpermutations if len(s) > median_length]

    winners = {}  # {kmer: weight}
    losers = {}  # {kmer: weight}

    shorter_counts = {}  # {kmer: count_in_shorter}
    for superpermutation in shorter_superpermutations:
        s_tuple = tuple(int(x) for x in superpermutation)
        for i in range(len(s_tuple) - n + 1):
            perm = s_tuple[i:i + n]
            if is_valid_permutation(perm, n):
                if i > 0:
                    kmer = "".join(str(x) for x in s_tuple[i - k:i])
                    shorter_counts[kmer] = shorter_counts.get(kmer, 0) + 1

    longer_counts = {}  # {kmer: count_in_longer}
    for superpermutation in longer_superpermutations:
        s_tuple = tuple(int(x) for x in superpermutation)
        for i in range(len(s_tuple) - n + 1):
            perm = s_tuple[i:i + n]
            if is_valid_permutation(perm, n):
                if i > 0:
                    kmer = "".join(str(x) for x in s_tuple[i - k:i])
                    longer_counts[kmer] = longer_counts.get(kmer, 0) + 1

    # Calculate weights based on the difference in counts
    for kmer in all_kmers:
        score = shorter_counts.get(kmer, 0) - longer_counts.get(kmer, 0)
        if score > 0:
            winners[kmer] = score
        elif score < 0:
            losers[kmer] = -score  # Store as positive weights

    return winners, losers


def calculate_sequence_winners_losers(superpermutations: list[str], n: int, sequence_length: int = 2) -> tuple[dict, dict]:
    """Calculates 'Winner' and 'Loser' weights for sequences of permutations.

    Args:
        superpermutations (list[str]): List of superpermutation strings.
        n (int): The value of n.
        sequence_length (int): The length of the permutation sequences to analyze (default: 2).

    Returns:
        tuple: (winners, losers), where winners and losers are dictionaries
               mapping sequence hashes (integers) to weights.
    """
    all_sequences = {} # {seq_hash : count}

    for superperm in superpermutations:
        s_tuple = tuple(int(x) for x in superperm)
        perms = []
        for i in range(len(s_tuple) - n + 1):
            perm = s_tuple[i:i+n]
            if is_valid_permutation(perm, n):
                perms.append(hash_permutation(perm))  # Append hash

        for i in range(len(perms) - (sequence_length -1)):
            seq = tuple(perms[i:i+sequence_length])
            seq_hash = hash(seq) #Hash the sequence of hashes.
            all_sequences[seq_hash] = all_sequences.get(seq_hash, 0) + 1

    #Divide into "shorter" and "longer"
    lengths = [len(s) for s in superpermutations]
    lengths.sort()
    median_length = lengths[len(lengths)//2]
    shorter_superpermutations = [s for s in superpermutations if len(s) <= median_length]
    longer_superpermutations = [s for s in superpermutations if len(s) > median_length]

    winners = {}
    losers = {}
    shorter_counts = {}
    for superperm in shorter_superpermutations:
        s_tuple = tuple(int(x) for x in superperm)
        perms = []
        for i in range(len(s_tuple) - n + 1):
            perm = s_tuple[i:i+n]
            if is_valid_permutation(perm, n):
                perms.append(hash_permutation(perm)) #Append hash
        for i in range(len(perms) - (sequence_length - 1)):
            seq = tuple(perms[i:i+sequence_length])
            seq_hash = hash(seq)
            shorter_counts[seq_hash] = shorter_counts.get(seq_hash, 0) + 1

    longer_counts = {}
    for superperm in longer_superpermutations:
        s_tuple = tuple(int(x) for x in superperm)
        perms = []
        for i in range(len(s_tuple) - n + 1):
            perm = s_tuple[i:i+n]
            if is_valid_permutation(perm, n):
                perms.append(hash_permutation(perm))
        for i in range(len(perms) - (sequence_length - 1)):
            seq = tuple(perms[i:i+sequence_length])
            seq_hash = hash(seq)
            longer_counts[seq_hash] = longer_counts.get(seq_hash, 0) + 1

    for seq_hash in all_sequences:
        score = shorter_counts.get(seq_hash, 0) - longer_counts.get(seq_hash, 0)
        if score > 0:
            winners[seq_hash] = score
        if score < 0:
            losers[seq_hash] = -score

    return winners, losers

# --- Anti-Prodigal Identification ---
def identify_anti_prodigals(superpermutations, n, k, overlap_threshold):
    """Identifies and returns a list of 'anti-prodigal' k-mers.

    Args:
        superpermutations (list): A list of superpermutation strings.
        n (int): The value of n.
        k (int): The length of k-mers.
        overlap_threshold (float): The *maximum* overlap allowed for an anti-prodigal

    Returns:
        set: A set of 'anti-prodigal' k-mers (strings).
    """
    anti_prodigals = set()
    for superpermutation in superpermutations:
        s_tuple = tuple(int(x) for x in superpermutation)
        for i in range(len(s_tuple) - n + 1):
            perm = s_tuple[i:i + n]
            if is_valid_permutation(perm, n):  # Valid
                if i > k:
                    kmer = "".join(str(x) for x in s_tuple[i-k:i])
                    overlap = calculate_overlap("".join(str(x) for x in s_tuple[i-k:i]), "".join(str(x) for x in perm))
                    if overlap < ( (n-1) * overlap_threshold):  #  Condition for Anti-Prodigal
                        anti_prodigals.add(kmer)
    return anti_prodigals
    
    # --- De Bruijn Graph Functions ---

def build_debruijn_graph(n: int, k: int, permutations=None, superpermutation=None) -> nx.DiGraph:
    """Builds a De Bruijn graph of order k for permutations of n symbols.

    Args:
        n: The number of symbols (e.g., 8 for n=8).
        k: The order of the De Bruijn graph (k-mers).
        permutations: (Optional) A list of permutation tuples.  If provided,
                      the graph is built ONLY from these permutations.
        superpermutation: (Optional) A superpermutation string. If provided,
                          the graph is built from the k-mers in this string.
                          Must provide one, and only one, of the two.

    Returns:
        A networkx.DiGraph representing the De Bruijn graph.
    """

    if (permutations is None and superpermutation is None) or (permutations is not None and superpermutation is not None):
        raise ValueError("Must provide either 'permutations' or 'superpermutation', but not both.")

    graph = nx.DiGraph()

    if permutations:
      for perm in permutations:
          perm_str = "".join(str(x) for x in perm)
          for i in range(len(perm_str) - k + 1):
              kmer1 = perm_str[i:i + k - 1]
              kmer2 = perm_str[i + 1:i + k]
              if len(kmer1) == k - 1 and len(kmer2) == k-1: #Should always be true
                graph.add_edge(kmer1, kmer2, weight=calculate_overlap(kmer1, kmer2))
    else: #Use superpermutation
        s_list = [int(x) for x in superpermutation]
        for i in range(len(s_list) - n + 1):
            perm = tuple(s_list[i:i+n])
            if is_valid_permutation(perm, n):
                #Valid permutation.
                if i >= k:
                  kmer1 = "".join(str(x) for x in s_list[i-k:i])
                  kmer2 = "".join(str(x) for x in s_list[i-k+1: i+1])
                  if len(kmer1) == k - 1 and len(kmer2) == k - 1:
                    graph.add_edge(kmer1, kmer2, weight = calculate_overlap(kmer1,kmer2))
    return graph

def add_weights_to_debruijn(graph: nx.DiGraph, winners: dict, losers: dict):
    """Adds 'winner_weight' and 'loser_weight' attributes to the edges of a De Bruijn graph.

    Args:
        graph: The De Bruijn graph (networkx.DiGraph).
        winners: A dictionary of Winner k-mers and their weights.
        losers: A dictionary of Loser k-mers and their weights.
    """

    for u, v, data in graph.edges(data=True):
        kmer = u[1:] + v[-1]  # Reconstruct the k-mer from the edge
        data['winner_weight'] = winners.get(kmer, 0)
        data['loser_weight'] = losers.get(kmer, 0)

def find_cycles(graph: nx.DiGraph) -> list[list[str]]:
    """Finds cycles in the De Bruijn graph using a simple DFS approach."""
    cycles = []
    visited = set()

    def dfs(node, path):
        visited.add(node)
        path.append(node)

        for neighbor in graph.neighbors(node):
            if neighbor == path[0] and len(path) > 1:  # Found a cycle
                cycles.append(path.copy())
            elif neighbor not in visited:
                dfs(neighbor, path)

        path.pop()
        visited.remove(node) #Correctly remove node

    for node in graph.nodes:
        if node not in visited:
            dfs(node, [])
    return cycles

def find_high_weight_paths(graph: nx.DiGraph, start_node: str, length_limit: int) -> list[list[str]]:
    """
    Finds high-weight paths in the De Bruijn graph starting from a given node.
    """
    best_paths = []
    best_score = -float('inf')

    def dfs(current_node, current_path, current_score):
        nonlocal best_score
        nonlocal best_paths

        if len(current_path) > length_limit:
            return

        current_score_adj = current_score
        if len(current_path) > 1:
            current_score_adj = current_score / (len(current_path)-1) #Average

        if current_score_adj > best_score:
            best_score = current_score_adj
            best_paths = [current_path.copy()]  # New best path
        elif current_score_adj == best_score and len(current_path) > 1: #Dont include starting nodes
            best_paths.append(current_path.copy())  # Add to list of best paths

        for neighbor in graph.neighbors(current_node):
            edge_data = graph.get_edge_data(current_node, neighbor)
            new_score = current_score + edge_data.get('weight', 0) + edge_data.get('winner_weight', 0) - edge_data.get('loser_weight', 0)
            dfs(neighbor, current_path + [neighbor], new_score)

    dfs(start_node, [start_node], 0)
    return best_paths

# --- Hypothetical Prodigal Generation ---
def generate_permutations_on_demand_hypothetical(current_sequence: str, kmer: str, n: int, k: int) -> set[int]:
    """Generates candidate permutations for hypothetical prodigals. More restrictive.

    Args:
        current_sequence (str): The sequence currently being built
        kmer (str): The k-mer to extend (either prefix or suffix).
        n (int): value of n
        k (int): The k value

    Returns:
        set[int]: A set of permutation *hashes*.
    """
    valid_permutations = set()
    all_perms = generate_permutations(n)  # Get *all* permutations
    for perm in all_perms:
        perm_str = "".join(str(x) for x in perm)
        if kmer == perm_str[:k] or kmer == perm_str[-k:]:
           valid_permutations.add(hash_permutation(perm))

    # We do NOT filter here, as these are for hypotheticals.
    return valid_permutations

def generate_mega_hypotheticals(prodigal_results: dict, winners: dict, losers: dict, n: int, num_to_generate: int = 20, min_length: int = 50, max_length: int = 200) -> dict:
    """Generates 'Mega-Hypothetical' Prodigals by combining existing Prodigals.

    Args:
        prodigal_results (dict): Dictionary of existing ProdigalResult objects.
        winners (dict): "Winner" k-mer data.  Used for scoring connections.
        losers (dict): "Loser" k-mer data. Used for filtering.
        n (int): The value of n.
        num_to_generate (int): The number of Mega-Hypotheticals to generate.
        min_length (int): Minimum length (in permutations).
        max_length (int): Maximum length (in permutations).

    Returns:
        dict: A dictionary of new 'Mega-Hypothetical' ProdigalResult objects.
    """
    mega_hypotheticals = {}
    next_mega_id = -1  # Use negative IDs to distinguish

    for _ in range(num_to_generate):
        num_prodigals_to_combine = random.randint(2, 4)  # Combine 2-4 Prodigals
        #Select prodigals, but with a bias towards those with higher overlap and greater length
        prodigal_weights = [p.overlap_rate * p.length for p in prodigal_results.values()]
        selected_prodigal_ids = random.choices(list(prodigal_results.keys()), weights=prodigal_weights, k=num_prodigals_to_combine)
        selected_prodigals = [prodigal_results[pid] for pid in selected_prodigal_ids]

        # Sort by length, in any order.
        selected_prodigals.sort(key=lambda p: len(p.sequence))

        #Use golden ratio for lengths.
        phi = (1 + math.sqrt(5)) / 2
        total_target_length = random.randint(n * min_length, n* max_length)
        target_lengths = []

        remaining_length = total_target_length
        for i in range(len(selected_prodigals)-1):
            segment_length = int(remaining_length / (phi**(i+1)))
            target_lengths.append(segment_length)
            remaining_length -= segment_length
        target_lengths.append(remaining_length) # Add the final length.

        combined_sequence = ""
        
        #Randomize starting location.
        start_location = random.randint(0,len(selected_prodigals)-1)
        prodigal_order = []
        for i in range(len(selected_prodigals)):
            prodigal_order.append(selected_prodigals[(start_location + i) % len(selected_prodigals)])

        for i in range(len(prodigal_order)):
            prodigal = prodigal_order[i]
            target_length = target_lengths[i]

            # Get a random section of the prodigal, unless it is too short.
            start_index = random.randint(0, max(0, len(prodigal.sequence) - target_length))  # Ensure valid start
            current_sequence = prodigal.sequence[start_index : start_index + target_length]

            # Connect to previous
            if combined_sequence != "":
                overlap = calculate_overlap(combined_sequence, current_sequence)
                if overlap == 0: #Need to use the combiner
                    prefix = combined_sequence[-(n-1):]
                    suffix = current_sequence[:n-1]
                    candidates = generate_permutations_on_demand_hypothetical(prefix, suffix,  n, n-1)
                    if candidates:
                        # Choose the best candidate based on winners/losers (simplified scoring)
                        best_candidate = None
                        best_score = -float('inf')
                        for cand_hash in candidates:
                            cand_perm = unhash_permutation(cand_hash, n)
                            cand_str = "".join(str(x) for x in cand_perm)
                            score = 0
                            for k in [7, 6]: # Using values for n=8
                                for j in range(len(cand_str) - k + 1):
                                    kmer = cand_str[j:j+k]
                                    score += winners.get(kmer, 0)
                                    score -= losers.get(kmer, 0)

                            if score > best_score:
                                best_score = score
                                best_candidate = cand_str

                        overlap = calculate_overlap(combined_sequence, best_candidate)
                        combined_sequence += best_candidate[overlap:]
                    else:
                        continue #Skip if we cannot connect.

                else: #Overlap exists
                    combined_sequence += current_sequence[overlap:]
            else:
                combined_sequence = current_sequence
        
        #Check if prodigal
        perms_in_sequence = set()
        for i in range(len(combined_sequence) - n + 1):
            perm = tuple(int(x) for x in combined_sequence[i:i+n])
            if is_valid_permutation(perm, n):
                perms_in_sequence.add(hash_permutation(perm))

        if is_prodigal(combined_sequence, [unhash_permutation(x,n) for x in perms_in_sequence], n, min_length=min_length, overlap_threshold=0.95) and len(perms_in_sequence) > 0:
            mega_hypotheticals[next_mega_id] = ProdigalResult(combined_sequence, next_mega_id)
            next_mega_id -= 1

    return mega_hypotheticals
```

### src/unified/embedder/cand_30_mdhg_hash_py.py

```python
# mdhg_hash.py
# Multi-Dimensional Hamiltonian Golden Ratio Hash Table
# Core data structure for AGRM-integrated hash-based compute optimization

# (The full class definition from your earlier submission would be pasted here.
# Omitted for brevity, but assumed to be part of this final package.)

```

### src/unified/embedder/cand_31_n6_searcher_py.py

```python
# n6_searcher.py

import random
import logging
import multiprocessing
import time
import itertools
from utils import setup_logging, normalize_sequence, compute_checksum, generate_permutations, is_valid_permutation, calculate_overlap, hash_permutation, unhash_permutation
from analysis_scripts_final import calculate_winners_losers, identify_anti_prodigals, is_prodigal,  calculate_sequence_winners_losers, find_prodigal_results
from laminate_utils import create_n7_constraint_laminate, is_compatible, create_anti_laminate, update_laminate, merge_laminates

# Constants for n=6 search
TARGET_LENGTH = 871
N = 6
GRID_DIMENSIONS = (2,) * N  # Bouncing Batch grid
INITIAL_SEED = 42
MAX_ATTEMPTS = 1000  #Limit per worker
K_VALUE = 5

def generate_constrained_candidate(n, target_length, constraint_laminate, anti_laminates, winners, losers, seed):
    """Generates a candidate superpermutation of the target length, respecting laminates.
       Uses a backtracking search.
    """
    random.seed(seed)
    all_permutations = generate_permutations(n)
    random.shuffle(all_permutations)

    superpermutation = []
    current_length = 0

    #Helper function to check compatibility.
    def is_fully_compatible(perm, lam, anti_lams):
      for laminate in lam:
        if not is_compatible(perm, laminate, n, n-1):
            return False
        if not is_compatible(perm, laminate, n, n-2):
            return False
      for anti_laminate in anti_lams:
        if not is_compatible(perm, anti_laminate, n, n-1):
            return False
        if not is_compatible(perm, anti_laminate, n, n-2):
          return False
      return True

    def backtrack(current_permutation, current_length):
        nonlocal superpermutation
        if current_length == target_length:
            superperm_str = "".join(map(str, current_permutation))
            if verify_permutation_coverage(superperm_str, n):
                superpermutation = superperm_str
                return True  # Found a solution
            else:
                return False

        prefix = "".join(map(str,current_permutation[-(n-1):]))
        for perm in all_permutations:
            perm_tuple = tuple(perm)
            if is_fully_compatible(perm_tuple,[constraint_laminate], anti_laminates):
                overlap = calculate_overlap(prefix, "".join(map(str,perm_tuple)))
                if overlap > 0:
                    new_superpermutation = current_permutation + list(str(x) for x in perm_tuple[overlap:])
                    if len(new_superpermutation) <= target_length: #Prune
                        if backtrack(new_superpermutation, len(new_superpermutation)):
                            return True  # Solution found
        return False

    # Start with a random *compatible* permutation
    for perm in all_permutations:
        if is_fully_compatible(perm,[constraint_laminate], anti_laminates):
            if backtrack(list(str(x) for x in perm), len(perm)):
                return superpermutation
            break #If it cant find it with the first, it wont find it.
    return None

def calculate_cell_winners_losers(hypothetical_superpermutation, cell_permutations, n, k=None):
    """Calculates winners and losers based on a superpermutation and a cell's permutations."""
    if k is None:
        k = n - 1

    winners = {}
    losers = {}

    # Convert hypothetical superpermutation to a list of integers
    s_list = [int(x) for x in hypothetical_superpermutation]

    for i in range(len(s_list) - n + 1):
        perm = tuple(s_list[i:i+n])
        if is_valid_permutation(perm, n):
            # Check if this permutation belongs to the current cell
            if hash_permutation(perm) in cell_permutations:  # Use efficient hash lookup
                if i >= k:
                    kmer = tuple(s_list[i-k:i])
                    kmer_str = "".join(map(str,kmer))
                    # Simplified: Just count occurrences
                    winners[kmer_str] = winners.get(kmer_str, 0) + 1
                    # Could add more sophisticated weighting here,

    return winners, losers

def bouncing_batch_test_n6(candidates, n, grid_dimensions, winners, losers, anti_laminates, cell_coords, laminate, anti_laminate):
  """Simplified Bouncing Batch test for the n=6 search."""
  all_permutations = generate_permutations(n)
  cell_permutations = {}  # {cell_coords: set(permutation_hashes)}
  for perm in all_permutations:
    cell = assign_permutation_to_cell(perm, n, grid_dimensions)
    if cell not in cell_permutations:
        cell_permutations[cell] = set()
    cell_permutations[cell].add(hash_permutation(perm))

  # Initialize local winners, losers
  new_winners = {}
  new_losers = {}
  new_anti_prodigals = []

  #Process the specific cell only
  for candidate in candidates:
    cell_winners, cell_losers = calculate_cell_winners_losers(candidate, cell_permutations[cell_coords], n)

    # Update local winners and losers
    for kmer, count in cell_winners.items():
        new_winners[kmer] = new_winners.get(kmer, 0) + count
    for kmer, count in cell_losers.items():
        new_losers[kmer] = new_losers.get(kmer, 0) - count  # Losers are *negative*

    # Identify anti-prodigals within this candidate
    new_anti_prodigals.extend(identify_anti_prodigals([candidate], n, 5, 0.8, winners, losers, 2))  # Example Thresholds

    #Update Laminates
    laminate = update_laminate(laminate, [candidate], n, n-1, "positive")
    laminate = update_laminate(laminate, [candidate], n, n-2, "positive")

  anti_laminate = update_laminate(anti_laminate, new_anti_prodigals, n, n-1, "negative")
  anti_laminate = update_laminate(anti_laminate, new_anti_prodigals, n, n-2, "negative")
  #No longer returning is_valid or length, we do that in the worker task.
  return new_winners, new_losers, new_anti_prodigals, laminate, anti_laminate

def worker_task_n6(args):
    """Worker task for n=6 search."""
    seed, n, target_length, constraint_laminate, anti_laminates, winners, losers, num_candidates, cell_coords, laminate, anti_laminate = args #Added cell, and the two lams

    candidates = []
    for _ in range(num_candidates):
        candidate = generate_constrained_candidate(n, target_length, [constraint_laminate, laminate], anti_laminates, winners, losers, seed)
        if candidate:
          candidates.append(candidate)
        seed += 1 # Increment seed for next candidate.


    if candidates:
        new_winners, new_losers, new_anti_prodigals, laminate, anti_laminate = bouncing_batch_test_n6(candidates, n, winners, losers, anti_laminates, cell_coords, laminate, anti_laminate) #Pass in data
        valid_list = []
        length_list = []
        for candidate in candidates:
          is_valid_result = verify_permutation_coverage(candidate, n)
          length_result = len(candidate)
          valid_list.append(is_valid_result)
          length_list.append(length_result)
          #Immediately save if we have a winner.
          if is_valid_result and length_result == TARGET_LENGTH:
            normalized_candidate = normalize_sequence(candidate)
            checksum = compute_checksum(normalized_candidate)
            print(f"Found valid n=6 superpermutation of length 871!  Checksum: {checksum}")
            with open("found_871_superpermutation.txt", "w") as f:  # Save it!
                f.write(normalized_candidate)
            exit() # Exit
        return valid_list, length_list, new_winners, new_losers, new_anti_prodigals, candidates, cell_coords, laminate, anti_laminate #Return candidates too
    else:
        return [], [], {}, {}, [], [], cell_coords, laminate, anti_laminate

def main():
   #Setup
   setup_logging()
   random.seed(INITIAL_SEED) # Use a constant, known seed.
   #Load existing data.

    # --- Load Initial Data ---
   winners, losers = {}, {}
   try:
       with open("winners_losers_data_n8.txt", "r") as f: #Use n=8 data, as that is most complete.
           for line in f:
               kmer, weight = line.strip().split(",")
               if float(weight) > 0:
                   winners[kmer] = int(weight)
               else:
                   losers[kmer] = int(weight)
   except FileNotFoundError:
       print("Initial Winners/Losers file not found. Starting with empty.")

   #Create n=6 constraint Laminate
   constraint_laminate = None
   try:
       with open("best_superpermutation_n6.txt", "r") as f: # You might need to create this file manually.
           best_n6 = f.read().strip()
           constraint_laminate = create_n7_constraint_laminate(best_n6, N, N-1)  # Use create_n7...
           constraint_laminate_2 = create_n7_constraint_laminate(best_n6, N, N-2)
   except FileNotFoundError:
       print("Error: Could not find n=6 file to make constraint laminate")
       return

   anti_laminates = []
   num_processes = multiprocessing.cpu_count()  # Use all available cores

    #Initialize the laminates and anti
   laminates = {}
   anti_laminates = {}
   grid_dimensions = (2,)*N
   for cell_coords in all_cell_coords(grid_dimensions):
      laminates[cell_coords] = nx.DiGraph()
      anti_laminates[cell_coords] = nx.DiGraph()

   with multiprocessing.Pool(processes=num_processes) as pool:
        args_list = []
        seed = INITIAL_SEED
        for i in range(1000):  # Example: Try many batches of candidates
            for cell_coords in all_cell_coords(grid_dimensions): #All cells.
              args_list.append((seed, N, TARGET_LENGTH, [constraint_laminate, constraint_laminate_2], anti_laminates[cell_coords], winners, losers, 5, cell_coords, laminates[cell_coords], anti_laminates[cell_coords])) # Pass all to worker.  5 attempts.
              seed += 1

            results = pool.map(worker_task_n6, args_list)
            args_list = [] #Clear

            # Process results
            for valid_list, length_list, new_winners, new_losers, new_anti_prodigals, candidates, cell_coords, new_laminate, new_anti_laminate in results:
                for i in range(len(valid_list)):
                    if valid_list[i] and length_list[i] == TARGET_LENGTH:
                        normalized_candidate = normalize_sequence(candidates[i])
                        checksum = compute_checksum(normalized_candidate)
                        print(f"Found valid n=6 superpermutation of length 871!  Checksum: {checksum}")
                        with open("found_871_superpermutation.txt", "w") as f:  # Save it!
                            f.write(normalized_candidate)
                        return # Exit

                # Update  winners/losers (simplified)
                for kmer, count in new_winners.items():
                    winners[kmer] = winners.get(kmer, 0) + count
                for kmer, count in new_losers.items():
                    losers[kmer] = losers.get(kmer, 0) + count

                # Update anti-laminates, specific to each cell:
                if new_anti_prodigals:
                  anti_laminates[cell_coords] = update_laminate(anti_laminates[cell_coords], new_anti_prodigals, N, N-1, "negative")
                  anti_laminates[cell_coords] = update_laminate(anti_laminates[cell_coords], new_anti_prodigals, N, N-2, "negative")
                laminates[cell_coords] = new_laminate

            # "Bounce" the laminates (both positive and negative)
            new_laminates = {}
            new_anti_laminates = {}
            for cell_coords in laminates: #all cells
                neighbor_coords = get_neighboring_cells(cell_coords, GRID_DIMENSIONS) #Need to get the neighbors
                neighbor_laminates = [laminates[coords] for coords in neighbor_coords if coords in laminates] # Get all the neightbor laminates
                neighbor_anti_laminates = [anti_laminates[coords] for coords in neighbor_coords if coords in anti_laminates]

                # Merge with neighbors
                new_laminates[cell_coords] = merge_laminates(neighbor_laminates + [laminates[cell_coords]], method="intersection")  # Include self
                new_anti_laminates[cell_coords] = merge_laminates(neighbor_anti_laminates + [anti_laminates[cell_coords]], method="union")  # Include self

            laminates = new_laminates #Update for next loop
            anti_laminates = new_anti_laminates

   logging.info("n=6 search completed. No 871-length superpermutation found.")

#Helper function
def all_cell_coords(grid_dimensions):
  # Use the global variable, since this is n=6 specific.
  ranges = [range(2) for _ in range(N)]
  return list(itertools.product(*ranges))

#Helper function
def get_neighboring_cells(cell_coords, grid_dimensions):
    """Gets the coordinates of neighboring cells in the grid."""
    neighbors = []
    for i in range(len(cell_coords)):
        for delta in [-1, 1]:
            neighbor_coords = list(cell_coords)
            neighbor_coords[i] = (neighbor_coords[i] + delta) % grid_dimensions[i]  # Wrap around
            neighbors.append(tuple(neighbor_coords))
    return neighbors

if __name__ == "__main__":
    main()
```

### src/unified/embedder/cand_32_prodigal_manager_py.py

```python
# prodigal_manager.py
import json
import logging
from analysis_scripts_final import calculate_winners_losers, identify_anti_prodigals, is_prodigal, calculate_sequence_score, find_prodigal_results,  calculate_extensibility_score
from utils import is_valid_permutation, calculate_overlap, hash_permutation, unhash_permutation


class ProdigalManager:
    """Manages the collection, storage, analysis, and ranking of prodigal results."""

    def __init__(self, n, prodigal_file="prodigal_results.json"):
        """Initializes the ProdigalManager.

        Args:
            n (int): The value of n for this ProdigalManager.
            prodigal_file (str): The filename for storing prodigal data.
        """
        self.n = n
        self.prodigal_file = prodigal_file
        self.prodigal_results = {}  # {prodigal_id: {data}}
        self.next_prodigal_id = 0
        self.load_prodigals() # Load any saved


    def add_prodigal(self, sequence, n_value, source, winners={}, losers={}, layout_memory=None, laminates=[], anti_laminates=[]):
        """Adds a new prodigal to the database, after extending and analyzing it.

        Args:
            sequence (str): The potential prodigal sequence.
            n_value (int): The n value for which this sequence was found.
            source (str):  Information about how the prodigal was found.
            winners, losers, layout_memory, laminates, anti_laminates: Current data.
        """

        # 1. Extend the prodigal
        extended_sequence, _ = extend_prodigal(sequence, n_value, winners, losers, layout_memory, laminates, anti_laminates)

        # 2. Analyze the prodigal
        prodigal_data = analyze_prodigal(extended_sequence, n_value, winners, losers, layout_memory, laminates, anti_laminates)

        # 3. Check if it's actually a prodigal (might not be, after extension)
        if prodigal_data["is_prodigal"]:

            # 4. Check if it's a duplicate (using sequence hash)
            is_new = True
            for existing_prodigal_id, existing_prodigal in self.prodigal_results.items():
                if extended_sequence == existing_prodigal['sequence']:
                    is_new = False
                    break

            if is_new:
                # 5. Add to the database
                prodigal_id = self.next_prodigal_id
                self.prodigal_results[prodigal_id] = {
                    "sequence": extended_sequence,
                    "length": len(extended_sequence),
                    "overlap_rate": prodigal_data["overlap_rate"],
                    "n_value": n_value,
                    "source": source,
                    "breakpoints": prodigal_data["breakpoints"],
                    "winner_score": prodigal_data["winner_score"],
                    "loser_score": prodigal_data["loser_score"],
                    "extensibility_score": prodigal_data["extensibility_score"],
                    "parent_prodigals": [],  # To be filled in if created by combining prodigals
                    "child_prodigals": [], # Or if this is used to make another.
                    "used_count": 0,
                }
                self.next_prodigal_id += 1
                logging.info(f"Added new prodigal (ID: {prodigal_id}, Length: {len(extended_sequence)} , Source: {source})")
            else:
                logging.debug("Skipped adding duplicate prodigal.")
        else:
            logging.debug("Sequence not prodigal after extension.")


    def get_best_prodigals(self, n, task, context):
        """Retrieves the best prodigals based on the current context.

        Args:
            n (int): The target n value.
            task (str): The task for which prodigals are needed (e.g., "generation", "completion").
            context (dict):  Additional context information (e.g., current superpermutation, missing permutations).

        Returns:
            dict: A dictionary of the best prodigals, sorted by rank.
        """
        # 1. Filter prodigals based on n_value
        relevant_prodigals = {
            pid: data for pid, data in self.prodigal_results.items() if data["n_value"] == n
        }

        # 2. Rank prodigals
        ranked_prodigals = self.rank_prodigals(relevant_prodigals)

        # 3.  Return (for now, return all, sorted)
        return ranked_prodigals

    def rank_prodigals(self, prodigals):
        """Ranks prodigals based on a combined score."""
        #This is a basic ranking, and can be improved.
        scored_prodigals = []
        for p_id, p_data in prodigals.items():
            score = (
                p_data["length"] * 1 +
                p_data["overlap_rate"] * 100 +
                p_data["winner_score"] -
                p_data["loser_score"] -
                len(p_data["breakpoints"]) * 10 + # Fewer breakpoints are better.
                p_data['extensibility_score'] * 5
            )
            scored_prodigals.append((score, p_id))

        # Sort by score (highest first) and return as a dictionary
        sorted_prodigals = sorted(scored_prodigals, reverse=True)
        return {prodigals[p_id]["id"]: prodigals[p_id] for score, p_id in sorted_prodigals}

    def load_prodigals(self):
        """Loads prodigal data from the JSON file."""
        try:
            with open(self.prodigal_file, 'r') as f:
                prodigal_data = json.load(f)
                # Convert keys to integers and 'breakpoints' to list of dicts
                loaded_prodigals = {}
                for p_id_str, p_data in prodigal_data.items():
                    p_id = int(p_id_str)
                    loaded_prodigals[p_id] = {
                        'sequence': p_data['sequence'],
                        'length': p_data['length'],
                        'overlap_rate': p_data['overlap_rate'],
                        'n_value': p_data['n_value'],
                        'source': p_data['source'],
                        'breakpoints': [{'position': bp['position'], 'reason': bp['reason'], 'n_values': bp['n_values']}
                                         for bp in p_data.get('breakpoints', [])],  # Ensure 'breakpoints' exists
                        'winner_score': p_data.get('winner_score', 0), # Handle potentially missing
                        'loser_score': p_data.get('loser_score', 0),
                        'extensibility_score': p_data.get('extensibility_score', 0),
                        'parent_prodigals': p_data.get('parent_prodigals', []),
                        'child_prodigals': p_data.get('child_prodigals', []),
                        'used_count': p_data.get('used_count',0)
                    }
                self.prodigal_results = loaded_prodigals
                self.next_prodigal_id = max(self.prodigal_results.keys(), default=0) + 1 if self.prodigal_results else 0 #Next id
                logging.info(f"Loaded {len(self.prodigal_results)} prodigals from {self.prodigal_file}.")
        except FileNotFoundError:
            logging.info(f"No prodigal data file found: {self.prodigal_file}. Starting with an empty database.")
            self.prodigal_results = {}
            self.next_prodigal_id = 0
        except json.JSONDecodeError:
            logging.error(f"Error decoding JSON from {self.prodigal_file}.  Check for file corruption.")
            self.prodigal_results = {}
            self.next_prodigal_id = 0

    def save_prodigals(self):
        """Saves the prodigal data to the JSON file."""
        with open(self.prodigal_file, 'w') as f:
            # Convert keys to strings before saving (JSON requires string keys)
            prodigal_results_str_keys = {str(k): v for k, v in self.prodigal_results.items()}
            json.dump(prodigal_results_str_keys, f, indent=4)
        logging.info(f"Saved {len(self.prodigal_results)} prodigals to {self.prodigal_file}.")
        
    def get_prodigal_by_id(self, prodigal_id):
      if prodigal_id in self.prodigal_results:
        return self.prodigal_results[prodigal_id]
      return None
    
    def update_prodigal(self, prodigal_id, new_data):
      if prodigal_id in self.prodigal_results:
        self.prodigal_results[prodigal_id] = new_data
        return True
      return False
```

### src/unified/embedder/cand_33_reconfigurator_py.py

```python
# reconfigurator.py
import random
import logging
from utils import calculate_overlap, is_valid_permutation, hash_permutation, unhash_permutation
from analysis_scripts_final import calculate_sequence_score, verify_permutation_coverage, find_prodigal_results, calculate_winners_losers, identify_anti_prodigals

class Reconfigurator:
    """
    Provides methods for reconfiguring (optimizing) existing superpermutations.
    """

    def __init__(self, n, winners, losers, layout_memory, laminates, anti_laminates):
        self.n = n
        self.winners = winners
        self.losers = losers
        self.layout_memory = layout_memory
        self.laminates = laminates
        self.anti_laminates = anti_laminates

    def reconfigure_superpermutation(self, superpermutation, max_attempts=1000, initial_temperature=1.0, cooling_rate=0.995, mutation_rate=0.1):
        """Attempts to improve a superpermutation using local modifications and simulated annealing.

        Args:
            superpermutation (str): The superpermutation string to reconfigure.
            max_attempts (int): Maximum number of reconfiguration attempts.
            initial_temperature (float): Initial temperature for simulated annealing.
            cooling_rate (float): Cooling rate for simulated annealing.

        Returns:
            str: The (potentially) improved superpermutation string.
        """

        current_superpermutation = list(superpermutation) # Work on a list for mutability
        current_score = calculate_sequence_score("".join(current_superpermutation), self.n, self.winners, self.losers, self.layout_memory, self.laminates, self.anti_laminates)
        temperature = initial_temperature
        
        for attempt in range(max_attempts):
            logging.debug(f"Reconfiguration attempt: {attempt + 1}, Temperature: {temperature:.4f}, Current Score: {current_score}")
            # Choose a mutation type randomly
            mutation_type = random.choice(["swap", "insert", "delete", "reverse", "kmer_swap"])

            if mutation_type == "swap":
                new_superpermutation = self.mutate_swap(current_superpermutation.copy()) #Must work on a copy
            elif mutation_type == "insert":
                new_superpermutation = self.mutate_insert(current_superpermutation.copy())
            elif mutation_type == "delete":
                new_superpermutation = self.mutate_delete(current_superpermutation.copy())
            elif mutation_type == "reverse":
                new_superpermutation = self.mutate_reverse(current_superpermutation.copy())
            elif mutation_type == "kmer_swap":
                new_superpermutation = self.mutate_kmer_swap(current_superpermutation.copy())
            else:
                new_superpermutation = current_superpermutation #Should not happen

            if new_superpermutation: #If a change was actually made
                new_score = calculate_sequence_score("".join(new_superpermutation), self.n, self.winners, self.losers, self.layout_memory, self.laminates, self.anti_laminates)

                # Decide whether to accept the change
                if self.accept_change(current_score, new_score, temperature):
                    current_superpermutation = new_superpermutation
                    current_score = new_score
                    logging.debug(f"Accepted mutation: {mutation_type}")

            # Cool down
            temperature *= cooling_rate
            if not verify_permutation_coverage("".join(current_superpermutation), self.n):
                return None #Not valid, so we discard.

        return "".join(current_superpermutation)

    def accept_change(self, old_score, new_score, temperature):
        """Decides whether to accept a change based on simulated annealing."""
        if new_score > old_score:
            return True  # Always accept improvements
        else:
            probability = math.exp((new_score - old_score) / temperature)
            return random.random() < probability

    def mutate_swap(self, superpermutation):
        """Swaps two permutations within a window."""
        window_size = self.n * 5  # Example window size
        start_index = random.randint(0, max(0, len(superpermutation) - window_size))
        end_index = min(len(superpermutation), start_index + window_size)

        # Find two valid permutation indices within the window
        valid_indices = []
        for i in range(start_index, end_index - self.n + 1):
            perm = tuple(int(x) for x in superpermutation[i:i + self.n])
            if is_valid_permutation(perm, self.n):
                valid_indices.append(i)

        if len(valid_indices) < 2:
            return None  # Not enough valid permutations to swap

        idx1, idx2 = random.sample(valid_indices, 2)
        perm1_slice = slice(idx1, idx1 + self.n)
        perm2_slice = slice(idx2, idx2 + self.n)
        superpermutation[perm1_slice], superpermutation[perm2_slice] = superpermutation[perm2_slice], superpermutation[perm1_slice]
        return superpermutation

    def mutate_insert(self, superpermutation):
        """Inserts a permutation at a random location."""
        #Insert a missing permutation
        missing_permutations = set()
        all_permutations = generate_permutations(self.n)
        s_tuple = tuple(int(x) for x in superpermutation)

        for p in all_permutations:
            found = False
            for i in range(len(s_tuple) - self.n + 1):
                if s_tuple[i:i+self.n] == p:
                    found = True
                    break
            if not found:
                missing_permutations.add(hash_permutation(p))
        candidates = generate_completion_candidates_n8("".join(superpermutation), missing_permutations, {}, self.winners, self.losers, self.n, [], [], [])

        if not candidates:
            return None

        insertion_point = random.randint(0, len(superpermutation))
        new_perm_hash = random.choice(list(candidates))
        new_perm = unhash_permutation(new_perm_hash, self.n)
        superpermutation = superpermutation[:insertion_point] + list(str(x) for x in new_perm) + superpermutation[insertion_point:]
        return superpermutation

    def mutate_delete(self, superpermutation):
        """Deletes a random permutation."""
        #Find a random permutation
        delete_pos = random.randint(0, len(superpermutation) - self.n)
        perm = tuple(int(x) for x in superpermutation[delete_pos:delete_pos + self.n])
        if is_valid_permutation(perm, self.n):
          superpermutation = superpermutation[:delete_pos] + superpermutation[delete_pos + self.n:] #Cut out that section.
          return superpermutation
        return None # Do nothing if not valid

    def mutate_reverse(self, superpermutation):
        """Reverses a random subsequence."""
        # Select a random subsequence of permutations
        start_pos = random.randint(0, len(superpermutation) - self.n)
        end_pos = random.randint(start_pos + self.n, min(len(superpermutation), start_pos + self.n * 5))  # Limit length

        #Make sure we are reversing valid perms
        valid = True
        for i in range(start_pos, end_pos -self.n + 1):
          perm = tuple(int(x) for x in superpermutation[i:i+self.n])
          if not is_valid_permutation(perm, self.n):
            valid = False
            break
        if valid:
          superpermutation[start_pos:end_pos] = superpermutation[start_pos:end_pos][::-1]  # Reverse that section
          return superpermutation
        return None

    def mutate_kmer_swap(self, superpermutation):
        """Swaps two overlapping k-mers within a window."""
        window_size = self.n * 3  # Example window size
        start_index = random.randint(0, max(0, len(superpermutation) - window_size))
        end_index = min(len(superpermutation), start_index + window_size)

        # Find two valid k-mer indices within the window
        k = self.n -1
        valid_indices = []
        for i in range(start_index, end_index - k + 1):
            kmer = tuple(int(x) for x in superpermutation[i:i + k])
            #We don't have to, but for consistency, we will check if the kmers are part of a valid permutation.
            if i > 0: #Need a previous perm
              perm = tuple(int(x) for x in superpermutation[i-1:i-1+self.n])
              if is_valid_permutation(perm, self.n):
                valid_indices.append(i)

        if len(valid_indices) < 2:
            return None  # Not enough valid k-mers to swap

        idx1, idx2 = random.sample(valid_indices, 2)
        kmer1_slice = slice(idx1, idx1 + k)
        kmer2_slice = slice(idx2, idx2 + k)
        superpermutation[kmer1_slice], superpermutation[kmer2_slice] = superpermutation[kmer2_slice], superpermutation[kmer1_slice]
        return superpermutation
```

### src/unified/embedder/cand_34_register_agents_py.py

```python
from agents.builder import BuilderAgent
from agents.validator import ValidatorAgent
from agents.audit import AuditAgent
from agents.foreman import ForemanAgent
from agents.teacher import TeacherAgent
from agents.clerk import ClerkAgent
from agents.janitor import JanitorAgent

class AgentManager:
    def __init__(self):
        self.agents = {}
        self.execution_order = []

    def register(self, name, agent):
        self.agents[name] = agent
        self.execution_order.append(name)

    def inject_agent(self, name, agent):
        self.agents[name] = agent
        if name not in self.execution_order:
            self.execution_order.append(name)

    def get_agent(self, name):
        return self.agents.get(name, None)

    def step_all(self, state):
        for name in self.execution_order:
            agent = self.agents.get(name)
            if agent and hasattr(agent, 'step'):
                agent.step(state)

    def replace_from_strategy(self, agent_map):
        from agents.validator import RobustValidator
        from core.patch_engine import PatchEngine
        from patch_strategies.default_patch import DefaultPatchStrategy
        from patch_strategies.vector_prune import VectorPrunePatchStrategy

        strategy_map = {
            "robust_validator": RobustValidator,
            "default_patch": lambda: PatchEngine(DefaultPatchStrategy()),
            "vector_prune": lambda: PatchEngine(VectorPrunePatchStrategy())
        }

        for name, key in agent_map.items():
            if key in strategy_map:
                self.inject_agent(name, strategy_map[key]())

def register_agents():
    mgr = AgentManager()
    mgr.register("builder", BuilderAgent())
    mgr.register("validator", ValidatorAgent())
    mgr.register("audit", AuditAgent())
    mgr.register("foreman", ForemanAgent())
    mgr.register("teacher", TeacherAgent())
    mgr.register("clerk", ClerkAgent())
    mgr.register("janitor", JanitorAgent())
    return mgr

```

### src/unified/embedder/cand_35_register_agents_runtime_patched_py.py

```python
from agents.builder import BuilderAgent
from agents.validator import ValidatorAgent
from agents.audit import AuditAgent
from agents.foreman import ForemanAgent
from agents.teacher import TeacherAgent
from agents.clerk import ClerkAgent
from agents.janitor import JanitorAgent

class AgentManager:
    def __init__(self, runtime_settings=None):
        self.agents = {}
        self.execution_order = []
        self.runtime_settings = runtime_settings or {}

    def register(self, name, agent):
        self.agents[name] = agent
        self.execution_order.append(name)

    def inject_agent(self, name, agent):
        self.agents[name] = agent
        if name not in self.execution_order:
            self.execution_order.append(name)

    def get_agent(self, name):
        return self.agents.get(name, None)

    def step_all(self, state):
        for name in self.execution_order:
            agent = self.agents.get(name)
            if agent and hasattr(agent, 'step'):
                agent.step(state)

    def replace_from_strategy(self, agent_map):
        from agents.validator import RobustValidator
        from core.patch_engine import PatchEngine
        from patch_strategies.default_patch import DefaultPatchStrategy
        from patch_strategies.vector_prune import VectorPrunePatchStrategy

        strategy_map = {
            "robust_validator": RobustValidator,
            "default_patch": lambda: PatchEngine(DefaultPatchStrategy()),
            "vector_prune": lambda: PatchEngine(VectorPrunePatchStrategy())
        }

        for name, key in agent_map.items():
            if key in strategy_map:
                self.inject_agent(name, strategy_map[key]())

def register_agents(runtime_settings=None):
    mgr = AgentManager(runtime_settings)
    mgr.register("builder", BuilderAgent())
    mgr.register("validator", ValidatorAgent())
    mgr.register("audit", AuditAgent())
    mgr.register("foreman", ForemanAgent())
    mgr.register("teacher", TeacherAgent(runtime_settings))
    mgr.register("clerk", ClerkAgent())
    mgr.register("janitor", JanitorAgent())
    return mgr

```

### src/unified/embedder/cand_36_sequence_generation_py.py

```python
# sequence_generation.py
import itertools
import random
import math
from utils import is_valid_permutation, calculate_overlap, hash_permutation, unhash_permutation, setup_logging, normalize_sequence, compute_checksum
#from analysis_scripts_final import find_prodigal_results #Removed, as we can use the main file.
#from construct_superpermutation import construct_superpermutation, generate_candidates, calculate_score, complete_from_partial
#from analysis_scripts_final import is_prodigal, find_prodigal_results, calculate_winners_losers, identify_anti_prodigals, calculate_sequence_winners_losers
from analysis_scripts_final import is_prodigal, find_prodigal_results, calculate_winners_losers, identify_anti_prodigals, calculate_sequence_winners_losers, calculate_segment_efficiency, approximate_derivative, calculate_discrepancy, identify_mega_winners, identify_mega_losers
from laminate_utils import create_laminate, is_compatible, create_anti_laminate, analyze_laminate_density, analyze_laminate_connectivity, update_laminate, merge_laminates, create_n7_constraint_laminate
from layout_memory import LayoutMemory
from graph_utils import build_de_bruijn_graph, add_weights_to_debruijn, find_high_weight_paths, analyze_debruijn_graph
from prodigal_manager import ProdigalManager

def generate_n_minus_1_superpermutation(n, seed):
    """Generates a distinct superpermutation for n-1.
    """
    #We will use our modified n-1 code.

    # --- Constants for n-1 ---
    prodigal_overlap_threshold = 0.98
    prodigal_min_length = 5 #Reduced for testing
    if n-1 > 6:
      prodigal_min_length = int(0.75 * math.factorial(n-1))
    hypothetical_prodigal_overlap_threshold = 0.90
    hypothetical_prodigal_min_length = 4
    hypothetical_prodigal_generation_count = 50  # n-1 is fast
    num_iterations = 1000 #Should be enough.
    layout_k_values = [n - 2, n - 3]

    if seed is not None:
        random.seed(seed)

    # Initialize data structures
    prodigal_manager = ProdigalManager(n-1) # Using prodigal manager

    # Load initial data, if not supplied, it starts with default values.
    winners = {}
    losers = {}

    meta_hierarchy = {}  # Track strategy effectiveness
    limbo_list = set()  # Set of permutation hashes.
    eput = {}  # The Enhanced Permutation Universe Tracker

    # Add initial n-1 prodigals, which in this case is ALWAYS the original 5906
    next_prodigal_id = 0
    

    #Create Initial Laminate, using all available n-7s
    laminates = []
    anti_laminates = []

    layout_memory = LayoutMemory() #Create a layout memory
    superpermutation = ""
    # --- Main Iterative Loop ---
    #for iteration in range(num_iterations): #Now stops when complete, or stuck, or iteration limit.
    iteration = 0
    while len(superpermutation) < sum(math.factorial(i) for i in range(1,n)) and iteration < num_iterations:
        iteration += 1
        #print(f"Starting iteration {iteration}...") #Removed for n=8 use.
        start_time = time.time()

        # 1. Generate Hypothetical Prodigals
        hypothetical_prodigals = generate_hypothetical_prodigals(prodigal_results, winners, losers, n-1, num_to_generate=hypothetical_prodigal_generation_count, min_length=hypothetical_prodigal_min_length, max_length=(n-1)*15) #Longer length limit, due to n=7 speed
        #print(f"  Generated {len(hypothetical_prodigals)} hypothetical prodigals.")

        #Add to the prodigals list
        for h_id, h_prodigal in hypothetical_prodigals.items():
            prodigal_manager.add_prodigal(h_prodigal['sequence'], n-1, "hypothetical") ##########Needs to add all data, not just sequence.
            next_prodigal_id += 1
            # Create and add a laminate for the new prodigal
            laminates.append(create_laminate(h_prodigal.sequence, n-1, n-2))
            laminates.append(create_laminate(h_prodigal.sequence, n-1, n-3))

        #Find best prodigals
        best_prodigals = prodigal_manager.get_best_prodigals(n=n-1, task="generation", context={})
        # 2. Construct Superpermutation (using the dynamic approach, starting from EMPTY)
        superpermutation, used_permutations = construct_superpermutation([], best_prodigals, winners, losers, layout_memory, meta_hierarchy, limbo_list, n-1, hypothetical_prodigals, laminates)
        #print(f"  Superpermutation length: {len(superpermutation)}")

        # 3. Update Data
        analysis_results = analyze_superpermutation(superpermutation, n-1)
        #print(f"  Valid: {analysis_results['validity']}")
        #print(f"  Overlap Distribution: {analysis_results['overlap_distribution']}")

        # Check for  length and distinctness
        if analysis_results['validity'] and len(superpermutation) == sum(math.factorial(i) for i in range(1,n)):
            return superpermutation

        # Find and add new prodigal results. Stricter criteria.
        new_prodigals = find_prodigal_results(superpermutation, n-1, overlap_threshold=prodigal_overlap_threshold,  min_length=prodigal_min_length)
        for prodigal_seq in new_prodigals:
            prodigal_manager.add_prodigal(prodigal_seq, n-1, "dynamic_generation")

        # Update "Winners" and "Losers" (using the new superpermutation, and k=6 and k=5)
        new_winners6, new_losers6 = calculate_winners_losers([superpermutation], n-1, k=n-2)
        new_winners5, new_losers5 = calculate_winners_losers([superpermutation], n-1, k=n-3)

        for kmer, weight in new_winners6.items():
            winners[kmer] = winners.get(kmer, 0) + weight
        for kmer, weight in new_losers6.items():
            losers[kmer] = losers.get(kmer, 0) + weight
            limbo_list.add(kmer) # Add to limbo list
        for kmer, weight in new_winners5.items():
            winners[kmer] = winners.get(kmer, 0) + weight
        for kmer, weight in new_losers5.items():
            losers[kmer] = losers.get(kmer, 0) + weight
            limbo_list.add(kmer)

        #Higher Order Winners/Losers
        new_seq_winners, new_seq_losers = calculate_sequence_winners_losers([superpermutation],n-1)
        for seq_hash, weight in new_seq_winners.items():
            if seq_hash in winners:
                winners[seq_hash] += weight
            else:
                winners[seq_hash] = weight
        for seq_hash, weight in new_seq_losers.items():
            if seq_hash in losers:
                losers[seq_hash] += weight
            else:
                losers[seq_hash] = weight

        # Update ePUT (add all *used* permutations)
        s_tuple = tuple(int(x) for x in superpermutation)
        for i in range(len(s_tuple) - (n-1) + 1):
            perm = s_tuple[i:i+(n-1)]
            if is_valid_permutation(perm, n-1):
                perm_hash = hash_permutation(perm)
                if perm_hash not in eput:
                    eput[perm_hash] = PermutationData(perm, in_sample=False, creation_method="dynamic_generation")
                eput[perm_hash].used_count += 1
                eput[perm_hash].used_in_final = True
                # Update neighbors in ePUT (using consistent k values)
                if i > 0:
                    prev_perm = s_tuple[i-1:i-1+(n-1)]
                    if is_valid_permutation(prev_perm, n-1):
                        eput[perm_hash].neighbors.add(hash_permutation(prev_perm))
                if i < len(s_tuple) - (n-1):
                    next_perm = s_tuple[i+1:i+1+(n-1)]
                    if is_valid_permutation(next_perm, n-1):
                        eput[perm_hash].neighbors.add(hash_permutation(next_perm))

        # Update Layout Memory
        layout_memory.add_sequence(superpermutation, n-1, n-2, f"run_{seed}")  # Add the new superpermutation
        layout_memory.add_sequence(superpermutation, n-1, n-3, f"run_{seed}")

        # Update "Meta-Hierarchy" (simplified for this example)
        meta_hierarchy.setdefault("run_lengths", []).append(len(superpermutation))
        meta_hierarchy.setdefault("prodigal_counts", []).append(len(prodigal_results))
        total_hypotheticals = len(hypothetical_prodigals)
        successful_hypotheticals = 0
        for h_id, h_prodigal in hypothetical_prodigals.items():
            if h_prodigal.sequence in superpermutation:
                successful_hypotheticals += 1
        success_rate = (successful_hypotheticals / total_hypotheticals) if total_hypotheticals > 0 else 0.0

        meta_hierarchy.setdefault("hypothetical_success_rate",[]).append(success_rate)
        # --- Dynamic Parameter Adjustments (Example) ---
        if len(meta_hierarchy["prodigal_counts"]) > 2 and meta_hierarchy["prodigal_counts"][-1] <= meta_hierarchy["prodigal_counts"][-2]:
            # Prodigal discovery rate is slowing down
            prodigal_overlap_threshold = max(0.90, prodigal_overlap_threshold - 0.01)  # Decrease threshold, but not below 0.90
            prodigal_min_length = max(7, prodigal_min_length - 2) # Reduce Min Length, not below 7.
            #print(f"  Adjusted Prodigal Criteria: Overlap Threshold = {prodigal_overlap_threshold}, Min Length = {prodigal_min_length}") # For Debugging

        #Stopping condition for n-1
        if len(superpermutation) == sum(math.factorial(i) for i in range(1,n)):
            return superpermutation

    return None  # Failed to find a  superpermutation within the iteration limit

def generate_distinct_n7_superpermutations(num_distinct, output_file, initial_seed=42):
    """Generates multiple distinct minimal superpermutations for n=7 and saves them to a file."""

    distinct_superpermutations = set()
    seed = initial_seed
    attempts = 0

    while len(distinct_superpermutations) < num_distinct and attempts < 1000:  # Added attempt limit
        attempts += 1
        superpermutation = generate_n_minus_1_superpermutation(8, seed)  # Generate for n=7 (8-1)

        if superpermutation:  # Check if generation was successful
            if len(superpermutation) == 5906:
                normalized_sp = normalize_sequence(superpermutation)
                checksum = compute_checksum(normalized_sp)
                if checksum not in distinct_superpermutations:
                    distinct_superpermutations.add(checksum)
                    with open(output_file, "a") as f:  # Append to file
                        f.write(normalized_sp + "\n")
                    print(f"Found distinct superpermutation (seed {seed}): {normalized_sp[:50]}... (length: {len(normalized_sp)})")
                #else: # Keep for debugging, but not needed for final runs.
                    #print(f"Duplicate superpermutation found (seed {seed}).")
        #else: # Keep for debugging, not needed for final runs.
            #print(f"Superpermutation generation failed (seed {seed}).")

        seed += 1  # Increment seed

    print(f"Generated {len(distinct_superpermutations)} distinct n=7 superpermutations.")
```

### src/unified/embedder/cand_39_utils_py.py

```python
utils.py

# utils.py
import itertools
import hashlib
import logging

def setup_logging():
    """Sets up logging to a file."""
    logging.basicConfig(level=logging.DEBUG, filename='superpermutation.log', filemode='w', format='%(asctime)s - %(levelname)s - %(message)s')

def compute_checksum(data: str) -> str:
    """Computes the SHA-256 checksum of a string."""
    return hashlib.sha256(data.encode('utf-8')).hexdigest()

def generate_permutations(n: int):
    """Generates all permutations of numbers from 1 to n."""
    return list(itertools.permutations(range(1, n + 1)))

def is_valid_permutation(perm: tuple, n: int) -> bool:
    """Checks if a tuple is a valid permutation."""
    return len(perm) == n and set(perm) == set(range(1, n + 1))

def calculate_overlap(s1: str, s2: str) -> int:
    """Calculates the maximum overlap between two strings."""
    max_len = min(len(s1), len(s2))
    for i in range(max_len, 0, -1):
        if s1[-i:] == s2[:i]:
            return i
    return 0

def normalize_sequence(seq: str) -> str:
    """Normalizes a sequence by rotating it."""
    min_char = min(seq)
    min_indices = [i for i, c in enumerate(seq) if c == min_char]
    rotations = [seq[i:] + seq[:i] for i in min_indices]
    return min(rotations)

def kmer_to_int(kmer):
    """Converts a k-mer tuple to an integer."""
    int_kmer = 0
    for i, digit in enumerate(kmer):
        int_kmer = int_kmer * 10 + digit
    return int_kmer

def int_to_kmer(int_kmer, k):
    """Converts an integer to a k-mer tuple."""
    kmer = ()
    for _ in range(k):
        digit = int_kmer % 10
        kmer = (digit,) + kmer
        int_kmer //= 10
    return kmer

def hash_permutation(perm):
    """Hashes a permutation (tuple or int)."""
    if isinstance(perm, tuple):
        return hash(perm)
    elif isinstance(perm, int):
        return perm
    else:
        raise TypeError("Permutation must be int or tuple")

def unhash_permutation(perm_hash, n):
    """Unhashes a permutation (int to tuple)."""
    if isinstance(perm_hash, int):
        return int_to_kmer(perm_hash, n)
    else:
        raise TypeError("Permutation hash must be int")
```

### src/unified/embedder/cand_3_generation_code_n7_dynamic_final_py.py

```python
import itertools
import random
import math
import time
import networkx as nx
from collections import deque, defaultdict
import heapq
import analysis_scripts
from layout_memory import LayoutMemory

# --- Constants ---
N = 7  # The value of n (number of symbols).
PRODIGAL_OVERLAP_THRESHOLD = 0.98  # Initial minimum overlap rate for "Prodigal Results"
PRODIGAL_MIN_LENGTH = 10 #Reduced
HYPOTHETICAL_PRODIGAL_OVERLAP_THRESHOLD = 0.90 #Reduced
HYPOTHETICAL_PRODIGAL_MIN_LENGTH = 7
HYPOTHETICAL_PRODIGAL_GENERATION_COUNT = 50 # Number of Hypothetical Prodigals to Generate
WINNER_THRESHOLD = 0.75  # Not directly used in scoring
LOSER_THRESHOLD = 0.25  # Not directly used in scoring
NUM_ITERATIONS = 10000  #  Set a large number; it will likely stop earlier
LAYOUT_K_VALUES = [N - 1, N - 2]  # k values for Layout Memory
DE_BRUIJN_K_VALUES = [N - 1, N - 2]  # k values for De Bruijn graph generation
RANDOM_SEED = 42  # For reproducibility.  CHANGE THIS FOR EACH RUN.

# --- Helper Functions --- (These are identical to the n=8 versions)
def calculate_overlap(s1: str, s2: str) -> int:
    """Calculates the maximum overlap between two strings."""
    max_overlap = 0
    for i in range(1, min(len(s1), len(s2)) + 1):
        if s1[-i:] == s2[:i]:
            max_overlap = i
    return max_overlap

def generate_permutations(n: int) -> list[tuple[int, ...]]:
    """Generates all permutations of 1 to n."""
    return list(itertools.permutations(range(1, n + 1)))

def calculate_distance(p1: str, p2: str, n: int) -> int:
    """Calculates distance (n-1 - overlap)."""
    return (n - 1) - max(calculate_overlap(p1, p2), calculate_overlap(p2, p1))

def hash_permutation(permutation: tuple) -> int:
    """Hashes a permutation tuple to a unique integer."""
    result = 0
    n = len(permutation)
    for i, val in enumerate(permutation):
        result += val * (n ** (n - 1 - i))
    return result

def unhash_permutation(hash_value: int, n: int) -> tuple:
    """Converts a hash value back to a permutation tuple."""
    permutation = []
    for i in range(n - 1, -1, -1):
        val = hash_value // (n ** i)
        permutation.append(val + 1)  # Adjust to be 1-indexed
        hash_value -= val * (n ** i)
    return tuple(permutation)

def is_valid_permutation(perm: tuple, n: int) -> bool:
    """Checks if a given sequence is a valid permutation."""
    return len(set(perm)) == n and min(perm) == 1 and max(perm) == n

def calculate_golden_ratio_points(length: int, levels: int = 1) -> list[int]:
    """Calculates multiple levels of golden ratio points."""
    phi = (1 + math.sqrt(5)) / 2
    points = []
    for _ in range(levels):
        new_points = []
        if not points:
            new_points = [int(length / phi), int(length - length / phi)]
        else:
            for p in points:
                new_points.extend([int(p / phi), int(p - p / phi)])
                new_points.extend([int((length-p) / phi) + p, int(length - (length-p) / phi)])
        points.extend(new_points)
        points = sorted(list(set(points)))  # Remove duplicates and sort
    return points

def get_kmers(sequence: str, n: int, k: int) -> set[str]:
    """Extracts all k-mers from a sequence, given n and k."""
    kmers = set()
    seq_list = [int(x) for x in sequence]
    for i in range(len(sequence) - n + 1):
        perm = tuple(seq_list[i:i+n])
        if is_valid_permutation(perm, n):
            if i >= k:
                kmer = "".join(str(x) for x in seq_list[i-k:i])
                kmers.add(kmer)
    return kmers
# --- Data Structures --- (Same as n=8 version)

class PermutationData:
    def __init__(self, permutation: tuple, in_sample: bool = False, creation_method: str = ""):
        """Stores data associated with a single permutation."""
        self.hash: int = hash_permutation(permutation)
        self.permutation: tuple = permutation
        self.in_sample: bool = in_sample  # Will always be False
        self.used_count: int = 0
        self.prodigal_status: list[int] = []
        self.creation_method: str = creation_method
        self.batch_ids: list[int] = [] #Not used
        self.used_in_final: bool = False
        self.neighbors: set[int] = set()

    def __str__(self) -> str:
        return (f"PermutationData(hash={self.hash}, permutation={self.permutation}, used_count={self.used_count}, "
                f"prodigal_status={self.prodigal_status}, creation_method={self.creation_method})")

    def __repr__(self) -> str:
        return self.__str__()

class ProdigalResult:
    def __init__(self, sequence: str, result_id: int):
        """Represents a 'Prodigal Result'."""
        self.id: int = result_id
        self.sequence: str = sequence
        self.length: int = len(sequence)
        self.permutations: set[int] = set()  # Store hashes
        self.calculate_permutations()
        self.overlap_rate: float = self.calculate_overlap_rate()

    def calculate_permutations(self):
        """Calculates and stores the set of permutations."""
        n = N
        for i in range(len(self.sequence) - n + 1):
            perm = tuple(int(x) for x in self.sequence[i:i + n])
            if is_valid_permutation(perm, n):
                self.permutations.add(hash_permutation(perm))

    def calculate_overlap_rate(self) -> float:
        """Calculates the overlap rate."""
        n = N
        total_length = sum(len(str(p)) for p in [unhash_permutation(x,n) for x in self.permutations])
        overlap_length = total_length - len(self.sequence)
        max_possible_overlap = (len(self.permutations) - 1) * (n - 1)
        if max_possible_overlap == 0:
            return 0
        return overlap_length / max_possible_overlap

    def __str__(self) -> str:
        return (f"ProdigalResult(id={self.id}, length={self.length}, "
                f"overlap_rate={self.overlap_rate:.4f}, num_permutations={len(self.permutations)})")

    def __repr__(self) -> str:
        return self.__str__()
# --- Initialization Function ---
def initialize_data(initial_n7_prodigals: list[str], initial_winners: dict, initial_losers: dict) -> tuple:
    """Initializes the data structures for the algorithm.
        Modified for n=7, no initial superpermutation, loads initial prodigals.
    Args:
        initial_n7_prodigals (list[str]): A list of initial n=7 Prodigal Result strings.
        initial_winners (dict):  Initial Winner k-mers and weights.
        initial_losers (dict): Initial Loser k-mers and weights.

    Returns:
        tuple: (prodigal_results, winners, losers, meta_hierarchy, limbo_list, eput, next_prodigal_id)
    """
    prodigal_results = {}  # {prodigal_id: ProdigalResult object}
    winners = initial_winners  # {kmer: weight}
    losers = initial_losers  # {kmer: weight}
    meta_hierarchy = {}  # Track strategy effectiveness
    limbo_list = set()  # Set of permutation hashes.
    eput = {}  # The Enhanced Permutation Universe Tracker

    # Add initial n=7 prodigals
    next_prodigal_id = 0
    for prodigal in initial_n7_prodigals:
        prodigal_results[next_prodigal_id] = ProdigalResult(prodigal, next_prodigal_id)
        next_prodigal_id += 1

    return prodigal_results, winners, losers, meta_hierarchy, limbo_list, eput, next_prodigal_id

# --- On-Demand Permutation Generation ---
def generate_permutations_on_demand(prefix: str, suffix: str, prodigal_results: dict, winners: dict, losers: dict,
                                   n: int, eput: dict, limbo_list: set, min_overlap: int,
                                   hypothetical_prodigals: dict = None, laminate_graphs: list = None) -> set[int]:
    """Generates permutations on demand, extending a given prefix or suffix.
       Prioritizes "Prodigal" extensions, uses "Winners" and "Losers," and
       avoids already-used permutations and the "Limbo List."  Also uses laminates.

    Args:
        prefix (str): The prefix of the current superpermutation.
        suffix (str): The suffix of the current superpermutation.
        prodigal_results (dict): Dictionary of ProdigalResult objects.
        winners (dict): Dictionary of Winner k-mers and their weights.
        losers (dict): Dictionary of Loser k-mers and their weights.
        n (int): The value of n.
        eput (dict): The Enhanced Permutation Universe Tracker.
        limbo_list (set): The set of permutation hashes to avoid.
        min_overlap (int):  The minimum required overlap.
        hypothetical_prodigals (dict): Optional dictionary of Hypothetical Prodigals
        laminate_graphs (list): List of laminate graphs.

    Returns:
        set[int]: A set of *permutation hashes* that are potential extensions.
    """
    valid_permutations = set()

    # Prioritize Hypothetical Prodigal extensions
    if hypothetical_prodigals:
        for prodigal_id, prodigal in hypothetical_prodigals.items():
            if prefix and prodigal.sequence.startswith(prefix[-min_overlap:]):
                for i in range(len(prodigal.sequence) - (n - 1)):
                    perm = tuple(int(x) for x in prodigal.sequence[i:i + n])
                    if analysis_scripts.is_valid_permutation(perm, n) and hash_permutation(perm) not in eput:
                         valid_permutations.add(hash_permutation(perm))
            if suffix and prodigal.sequence.endswith(suffix[:min_overlap]):
                for i in range(len(prodigal.sequence) - (n - 1)):
                    perm = tuple(int(x) for x in prodigal.sequence[i:i + n])
                    if analysis_scripts.is_valid_permutation(perm, n) and hash_permutation(perm) not in eput:
                        valid_permutations.add(hash_permutation(perm))

    # Prioritize Prodigal extensions
    for prodigal_id, prodigal in prodigal_results.items():
        if prefix and prodigal.sequence.startswith(prefix[-min_overlap:]):
            for i in range(len(prodigal.sequence) - (n - 1)):
                perm = tuple(int(x) for x in prodigal.sequence[i:i + n])
                if analysis_scripts.is_valid_permutation(perm, n) and hash_permutation(perm) not in eput:
                    valid_permutations.add(hash_permutation(perm))
        if suffix and prodigal.sequence.endswith(suffix[:min_overlap]):
            for i in range(len(prodigal.sequence) - (n - 1)):
                perm = tuple(int(x) for x in prodigal.sequence[i:i + n])
                if analysis_scripts.is_valid_permutation(perm, n) and hash_permutation(perm) not in eput:
                    valid_permutations.add(hash_permutation(perm))


    # Filter based on Limbo List and Laminate
    filtered_permutations = set()
    for perm_hash in valid_permutations:
        perm = unhash_permutation(perm_hash, n)
        perm_str = "".join(str(x) for x in perm)
        is_in_limbo = False

        # Basic Loser check (using 5-mers and 6-mers for n=7)
        for k in [6, 5]:  # Check 6-mers and 5-mers
            for i in range(len(perm_str) - k + 1):
                kmer = perm_str[i:i+k]
                if kmer in limbo_list:
                    is_in_limbo = True
                    break
            if is_in_limbo:
                break
        
        if not is_in_limbo:
            valid_perm = False
            if laminate_graphs:
                for laminate in laminate_graphs:
                    if analysis_scripts.is_compatible(perm, laminate, n, n - 1):
                        valid_perm = True
                        break
                    elif analysis_scripts.is_compatible(perm, laminate, n, n - 2):
                        valid_perm = True
                        break
            else: #If no laminates, pass.
                valid_perm = True
            if valid_perm:
                filtered_permutations.add(perm_hash)

    return filtered_permutations

# --- Scoring Function ---
def calculate_score(current_superpermutation: str, permutation_hash: int, prodigal_results: dict,
                    winners: dict, losers: dict, layout_memory: LayoutMemory, n: int,
                    golden_ratio_points: list[int], hypothetical_prodigals:dict) -> float:
    """Calculates the score for adding a permutation (n=7 version)."""
    permutation = unhash_permutation(permutation_hash, n)
    permutation_string = "".join(str(x) for x in permutation)
    overlap = calculate_overlap(current_superpermutation, permutation_string)
    score = overlap * 5  # Base score based on overlap

    # Prodigal Result Bonus (very high)
    prodigal_bonus = 0
    for prodigal_id, prodigal in prodigal_results.items():
        if permutation_string in prodigal.sequence:
            prodigal_bonus += prodigal.length * 100  # Large bonus
            break

    #Hypothetical bonus
    hypothetical_bonus = 0
    if hypothetical_prodigals:
        for h_prodigal_id, h_prodigal in hypothetical_prodigals.items():
            if permutation_string in h_prodigal.sequence:
                hypothetical_bonus +=  h_prodigal.length * 25

    # Winner and Loser Bonus/Penalty (using Layout Memory)
    k_values = [n - 1, n - 2]
    layout_bonus = 0
    for k in k_values:
        if len(current_superpermutation) >= k:
            kmer_end = current_superpermutation[-k:]
            kmer_start = permutation_string[:k]
            layout_bonus += layout_memory.get_layout_score(kmer_end, kmer_start, 1)

    # Golden Ratio Bonus (small, dynamic)
    golden_ratio_bonus = 0
    insertion_point = len(current_superpermutation)
    for point in golden_ratio_points:
        distance = abs(insertion_point - point)
        golden_ratio_bonus += math.exp(-distance / (len(current_superpermutation)/20))  # Smaller divisor = tighter

    # Loser Penalty (Veto)
    loser_penalty = 0
    for k in [6, 5]:  # Check 6-mers and 5-mers for n=7
        for i in range(len(permutation_string) - k + 1):
            kmer = permutation_string[i:i+k]
            loser_penalty += losers.get(kmer, 0) * 5 # Penalty for losers

    # Higher-Order Winners/Losers
    higher_order_bonus = 0
    for seq_length in [2, 3]:  # Check sequences of length 2 and 3
      if len(current_superpermutation) >= (n * seq_length):
        prev_seq = current_superpermutation[-(n*seq_length):]
        prev_perms = []
        for i in range(len(prev_seq) - n + 1):
            pp = tuple(int(x) for x in prev_seq[i:i+n])
            if is_valid_permutation(pp, n):
                prev_perms.append(hash_permutation(pp))
        if len(prev_perms) >= (seq_length -1):
            current_seq = tuple(prev_perms[-(seq_length - 1):] + [permutation_hash])
            current_seq_hash = hash(current_seq)
            higher_order_bonus += winners.get(current_seq_hash, 0) * 5
            loser_penalty += losers.get(current_seq_hash, 0) * 5

    score += prodigal_bonus + layout_bonus + golden_ratio_bonus + hypothetical_bonus- loser_penalty + higher_order_bonus

    return score
# --- Main Construction Function ---

def construct_superpermutation(initial_permutations: list, prodigal_results: dict, winners: dict, losers: dict,
                              layout_memory: LayoutMemory, meta_hierarchy: dict, limbo_list: set, n: int,
                              hypothetical_prodigals: dict) -> tuple[str, set[int]]:
    """Constructs a superpermutation using the dynamic prodigal approach (n=7 version).

    Args:
        initial_permutations (list):  Empty list
        prodigal_results (dict): Dictionary of ProdigalResult objects.
        winners (dict): Dictionary of Winner k-mers and their weights.
        losers (dict): Dictionary of Loser k-mers and their weights.
        layout_memory (LayoutMemory): The LayoutMemory object.
        meta_hierarchy (dict): Dictionary for tracking strategy effectiveness.
        limbo_list (set): Set of permutation hashes to avoid.
        n (int): The value of n.
        hypothetical_prodigals (dict): "Hypothetical Prodigals" to use.

    Returns:
        tuple: (superpermutation string, set of used permutation hashes)
    """

    superpermutation = ""
    used_permutations = set()

    # Initialize with the longest "Prodigal Result" (should be the initial 5906)
    if prodigal_results:
        best_prodigal_key = max(prodigal_results, key=lambda k: prodigal_results[k].length)
        superpermutation = prodigal_results[best_prodigal_key].sequence
        used_permutations.update(prodigal_results[best_prodigal_key].permutations)
    else:
        #Should never reach here.
        superpermutation = "1234567" #Bare minimum to start.
        used_permutations.add(hash_permutation((1,2,3,4,5,6,7)))

    golden_ratio_points = calculate_golden_ratio_points(len(superpermutation), levels=3)
    #Create Laminates
    laminates = [analysis_scripts.create_laminate(prodigal_results[key].sequence, n, k) for key in prodigal_results for k in LAYOUT_K_VALUES]


    while True:  # Continue until no more additions can be made or 5906 is hit
        best_candidate = None
        best_score = -float('inf')
        best_candidate_string = ""

        # Find frontier k-mers
        prefix = superpermutation[:n - 1]
        suffix = superpermutation[-(n - 1):]

        # Generate candidates (very small set, focused on the frontier)
        candidates = generate_permutations_on_demand(prefix, suffix, prodigal_results, winners, losers, n, used_permutations, limbo_list, n - 1, hypothetical_prodigals, laminates)
        if not candidates:
            candidates = generate_permutations_on_demand(prefix, suffix, prodigal_results, winners, losers, n, used_permutations, limbo_list, n - 2, hypothetical_prodigals, laminates)
        if not candidates:
            # print("No candidates found. Stopping.")  # Keep for debugging
            break  # No more candidates can be added
        #Deterministic selection from candidates:
        scored_candidates = []
        for candidate_hash in candidates:
            score = calculate_score(superpermutation, candidate_hash, prodigal_results, winners, losers, layout_memory, n, golden_ratio_points, hypothetical_prodigals)
            scored_candidates.append((score, candidate_hash))
        
        best_candidate = None
        if scored_candidates:
            scored_candidates.sort(reverse=True, key=lambda item: item[0]) #Sort by score, DESCENDING
            best_candidate = scored_candidates[0][1] #Get the hash

        if best_candidate is not None:
            best_candidate_perm = unhash_permutation(best_candidate, n)
            overlap = calculate_overlap(superpermutation, "".join(str(x) for x in best_candidate_perm))
            superpermutation += "".join(str(x) for x in best_candidate_perm)[overlap:]  # Add to superpermutation
            used_permutations.add(best_candidate)

            # Update golden ratio points
            golden_ratio_points = calculate_golden_ratio_points(len(superpermutation))

            # Prodigal Result Check
            new_prodigals = analysis_scripts.find_prodigal_results(superpermutation, n, min_length=PRODIGAL_MIN_LENGTH, overlap_threshold=PRODIGAL_OVERLAP_THRESHOLD)  #Use current thresholds
            for prodigal_seq in new_prodigals:
                is_new = True
                for existing_prodigal_id, existing_prodigal in prodigal_results.items():
                    if prodigal_seq in existing_prodigal.sequence:
                        is_new = False
                        break
                if is_new:
                    new_id = len(prodigal_results) + 1
                    prodigal_results[new_id] = ProdigalResult(prodigal_seq, new_id)
                    # print(f"New Prodigal Result found: {prodigal_seq}")  # Keep for debugging
                    #Create and add laminates:
                    laminates.append(analysis_scripts.create_laminate(prodigal_seq, n, n-1))
                    laminates.append(analysis_scripts.create_laminate(prodigal_seq, n, n-2))

            # Update ePUT
            perm_hash = best_candidate
            perm_string = "".join(str(x) for x in best_candidate_perm)
            if perm_hash not in eput:  # Should always be true here
                eput[perm_hash] = PermutationData(best_candidate_perm, in_sample=False, creation_method="dynamic_generation")
            eput[perm_hash].used_count += 1
            eput[perm_hash].used_in_final = True
            # Update neighbors in ePUT (using consistent k values)
            for k in [n - 1, n - 2]:
                prefix = superpermutation[:k]
                suffix = superpermutation[-k:]
                prefix_perms = set()
                suffix_perms = set()
                for i in range(len(prefix) - n + 1):
                    p = tuple(int(x) for x in prefix[i:i+n])
                    if is_valid_permutation(p,n):
                        prefix_perms.add(hash_permutation(p))
                for i in range(len(suffix) - n + 1):
                    p = tuple(int(x) for x in suffix[i:i+n])
                    if is_valid_permutation(p,n):
                        suffix_perms.add(hash_permutation(p))

                for other_perm_hash in prefix_perms:
                    if other_perm_hash != perm_hash:
                        eput[perm_hash].neighbors.add(other_perm_hash)
                        # Add to layout memory if not exist
                        kmer1 = "".join(str(x) for x in unhash_permutation(other_perm_hash, n)[-k:])
                        kmer2 = perm_string[:k]
                        layout_memory.update_distances(kmer1,kmer2, len(superpermutation) - i - len(prefix), "n7_dynamic")
                for other_perm_hash in suffix_perms:
                    if other_perm_hash != perm_hash:
                        eput[perm_hash].neighbors.add(other_perm_hash)
                        kmer1 = perm_string[-k:]
                        kmer2 = "".join(str(x) for x in unhash_permutation(other_perm_hash, n)[:k])
                        layout_memory.update_distances(kmer1, kmer2, 1, "n7_dynamic")
        else:
            break  # If we get here, we are stuck

        if len(superpermutation) >= 5906: #If we reach the target, break.
            break

    return superpermutation, used_permutations

# --- Main Function ---
def main():
    """
    Main function to execute the Dynamic Prodigal Assembly algorithm for n=7.
    Generates multiple distinct 5906 superpermutations.
    """
    # File Paths
    initial_winners_losers_n7_file = "initial_winners_losers_n7.txt"
    prodigal_results_n7_file = "prodigal_results_n7.txt"
    distinct_superpermutations_file = "distinct_superpermutations_n7.txt"
    layout_memory_file = "layout_memory_n7.pkl"

    # Load Initial Data
    initial_winners, initial_losers = {}, {}
    try:
        with open(initial_winners_losers_n7_file, "r") as f:
            for line in f:
                kmer, w_type, weight = line.strip().split(",")
                if w_type == "winner":
                    initial_winners[kmer] = int(weight)
                else:
                    initial_losers[kmer] = int(weight)
    except FileNotFoundError:
        print("Initial Winners/Losers file not found. Starting with empty.")

    initial_n7_prodigals = []
    try:
        with open(prodigal_results_n7_file, "r") as f:
            initial_n7_prodigals = [line.strip() for line in f]
    except FileNotFoundError:
        print("Initial n=7 Prodigal Results file not found.  Starting with empty.")
        #In this case, we will create an empty file
        with open(prodigal_results_n7_file, "w") as f:
            pass
    # Load existing distinct superpermutations
    existing_superpermutations = set()
    try:
        with open(distinct_superpermutations_file, "r") as f:
            for line in f:
                existing_superpermutations.add(line.strip())
    except FileNotFoundError:
        print("No existing distinct superpermutations file found.")
        #Create File
        with open(distinct_superpermutations_file, "w") as f:
            pass

    # Initialize data structures
    prodigal_results, winners, losers, meta_hierarchy, limbo_list, eput, next_prodigal_id = initialize_data(
        "", initial_n7_prodigals, initial_winners, initial_losers
    )  # Start with an EMPTY string

    #Create Initial Laminate
    laminates = []
    for prodigal in initial_n7_prodigals:
        laminates.append(analysis_scripts.create_laminate(prodigal, N, N-1))
        laminates.append(analysis_scripts.create_laminate(prodigal, N, N-2))
    #If no initial, create blank laminate
    if not laminates:
        laminates = [nx.DiGraph()]
    
    # Load Layout memory
    layout_memory = LayoutMemory()
    try:
        layout_memory.load_from_file(layout_memory_file)
        print("Loaded LayoutMemory from file.")
    except FileNotFoundError:
        print("No existing LayoutMemory file found. Starting with a new one.")

    distinct_count = len(existing_superpermutations)
    run_count = 0

    # --- Main Iterative Loop ---
    while True: # Keep generating until a stopping criterion is met
        run_count += 1
        print(f"Starting run {run_count}...")
        start_time = time.time()

        # Set a new random seed for each run
        random.seed(RANDOM_SEED + run_count)  # Use a different seed each run

        # 1. Generate Hypothetical Prodigals
        hypothetical_prodigals = analysis_scripts.generate_hypothetical_prodigals(prodigal_results, winners, losers, N)
        print(f"  Generated {len(hypothetical_prodigals)} hypothetical prodigals.")

        # 2. Construct Superpermutation (Dynamic, Prodigal-Focused)
        superpermutation, used_permutations = construct_superpermutation([], prodigal_results, winners, losers, layout_memory, meta_hierarchy, limbo_list, N, hypothetical_prodigals)
        print(f"  Superpermutation length: {len(superpermutation)}")

        # 3. Analysis and Updates
        analysis_results = analysis_scripts.analyze_superpermutation(superpermutation, N)
        print(f"  Valid: {analysis_results['validity']}")
        print(f"  Overlap Distribution: {analysis_results['overlap_distribution']}")
        
        # Check for 5906 and distinctness
        if analysis_results['validity'] and len(superpermutation) == 5906:
            is_distinct = True
            for existing_sp in existing_superpermutations:
                if not analysis_scripts.is_cyclically_distinct(superpermutation, existing_sp):
                    is_distinct = False
                    break

            if is_distinct:
                print("  Found a *distinct* 5906 superpermutation!")
                distinct_count += 1
                existing_superpermutations.add(superpermutation)
                with open(distinct_superpermutations_file, "a") as f:  # Append
                    f.write(superpermutation + "\n")
                #Create and add laminates
                new_lam_1 = analysis_scripts.create_laminate(superpermutation, N, N-1)
                new_lam_2 = analysis_scripts.create_laminate(superpermutation, N, N-2)
                laminates.append(new_lam_1)
                laminates.append(new_lam_2)
            else:
                print("  Found a 5906 superpermutation, but it's a duplicate.")
        else:
            print("  Run did not produce a valid minimal superpermutation.")
            
        # Update "Winners" and "Losers" (using the new superpermutation)
        new_winners, new_losers = analysis_scripts.calculate_winners_losers([superpermutation], N, k=N-1)
        new_winners2, new_losers2 = analysis_scripts.calculate_winners_losers([superpermutation], N, k=N-2)

        for kmer, weight in new_winners.items():
            winners[kmer] = winners.get(kmer, 0) + weight
        for kmer, weight in new_losers.items():
            losers[kmer] = losers.get(kmer, 0) + weight
        for kmer, weight in new_winners2.items():
            winners[kmer] = winners.get(kmer, 0) + weight
        for kmer, weight in new_losers2.items():
            losers[kmer] = losers.get(kmer, 0) + weight

        # Higher-Order Winners/Losers
        new_seq_winners, new_seq_losers = analysis_scripts.calculate_sequence_winners_losers([superpermutation], N)
        for seq_hash, weight in new_seq_winners.items():
            if seq_hash in winners:
                winners[seq_hash] += weight
            else:
                winners[seq_hash] = weight
        for seq_hash, weight in new_seq_losers.items():
            if seq_hash in losers:
                losers[seq_hash] += weight
            else:
                losers[seq_hash] = weight

        # Add new losers to limbo list
        for kmer, weight in losers.items():
            limbo_list.add(kmer)  # Add to limbo list

        # Find and add new prodigal results (stricter criteria).
        new_prodigals = analysis_scripts.find_prodigal_results(superpermutation, N, min_length=PRODIGAL_MIN_LENGTH, overlap_threshold=PRODIGAL_OVERLAP_THRESHOLD)
        for prodigal_seq in new_prodigals:
            is_new = True
            for existing_prodigal_id, existing_prodigal in prodigal_results.items():
                if prodigal_seq in existing_prodigal.sequence:
                    is_new = False
                    break
            if is_new:
                prodigal_results[next_prodigal_id] = ProdigalResult(prodigal_seq, next_prodigal_id)
                next_prodigal_id += 1
                #Create and add laminate
                laminates.append(analysis_scripts.create_laminate(prodigal_seq, N, N-1))
                laminates.append(analysis_scripts.create_laminate(prodigal_seq, N, N-2))


        # Update ePUT (add all *used* permutations)
        s_tuple = tuple(int(x) for x in superpermutation)
        for i in range(len(s_tuple) - n + 1):
            perm = s_tuple[i:i+n]
            if is_valid_permutation(perm, n):
                perm_hash = hash_permutation(perm)
                if perm_hash not in eput:
                    eput[perm_hash] = PermutationData(perm, in_sample=False, creation_method="dynamic_generation") # Dynamic
                eput[perm_hash].used_count += 1
                eput[perm_hash].used_in_final = True  # Mark as used in this iteration's superpermutation
                # Update neighbors in ePUT (using consistent k values)
                if i > 0:
                    prev_perm = s_tuple[i-1:i-1+n]
                    if is_valid_permutation(prev_perm, n):
                         eput[perm_hash].neighbors.add(hash_permutation(prev_perm))
                if i < len(s_tuple) - n:
                    next_perm = s_tuple[i+1:i+1+n]
                    if is_valid_permutation(next_perm, n):
                        eput[perm_hash].neighbors.add(hash_permutation(next_perm))

        # Update Layout Memory
        layout_memory.add_sequence(superpermutation, N, N-1, f"run_{run_count}")  # Add the new superpermutation
        layout_memory.add_sequence(superpermutation, N, N-2, f"run_{run_count}")

        # Update "Meta-Hierarchy" (simplified for this example)
        meta_hierarchy.setdefault("run_lengths", []).append(len(superpermutation))
        meta_hierarchy.setdefault("prodigal_counts", []).append(len(prodigal_results))
        total_hypotheticals = len(hypothetical_prodigals)
        successful_hypotheticals = 0
        for h_id, h_prodigal in hypothetical_prodigals.items():
            if h_prodigal.sequence in superpermutation:
               successful_hypotheticals += 1
        success_rate = ( successful_hypotheticals / total_hypotheticals) if total_hypotheticals > 0 else 0
        meta_hierarchy.setdefault("hypothetical_success_rate",[]).append(success_rate)


        end_time = time.time()
        print(f"  Run {run_count} completed in {end_time - start_time:.2f} seconds.")
        print(f"  Distinct 5906 superpermutations found so far: {distinct_count}")


        #Stopping Criteria
        if len(superpermutation) == 5906:
            # Check for early stopping based on lack of new distinct solutions
            is_new_solution = True
            for existing_sp in existing_superpermutations:
                if not analysis_scripts.is_cyclically_distinct(superpermutation, existing_sp):
                    is_new_solution = False
                    break

            if not is_new_solution:
                meta_hierarchy["no_new_solutions_count"] = meta_hierarchy.get("no_new_solutions_count",0) + 1
                if meta_hierarchy["no_new_solutions_count"] >= 20:
                    print("No new distinct 5906 solutions found in 20 iterations. Stopping.")
                    break
            else:
                meta_hierarchy["no_new_solutions_count"] = 0  # Reset counter
        if run_count >= 100:
            break


    layout_memory.save_to_file(layout_memory_file)

if __name__ == "__main__":
    random.seed(RANDOM_SEED)
    main()
```

### src/unified/embedder/cand_40_utils_py_graph_utils_py.py

```python
# utils.py
import itertools
import hashlib
import logging

def setup_logging():
    """Sets up logging to a file named 'superpermutation.log'."""
    logging.basicConfig(level=logging.DEBUG, filename='superpermutation.log', filemode='w', format='%(asctime)s - %(levelname)s - %(message)s')

def compute_checksum(data: str) -> str:
    """Computes the SHA-256 checksum of a given string.  This is useful for verifying data integrity."""
    return hashlib.sha256(data.encode('utf-8')).hexdigest()

def generate_permutations(n: int):
    """Generates all possible permutations of the numbers from 1 to n (inclusive).  Returns a list of tuples, where each tuple is a permutation."""
    return list(itertools.permutations(range(1, n + 1)))

def is_valid_permutation(perm: tuple, n: int) -> bool:
    """Checks if a given tuple is a valid permutation of numbers from 1 to n.  A valid permutation contains each number from 1 to n exactly once."""
    return len(perm) == n and set(perm) == set(range(1, n + 1))

def calculate_overlap(s1: str, s2: str) -> int:
    """Calculates the length of the maximum suffix of s1 that is also a prefix of s2.  This is used to determine how much two sequences overlap."""
    max_len = min(len(s1), len(s2))
    for i in range(max_len, 0, -1):
        if s1[-i:] == s2[:i]:
            return i
    return 0

def normalize_sequence(seq: str) -> str:
    """Normalizes a sequence by rotating it so that it starts with the smallest character.  This ensures that equivalent sequences (rotations of each other) are treated as the same."""
    min_char = min(seq)
    min_indices = [i for i, c in enumerate(seq) if c == min_char]
    rotations = [seq[i:] + seq[:i] for i in min_indices]
    return min(rotations)

def kmer_to_int(kmer):
    """Converts a k-mer (represented as a tuple of digits) to its integer representation.  This is done for efficiency, as integer comparisons are faster than tuple comparisons."""
    int_kmer = 0
    for i, digit in enumerate(kmer):
        int_kmer = int_kmer * 10 + digit
    return int_kmer

def int_to_kmer(int_kmer, k):
    """Converts an integer representation of a k-mer back to a k-mer tuple."""
    kmer = ()
    for _ in range(k):
        digit = int_kmer % 10
        kmer = (digit,) + kmer
        int_kmer //= 10
    return kmer

def hash_permutation(perm):
    """Hashes a permutation.  Handles both integer and tuple representations."""
    if isinstance(perm, tuple):
        return hash(perm)
    elif isinstance(perm, int):
        return perm
    else:
        raise TypeError("Permutation must be int or tuple")

def unhash_permutation(perm_hash, n):
    """Unhashes a permutation.  Handles integer to tuple conversion."""
    if isinstance(perm_hash, int):
        return int_to_kmer(perm_hash, n)
    else:
        raise TypeError("Permutation hash must be int")


# graph_utils.py
import networkx as nx
import logging

def build_de_bruijn_graph(kmers):
    """Builds a De Bruijn graph from a list of k-mers.  The De Bruijn graph is a directed graph where nodes represent (k-1)-mers and edges represent k-mers."""
    graph = nx.DiGraph()
    for kmer in kmers:
        graph.add_edge(kmer[:-1], kmer[1:])  # Add an edge for each k-mer
    logging.debug(f"De Bruijn graph: {graph.number_of_nodes()} nodes, {graph.number_of_edges()} edges.")
    return graph

def find_eulerian_path(graph: nx.DiGraph):
    """Finds an Eulerian path in a given graph.  An Eulerian path is a path that visits every edge exactly once."""
    try:
        path = list(nx.eulerian_path(graph))
        return [u for u, v in path] + [path[-1][1]]  # Extract the nodes from the path
    except nx.NetworkXError:
        logging.error("No Eulerian path found.")
        return []
        
        #This chunk contains the core utility functions (utils.py) and the graph-related functions (graph_utils.py).  The utility functions handle common tasks like logging, checksum calculation, permutation generation, overlap calculation, and k-mer conversion. The graph_utils.py file provides functions for building a De Bruijn graph and finding an Eulerian path within it.  These are fundamental tools used in superpermutation generation.
```

### src/unified/embedder/cand_4_analysis_scripts_final_py.py

```python
import itertools
import math
import networkx as nx
from collections import deque, defaultdict
import heapq
import random  #Needed for Hypothetical Prodigal Generation


def calculate_overlap(s1: str, s2: str) -> int:
    """Calculates the maximum overlap between two strings.

    Args:
        s1 (str): The first string.
        s2 (str): The second string.

    Returns:
        int: The length of the maximum overlap. Returns 0 if there is no overlap.
    """
    max_overlap = 0
    for i in range(1, min(len(s1), len(s2)) + 1):
        if s1[-i:] == s2[:i]:
            max_overlap = i
    return max_overlap


def hash_permutation(permutation: tuple) -> int:
    """Hashes a permutation tuple to a unique integer.

    Args:
        permutation: The permutation tuple (e.g., (1, 2, 3, 4, 5, 6, 7, 8)).

    Returns:
        A unique integer hash value.
    """
    result = 0
    n = len(permutation)
    for i, val in enumerate(permutation):
        result += val * (n ** (n - 1 - i))
    return result


def unhash_permutation(hash_value: int, n: int) -> tuple:
    """Converts a hash value back to a permutation tuple.

    Args:
        hash_value: The integer hash value.
        n: The value of n.

    Returns:
        The corresponding permutation tuple.
    """
    permutation = []
    for i in range(n - 1, -1, -1):
        val = hash_value // (n ** i)
        permutation.append(val + 1)  # Adjust to be 1-indexed
        hash_value -= val * (n ** i)
    return tuple(permutation)


def is_valid_permutation(perm: tuple, n: int) -> bool:
    """Checks if a given sequence is a valid permutation.

    Args:
        perm: The sequence (tuple of integers).
        n: The value of n.

    Returns:
        True if the sequence is a valid permutation, False otherwise.
    """
    return len(set(perm)) == n and min(perm) == 1 and max(perm) == n


def get_kmers(sequence: str, n: int, k: int) -> set[str]:
    """Extracts all k-mers from a sequence of digits, ensuring they form valid permutations.

    Args:
        sequence: The input sequence (string of digits).
        n: The value of n.
        k: The length of the k-mers to extract.

    Returns:
        A set of k-mer strings.
    """
    kmers = set()
    seq_list = [int(x) for x in sequence]  # Ensure sequence is treated as digits
    for i in range(len(sequence) - n + 1):
        perm = tuple(seq_list[i:i + n])
        if is_valid_permutation(perm, n):
            if i >= k:
                kmer = "".join(str(x) for x in seq_list[i - k:i])
                kmers.add(kmer)
    return kmers


def calculate_golden_ratio_points(length: int, levels: int = 1) -> list[int]:
    """Calculates multiple levels of golden ratio points within a sequence.

    Args:
        length (int): The total length of the sequence.
        levels (int): The number of recursive divisions to perform.

    Returns:
        list: A sorted list of unique golden ratio points (integers).
    """
    phi = (1 + math.sqrt(5)) / 2
    points = []
    for _ in range(levels):
        new_points = []
        if not points:
            new_points = [int(length / phi), int(length - length / phi)]
        else:
            for p in points:
                new_points.extend([int(p / phi), int(p - p / phi)])
                new_points.extend([int((length - p) / phi) + p, int(length - (length - p) / phi)])
        points.extend(new_points)
        points = sorted(list(set(points)))  # Remove duplicates and sort
    return points


def calculate_fixed_segments(length: int, segment_size: int, overlap_size: int) -> list[tuple[int, int]]:
    """Calculates start and end indices for fixed-size segments with overlap.

    Args:
        length (int): The total length of the sequence.
        segment_size (int): The desired size of each segment.
        overlap_size (int): The desired overlap between adjacent segments.

    Returns:
        list: A list of tuples, where each tuple contains the (start, end) indices of a segment.
    """
    segments = []
    start = 0
    while start < length:
        end = min(start + segment_size, length)
        segments.append((start, end))
        start += segment_size - overlap_size
    return segments

def generate_permutations(n: int) -> list[tuple[int, ...]]:
    """Generates all permutations of 1 to n.  Used for creating complete sets.

    Args:
        n (int): The number of symbols.

    Returns:
        list[tuple[int, ...]]: A list of all permutations as tuples.
    """
    return list(itertools.permutations(range(1, n + 1)))

# --- Superpermutation Analysis ---

def analyze_superpermutation(superpermutation: str, n: int) -> dict:
    """Analyzes a given superpermutation and provides statistics.

    Args:
        superpermutation (str): The superpermutation string.
        n (int): The value of n (number of symbols).

    Returns:
        dict: A dictionary containing:
            - length: The length of the superpermutation.
            - validity: True if valid, False otherwise.
            - missing_permutations: A list of missing permutations (empty if valid).
            - overlap_distribution: A dictionary showing counts of each overlap length.
            - average_overlap: The average overlap between consecutive permutations.
    """
    permutations = set(itertools.permutations(range(1, n + 1)))
    s_tuple = tuple(int(x) for x in superpermutation)
    found_permutations = set()
    missing_permutations = []
    overlap_distribution = {}

    total_overlap = 0

    for i in range(len(s_tuple) - n + 1):
        perm = s_tuple[i:i+n]
        if is_valid_permutation(perm, n):  # Valid
            found_permutations.add(tuple(perm))  # Use tuples for set
            if i > 0:
                overlap = calculate_overlap("".join(str(x) for x in s_tuple[i-n:i]), "".join(str(x) for x in perm))
                total_overlap += overlap
                overlap_distribution[overlap] = overlap_distribution.get(overlap, 0) + 1

    missing_permutations = list(permutations - found_permutations)
    validity = len(missing_permutations) == 0
    average_overlap = total_overlap / (len(found_permutations) - 1) if len(found_permutations) > 1 else 0

    return {
        "length": len(superpermutation),
        "validity": validity,
        "missing_permutations": missing_permutations,
        "overlap_distribution": overlap_distribution,
        "average_overlap": average_overlap,
    }

# --- Prodigal Result Identification ---

def is_prodigal(sequence: str, permutations_in_sequence: list[tuple[int]], n: int, min_length: int, overlap_threshold: float) -> bool:
    """Checks if a sequence is a 'Prodigal Result'.

    Args:
        sequence (str): The sequence of digits to check.
        permutations_in_sequence (list):  A list of *tuples* representing the
                                         permutations contained within the sequence.
        n (int): The value of n.
        min_length (int): The minimum number of permutations for a "Prodigal Result."
        overlap_threshold (float): The minimum overlap rate (0 to 1).

    Returns:
        bool: True if the sequence is a "Prodigal Result," False otherwise.
    """
    if len(permutations_in_sequence) < min_length:
        return False

    total_length = sum(len(str(p)) for p in permutations_in_sequence)
    overlap_length = total_length - len(sequence)
    max_possible_overlap = (len(permutations_in_sequence) - 1) * (n - 1)

    if max_possible_overlap == 0:
        return False
    return (overlap_length / max_possible_overlap) >= overlap_threshold

def find_prodigal_results(superpermutation: str, n: int, min_length: int = None, overlap_threshold: float = 0.98) -> list[str]:
    """Identifies and returns a list of prodigal results within a given superpermutation.

    Args:
        superpermutation (str): The superpermutation string.
        n (int): The value of n.
        min_length (int): Minimum number of permutations in a prodigal result.
                          Defaults to n-1.  This can be dynamically adjusted.
        overlap_threshold (float): Minimum overlap percentage. Defaults to 0.98,
                                   but can be dynamically adjusted.

    Returns:
        list: A list of "Prodigal Result" sequences (strings).
    """
    prodigal_results = []
    superperm_list = [int(x) for x in superpermutation]  # Convert to list of integers
    if min_length is None:
        min_length = n-1

    for length in range(n * min_length, len(superpermutation) + 1):  # Iterate through possible lengths
        for i in range(len(superpermutation) - length + 1):  # Iterate through starting positions
            subsequence = tuple(superperm_list[i:i+length])  # Get the subsequence as a tuple
            permutations_in_subsequence = set()
            for j in range(len(subsequence) - n + 1):
                perm = subsequence[j:j+n]
                if is_valid_permutation(perm, n):  # Valid permutation
                    permutations_in_subsequence.add(tuple(perm))  # Use tuples for set membership

            if is_prodigal(subsequence, list(permutations_in_subsequence), n, min_length, overlap_threshold):
                prodigal_results.append("".join(str(x) for x in subsequence))  # Store as string
    return prodigal_results

    # --- Winner/Loser Calculation ---

def calculate_winners_losers(superpermutations: list[str], n: int, k: int) -> tuple[dict, dict]:
    """Calculates "Winner" and "Loser" k-mer weights from a list of superpermutations.

    Args:
        superpermutations (list): A list of superpermutation strings.
        n (int): The value of n.
        k (int): The length of the k-mers to analyze.

    Returns:
        tuple: (winners, losers), where:
            - winners: A dictionary of {kmer: weight} for winning k-mers.
            - losers: A dictionary of {kmer: weight} for losing k-mers.
    """

    all_kmers = {}  # {kmer: total_count}
    for superpermutation in superpermutations:
        s_tuple = tuple(int(x) for x in superpermutation)
        for i in range(len(s_tuple) - n + 1):
            perm = s_tuple[i:i + n]
            if is_valid_permutation(perm, n):  # Valid
                if i > 0:
                    kmer = "".join(str(x) for x in s_tuple[i - k:i])
                    all_kmers[kmer] = all_kmers.get(kmer, 0) + 1

    # Divide into "shorter" and "longer" groups based on median length
    lengths = [len(s) for s in superpermutations]
    lengths.sort()
    median_length = lengths[len(lengths) // 2]
    shorter_superpermutations = [s for s in superpermutations if len(s) <= median_length]
    longer_superpermutations = [s for s in superpermutations if len(s) > median_length]

    winners = {}  # {kmer: weight}
    losers = {}  # {kmer: weight}

    shorter_counts = {}  # {kmer: count_in_shorter}
    for superpermutation in shorter_superpermutations:
        s_tuple = tuple(int(x) for x in superpermutation)
        for i in range(len(s_tuple) - n + 1):
            perm = s_tuple[i:i + n]
            if is_valid_permutation(perm, n):
                if i > 0:
                    kmer = "".join(str(x) for x in s_tuple[i - k:i])
                    shorter_counts[kmer] = shorter_counts.get(kmer, 0) + 1

    longer_counts = {}  # {kmer: count_in_longer}
    for superpermutation in longer_superpermutations:
        s_tuple = tuple(int(x) for x in superpermutation)
        for i in range(len(s_tuple) - n + 1):
            perm = s_tuple[i:i + n]
            if is_valid_permutation(perm, n):
                if i > 0:
                    kmer = "".join(str(x) for x in s_tuple[i - k:i])
                    longer_counts[kmer] = longer_counts.get(kmer, 0) + 1

    # Calculate weights based on the difference in counts
    for kmer in all_kmers:
        score = shorter_counts.get(kmer, 0) - longer_counts.get(kmer, 0)
        if score > 0:
            winners[kmer] = score
        elif score < 0:
            losers[kmer] = -score  # Store as positive weights

    return winners, losers


def calculate_sequence_winners_losers(superpermutations: list[str], n: int, sequence_length: int = 2) -> tuple[dict, dict]:
    """Calculates 'Winner' and 'Loser' weights for sequences of permutations.

    Args:
        superpermutations (list[str]): List of superpermutation strings.
        n (int): The value of n.
        sequence_length (int): The length of the permutation sequences to analyze (default: 2).

    Returns:
        tuple: (winners, losers), where winners and losers are dictionaries
               mapping sequence hashes (integers) to weights.
    """
    all_sequences = {} # {seq_hash : count}

    for superperm in superpermutations:
        s_tuple = tuple(int(x) for x in superperm)
        perms = []
        for i in range(len(s_tuple) - n + 1):
            perm = s_tuple[i:i+n]
            if is_valid_permutation(perm, n):
                perms.append(hash_permutation(perm))  # Append hash

        for i in range(len(perms) - (sequence_length -1)):
            seq = tuple(perms[i:i+sequence_length])
            seq_hash = hash(seq) #Hash the sequence of hashes.
            all_sequences[seq_hash] = all_sequences.get(seq_hash, 0) + 1

    #Divide into "shorter" and "longer"
    lengths = [len(s) for s in superpermutations]
    lengths.sort()
    median_length = lengths[len(lengths)//2]
    shorter_superpermutations = [s for s in superpermutations if len(s) <= median_length]
    longer_superpermutations = [s for s in superpermutations if len(s) > median_length]

    winners = {}
    losers = {}
    shorter_counts = {}
    for superperm in shorter_superpermutations:
        s_tuple = tuple(int(x) for x in superperm)
        perms = []
        for i in range(len(s_tuple) - n + 1):
            perm = s_tuple[i:i+n]
            if is_valid_permutation(perm, n):
                perms.append(hash_permutation(perm)) #Append hash
        for i in range(len(perms) - (sequence_length - 1)):
            seq = tuple(perms[i:i+sequence_length])
            seq_hash = hash(seq)
            shorter_counts[seq_hash] = shorter_counts.get(seq_hash, 0) + 1

    longer_counts = {}
    for superperm in longer_superpermutations:
        s_tuple = tuple(int(x) for x in superperm)
        perms = []
        for i in range(len(s_tuple) - n + 1):
            perm = s_tuple[i:i+n]
            if is_valid_permutation(perm, n):
                perms.append(hash_permutation(perm))
        for i in range(len(perms) - (sequence_length - 1)):
            seq = tuple(perms[i:i+sequence_length])
            seq_hash = hash(seq)
            longer_counts[seq_hash] = longer_counts.get(seq_hash, 0) + 1

    for seq_hash in all_sequences:
        score = shorter_counts.get(seq_hash, 0) - longer_counts.get(seq_hash, 0)
        if score > 0:
            winners[seq_hash] = score
        if score < 0:
            losers[seq_hash] = -score

    return winners, losers

# --- Anti-Prodigal Identification ---
def identify_anti_prodigals(superpermutations, n, k, overlap_threshold):
    """Identifies and returns a list of 'anti-prodigal' k-mers.

    Args:
        superpermutations (list): A list of superpermutation strings.
        n (int): The value of n.
        k (int): The length of k-mers.
        overlap_threshold (float): The *maximum* overlap allowed for an anti-prodigal

    Returns:
        set: A set of 'anti-prodigal' k-mers (strings).
    """
    anti_prodigals = set()
    for superpermutation in superpermutations:
        s_tuple = tuple(int(x) for x in superpermutation)
        for i in range(len(s_tuple) - n + 1):
            perm = s_tuple[i:i + n]
            if is_valid_permutation(perm, n):  # Valid
                if i > k:
                    kmer = "".join(str(x) for x in s_tuple[i-k:i])
                    overlap = calculate_overlap("".join(str(x) for x in s_tuple[i-k:i]), "".join(str(x) for x in perm))
                    if overlap < ( (n-1) * overlap_threshold):  #  Condition for Anti-Prodigal
                        anti_prodigals.add(kmer)
    return anti_prodigals
    
    # --- De Bruijn Graph Functions ---

def build_debruijn_graph(n: int, k: int, permutations=None, superpermutation=None) -> nx.DiGraph:
    """Builds a De Bruijn graph of order k for permutations of n symbols.

    Args:
        n: The number of symbols (e.g., 8 for n=8).
        k: The order of the De Bruijn graph (k-mers).
        permutations: (Optional) A list of permutation tuples.  If provided,
                      the graph is built ONLY from these permutations.
        superpermutation: (Optional) A superpermutation string. If provided,
                          the graph is built from the k-mers in this string.
                          Must provide one, and only one, of the two.

    Returns:
        A networkx.DiGraph representing the De Bruijn graph.
    """

    if (permutations is None and superpermutation is None) or (permutations is not None and superpermutation is not None):
        raise ValueError("Must provide either 'permutations' or 'superpermutation', but not both.")

    graph = nx.DiGraph()

    if permutations:
      for perm in permutations:
          perm_str = "".join(str(x) for x in perm)
          for i in range(len(perm_str) - k + 1):
              kmer1 = perm_str[i:i + k - 1]
              kmer2 = perm_str[i + 1:i + k]
              if len(kmer1) == k - 1 and len(kmer2) == k-1: #Should always be true
                graph.add_edge(kmer1, kmer2, weight=calculate_overlap(kmer1, kmer2))
    else: #Use superpermutation
        s_list = [int(x) for x in superpermutation]
        for i in range(len(s_list) - n + 1):
            perm = tuple(s_list[i:i+n])
            if is_valid_permutation(perm, n):
                #Valid permutation.
                if i >= k:
                  kmer1 = "".join(str(x) for x in s_list[i-k:i])
                  kmer2 = "".join(str(x) for x in s_list[i-k+1: i+1])
                  if len(kmer1) == k - 1 and len(kmer2) == k - 1:
                    graph.add_edge(kmer1, kmer2, weight = calculate_overlap(kmer1,kmer2))
    return graph

def add_weights_to_debruijn(graph: nx.DiGraph, winners: dict, losers: dict):
    """Adds 'winner_weight' and 'loser_weight' attributes to the edges of a De Bruijn graph.

    Args:
        graph: The De Bruijn graph (networkx.DiGraph).
        winners: A dictionary of Winner k-mers and their weights.
        losers: A dictionary of Loser k-mers and their weights.
    """

    for u, v, data in graph.edges(data=True):
        kmer = u[1:] + v[-1]  # Reconstruct the k-mer from the edge
        data['winner_weight'] = winners.get(kmer, 0)
        data['loser_weight'] = losers.get(kmer, 0)

def find_cycles(graph: nx.DiGraph) -> list[list[str]]:
    """Finds cycles in the De Bruijn graph using a simple DFS approach."""
    cycles = []
    visited = set()

    def dfs(node, path):
        visited.add(node)
        path.append(node)

        for neighbor in graph.neighbors(node):
            if neighbor == path[0] and len(path) > 1:  # Found a cycle
                cycles.append(path.copy())
            elif neighbor not in visited:
                dfs(neighbor, path)

        path.pop()
        visited.remove(node) #Correctly remove node

    for node in graph.nodes:
        if node not in visited:
            dfs(node, [])
    return cycles

def find_high_weight_paths(graph: nx.DiGraph, start_node: str, length_limit: int) -> list[list[str]]:
    """
    Finds high-weight paths in the De Bruijn graph starting from a given node.
    """
    best_paths = []
    best_score = -float('inf')

    def dfs(current_node, current_path, current_score):
        nonlocal best_score
        nonlocal best_paths

        if len(current_path) > length_limit:
            return

        current_score_adj = current_score
        if len(current_path) > 1:
            current_score_adj = current_score / (len(current_path)-1) #Average

        if current_score_adj > best_score:
            best_score = current_score_adj
            best_paths = [current_path.copy()]  # New best path
        elif current_score_adj == best_score and len(current_path) > 1: #Dont include starting nodes
            best_paths.append(current_path.copy())  # Add to list of best paths

        for neighbor in graph.neighbors(current_node):
            edge_data = graph.get_edge_data(current_node, neighbor)
            new_score = current_score + edge_data.get('weight', 0) + edge_data.get('winner_weight', 0) - edge_data.get('loser_weight', 0)
            dfs(neighbor, current_path + [neighbor], new_score)

    dfs(start_node, [start_node], 0)
    return best_paths

# --- Hypothetical Prodigal Generation ---
def generate_permutations_on_demand_hypothetical(current_sequence: str, kmer: str, n: int, k: int) -> set[int]:
    """Generates candidate permutations for hypothetical prodigals. More restrictive.

    Args:
        current_sequence (str): The sequence currently being built
        kmer (str): The k-mer to extend (either prefix or suffix).
        n (int): value of n
        k (int): The k value

    Returns:
        set[int]: A set of permutation *hashes*.
    """
    valid_permutations = set()
    all_perms = generate_permutations(n)  # Get *all* permutations
    for perm in all_perms:
        perm_str = "".join(str(x) for x in perm)
        if kmer == perm_str[:k] or kmer == perm_str[-k:]:
           valid_permutations.add(hash_permutation(perm))

    # We do NOT filter here, as these are for hypotheticals.
    return valid_permutations

def generate_mega_hypotheticals(prodigal_results: dict, winners: dict, losers: dict, n: int, num_to_generate: int = 20, min_length: int = 50, max_length: int = 200) -> dict:
    """Generates 'Mega-Hypothetical' Prodigals by combining existing Prodigals.

    Args:
        prodigal_results (dict): Dictionary of existing ProdigalResult objects.
        winners (dict): "Winner" k-mer data.  Used for scoring connections.
        losers (dict): "Loser" k-mer data. Used for filtering.
        n (int): The value of n.
        num_to_generate (int): The number of Mega-Hypotheticals to generate.
        min_length (int): Minimum length (in permutations).
        max_length (int): Maximum length (in permutations).

    Returns:
        dict: A dictionary of new 'Mega-Hypothetical' ProdigalResult objects.
    """
    mega_hypotheticals = {}
    next_mega_id = -1  # Use negative IDs to distinguish

    for _ in range(num_to_generate):
        num_prodigals_to_combine = random.randint(2, 4)  # Combine 2-4 Prodigals
        #Select prodigals, but with a bias towards those with higher overlap and greater length
        prodigal_weights = [p.overlap_rate * p.length for p in prodigal_results.values()]
        selected_prodigal_ids = random.choices(list(prodigal_results.keys()), weights=prodigal_weights, k=num_prodigals_to_combine)
        selected_prodigals = [prodigal_results[pid] for pid in selected_prodigal_ids]

        # Sort by length, in any order.
        selected_prodigals.sort(key=lambda p: len(p.sequence))

        #Use golden ratio for lengths.
        phi = (1 + math.sqrt(5)) / 2
        total_target_length = random.randint(n * min_length, n* max_length)
        target_lengths = []

        remaining_length = total_target_length
        for i in range(len(selected_prodigals)-1):
            segment_length = int(remaining_length / (phi**(i+1)))
            target_lengths.append(segment_length)
            remaining_length -= segment_length
        target_lengths.append(remaining_length) # Add the final length.

        combined_sequence = ""
        
        #Randomize starting location.
        start_location = random.randint(0,len(selected_prodigals)-1)
        prodigal_order = []
        for i in range(len(selected_prodigals)):
            prodigal_order.append(selected_prodigals[(start_location + i) % len(selected_prodigals)])

        for i in range(len(prodigal_order)):
            prodigal = prodigal_order[i]
            target_length = target_lengths[i]

            # Get a random section of the prodigal, unless it is too short.
            start_index = random.randint(0, max(0, len(prodigal.sequence) - target_length))  # Ensure valid start
            current_sequence = prodigal.sequence[start_index : start_index + target_length]

            # Connect to previous
            if combined_sequence != "":
                overlap = calculate_overlap(combined_sequence, current_sequence)
                if overlap == 0: #Need to use the combiner
                    prefix = combined_sequence[-(n-1):]
                    suffix = current_sequence[:n-1]
                    candidates = generate_permutations_on_demand_hypothetical(prefix, suffix,  n, n-1)
                    if candidates:
                        # Choose the best candidate based on winners/losers (simplified scoring)
                        best_candidate = None
                        best_score = -float('inf')
                        for cand_hash in candidates:
                            cand_perm = unhash_permutation(cand_hash, n)
                            cand_str = "".join(str(x) for x in cand_perm)
                            score = 0
                            for k in [7, 6]: # Using values for n=8
                                for j in range(len(cand_str) - k + 1):
                                    kmer = cand_str[j:j+k]
                                    score += winners.get(kmer, 0)
                                    score -= losers.get(kmer, 0)

                            if score > best_score:
                                best_score = score
                                best_candidate = cand_str

                        overlap = calculate_overlap(combined_sequence, best_candidate)
                        combined_sequence += best_candidate[overlap:]
                    else:
                        continue #Skip if we cannot connect.

                else: #Overlap exists
                    combined_sequence += current_sequence[overlap:]
            else:
                combined_sequence = current_sequence
        
        #Check if prodigal
        perms_in_sequence = set()
        for i in range(len(combined_sequence) - n + 1):
            perm = tuple(int(x) for x in combined_sequence[i:i+n])
            if is_valid_permutation(perm, n):
                perms_in_sequence.add(hash_permutation(perm))

        if is_prodigal(combined_sequence, [unhash_permutation(x,n) for x in perms_in_sequence], n, min_length=min_length, overlap_threshold=0.95) and len(perms_in_sequence) > 0:
            mega_hypotheticals[next_mega_id] = ProdigalResult(combined_sequence, next_mega_id)
            next_mega_id -= 1

    return mega_hypotheticals
```

### src/unified/embedder/cand_5_generation_code_n7_dynamic_final_py.py

```python
import itertools
import random
import math
import time
import networkx as nx
from collections import deque, defaultdict
import heapq
import analysis_scripts
from layout_memory import LayoutMemory

# --- Constants ---
N = 7  # The value of n (number of symbols).
PRODIGAL_OVERLAP_THRESHOLD = 0.98  # Initial minimum overlap rate for "Prodigal Results"
PRODIGAL_MIN_LENGTH = 10 #Reduced
HYPOTHETICAL_PRODIGAL_OVERLAP_THRESHOLD = 0.90 #Reduced
HYPOTHETICAL_PRODIGAL_MIN_LENGTH = 7
HYPOTHETICAL_PRODIGAL_GENERATION_COUNT = 50 # Number of Hypothetical Prodigals to Generate
WINNER_THRESHOLD = 0.75  # Not directly used in scoring
LOSER_THRESHOLD = 0.25  # Not directly used in scoring
NUM_ITERATIONS = 10000  #  Set a large number; it will likely stop earlier
LAYOUT_K_VALUES = [N - 1, N - 2]  # k values for Layout Memory
DE_BRUIJN_K_VALUES = [N - 1, N - 2]  # k values for De Bruijn graph generation
RANDOM_SEED = 42  # For reproducibility.  CHANGE THIS FOR EACH RUN.

# --- Helper Functions --- (These are identical to the n=8 versions)
def calculate_overlap(s1: str, s2: str) -> int:
    """Calculates the maximum overlap between two strings."""
    max_overlap = 0
    for i in range(1, min(len(s1), len(s2)) + 1):
        if s1[-i:] == s2[:i]:
            max_overlap = i
    return max_overlap

def generate_permutations(n: int) -> list[tuple[int, ...]]:
    """Generates all permutations of 1 to n."""
    return list(itertools.permutations(range(1, n + 1)))

def calculate_distance(p1: str, p2: str, n: int) -> int:
    """Calculates distance (n-1 - overlap)."""
    return (n - 1) - max(calculate_overlap(p1, p2), calculate_overlap(p2, p1))

def hash_permutation(permutation: tuple) -> int:
    """Hashes a permutation tuple to a unique integer."""
    result = 0
    n = len(permutation)
    for i, val in enumerate(permutation):
        result += val * (n ** (n - 1 - i))
    return result

def unhash_permutation(hash_value: int, n: int) -> tuple:
    """Converts a hash value back to a permutation tuple."""
    permutation = []
    for i in range(n - 1, -1, -1):
        val = hash_value // (n ** i)
        permutation.append(val + 1)  # Adjust to be 1-indexed
        hash_value -= val * (n ** i)
    return tuple(permutation)

def is_valid_permutation(perm: tuple, n: int) -> bool:
    """Checks if a given sequence is a valid permutation."""
    return len(set(perm)) == n and min(perm) == 1 and max(perm) == n

def calculate_golden_ratio_points(length: int, levels: int = 1) -> list[int]:
    """Calculates multiple levels of golden ratio points."""
    phi = (1 + math.sqrt(5)) / 2
    points = []
    for _ in range(levels):
        new_points = []
        if not points:
            new_points = [int(length / phi), int(length - length / phi)]
        else:
            for p in points:
                new_points.extend([int(p / phi), int(p - p / phi)])
                new_points.extend([int((length-p) / phi) + p, int(length - (length-p) / phi)])
        points.extend(new_points)
        points = sorted(list(set(points)))  # Remove duplicates and sort
    return points

def get_kmers(sequence: str, n: int, k: int) -> set[str]:
    """Extracts all k-mers from a sequence, given n and k."""
    kmers = set()
    seq_list = [int(x) for x in sequence]
    for i in range(len(sequence) - n + 1):
        perm = tuple(seq_list[i:i+n])
        if is_valid_permutation(perm, n):
            if i >= k:
                kmer = "".join(str(x) for x in seq_list[i-k:i])
                kmers.add(kmer)
    return kmers
# --- Data Structures --- (Same as n=8 version)

class PermutationData:
    def __init__(self, permutation: tuple, in_sample: bool = False, creation_method: str = ""):
        """Stores data associated with a single permutation."""
        self.hash: int = hash_permutation(permutation)
        self.permutation: tuple = permutation
        self.in_sample: bool = in_sample  # Will always be False
        self.used_count: int = 0
        self.prodigal_status: list[int] = []
        self.creation_method: str = creation_method
        self.batch_ids: list[int] = [] #Not used
        self.used_in_final: bool = False
        self.neighbors: set[int] = set()

    def __str__(self) -> str:
        return (f"PermutationData(hash={self.hash}, permutation={self.permutation}, used_count={self.used_count}, "
                f"prodigal_status={self.prodigal_status}, creation_method={self.creation_method})")

    def __repr__(self) -> str:
        return self.__str__()

class ProdigalResult:
    def __init__(self, sequence: str, result_id: int):
        """Represents a 'Prodigal Result'."""
        self.id: int = result_id
        self.sequence: str = sequence
        self.length: int = len(sequence)
        self.permutations: set[int] = set()  # Store hashes
        self.calculate_permutations()
        self.overlap_rate: float = self.calculate_overlap_rate()

    def calculate_permutations(self):
        """Calculates and stores the set of permutations."""
        n = N
        for i in range(len(self.sequence) - n + 1):
            perm = tuple(int(x) for x in self.sequence[i:i + n])
            if is_valid_permutation(perm, n):
                self.permutations.add(hash_permutation(perm))

    def calculate_overlap_rate(self) -> float:
        """Calculates the overlap rate."""
        n = N
        total_length = sum(len(str(p)) for p in [unhash_permutation(x,n) for x in self.permutations])
        overlap_length = total_length - len(self.sequence)
        max_possible_overlap = (len(self.permutations) - 1) * (n - 1)
        if max_possible_overlap == 0:
            return 0
        return overlap_length / max_possible_overlap

    def __str__(self) -> str:
        return (f"ProdigalResult(id={self.id}, length={self.length}, "
                f"overlap_rate={self.overlap_rate:.4f}, num_permutations={len(self.permutations)})")

    def __repr__(self) -> str:
        return self.__str__()
# --- Initialization Function ---
def initialize_data(initial_n7_prodigals: list[str], initial_winners: dict, initial_losers: dict) -> tuple:
    """Initializes the data structures for the algorithm.
        Modified for n=7, no initial superpermutation, loads initial prodigals.
    Args:
        initial_n7_prodigals (list[str]): A list of initial n=7 Prodigal Result strings.
        initial_winners (dict):  Initial Winner k-mers and weights.
        initial_losers (dict): Initial Loser k-mers and weights.

    Returns:
        tuple: (prodigal_results, winners, losers, meta_hierarchy, limbo_list, eput, next_prodigal_id)
    """
    prodigal_results = {}  # {prodigal_id: ProdigalResult object}
    winners = initial_winners  # {kmer: weight}
    losers = initial_losers  # {kmer: weight}
    meta_hierarchy = {}  # Track strategy effectiveness
    limbo_list = set()  # Set of permutation hashes.
    eput = {}  # The Enhanced Permutation Universe Tracker

    # Add initial n=7 prodigals
    next_prodigal_id = 0
    for prodigal in initial_n7_prodigals:
        prodigal_results[next_prodigal_id] = ProdigalResult(prodigal, next_prodigal_id)
        next_prodigal_id += 1

    return prodigal_results, winners, losers, meta_hierarchy, limbo_list, eput, next_prodigal_id

# --- On-Demand Permutation Generation ---
def generate_permutations_on_demand(prefix: str, suffix: str, prodigal_results: dict, winners: dict, losers: dict,
                                   n: int, eput: dict, limbo_list: set, min_overlap: int,
                                   hypothetical_prodigals: dict = None, laminate_graphs: list = None) -> set[int]:
    """Generates permutations on demand, extending a given prefix or suffix.
       Prioritizes "Prodigal" extensions, uses "Winners" and "Losers," and
       avoids already-used permutations and the "Limbo List."  Also uses laminates.

    Args:
        prefix (str): The prefix of the current superpermutation.
        suffix (str): The suffix of the current superpermutation.
        prodigal_results (dict): Dictionary of ProdigalResult objects.
        winners (dict): Dictionary of Winner k-mers and their weights.
        losers (dict): Dictionary of Loser k-mers and their weights.
        n (int): The value of n.
        eput (dict): The Enhanced Permutation Universe Tracker.
        limbo_list (set): The set of permutation hashes to avoid.
        min_overlap (int):  The minimum required overlap.
        hypothetical_prodigals (dict): Optional dictionary of Hypothetical Prodigals
        laminate_graphs (list): List of laminate graphs.

    Returns:
        set[int]: A set of *permutation hashes* that are potential extensions.
    """
    valid_permutations = set()

    # Prioritize Hypothetical Prodigal extensions
    if hypothetical_prodigals:
        for prodigal_id, prodigal in hypothetical_prodigals.items():
            if prefix and prodigal.sequence.startswith(prefix[-min_overlap:]):
                for i in range(len(prodigal.sequence) - (n - 1)):
                    perm = tuple(int(x) for x in prodigal.sequence[i:i + n])
                    if analysis_scripts.is_valid_permutation(perm, n) and hash_permutation(perm) not in eput:
                         valid_permutations.add(hash_permutation(perm))
            if suffix and prodigal.sequence.endswith(suffix[:min_overlap]):
                for i in range(len(prodigal.sequence) - (n - 1)):
                    perm = tuple(int(x) for x in prodigal.sequence[i:i + n])
                    if analysis_scripts.is_valid_permutation(perm, n) and hash_permutation(perm) not in eput:
                        valid_permutations.add(hash_permutation(perm))

    # Prioritize Prodigal extensions
    for prodigal_id, prodigal in prodigal_results.items():
        if prefix and prodigal.sequence.startswith(prefix[-min_overlap:]):
            for i in range(len(prodigal.sequence) - (n - 1)):
                perm = tuple(int(x) for x in prodigal.sequence[i:i + n])
                if analysis_scripts.is_valid_permutation(perm, n) and hash_permutation(perm) not in eput:
                    valid_permutations.add(hash_permutation(perm))
        if suffix and prodigal.sequence.endswith(suffix[:min_overlap]):
            for i in range(len(prodigal.sequence) - (n - 1)):
                perm = tuple(int(x) for x in prodigal.sequence[i:i + n])
                if analysis_scripts.is_valid_permutation(perm, n) and hash_permutation(perm) not in eput:
                    valid_permutations.add(hash_permutation(perm))


    # Filter based on Limbo List and Laminate
    filtered_permutations = set()
    for perm_hash in valid_permutations:
        perm = unhash_permutation(perm_hash, n)
        perm_str = "".join(str(x) for x in perm)
        is_in_limbo = False

        # Basic Loser check (using 5-mers and 6-mers for n=7)
        for k in [6, 5]:  # Check 6-mers and 5-mers
            for i in range(len(perm_str) - k + 1):
                kmer = perm_str[i:i+k]
                if kmer in limbo_list:
                    is_in_limbo = True
                    break
            if is_in_limbo:
                break
        
        if not is_in_limbo:
            valid_perm = False
            if laminate_graphs:
                for laminate in laminate_graphs:
                    if analysis_scripts.is_compatible(perm, laminate, n, n - 1):
                        valid_perm = True
                        break
                    elif analysis_scripts.is_compatible(perm, laminate, n, n - 2):
                        valid_perm = True
                        break
            else: #If no laminates, pass.
                valid_perm = True
            if valid_perm:
                filtered_permutations.add(perm_hash)

    return filtered_permutations

# --- Scoring Function ---
def calculate_score(current_superpermutation: str, permutation_hash: int, prodigal_results: dict,
                    winners: dict, losers: dict, layout_memory: LayoutMemory, n: int,
                    golden_ratio_points: list[int], hypothetical_prodigals:dict) -> float:
    """Calculates the score for adding a permutation (n=7 version)."""
    permutation = unhash_permutation(permutation_hash, n)
    permutation_string = "".join(str(x) for x in permutation)
    overlap = calculate_overlap(current_superpermutation, permutation_string)
    score = overlap * 5  # Base score based on overlap

    # Prodigal Result Bonus (very high)
    prodigal_bonus = 0
    for prodigal_id, prodigal in prodigal_results.items():
        if permutation_string in prodigal.sequence:
            prodigal_bonus += prodigal.length * 100  # Large bonus
            break

    #Hypothetical bonus
    hypothetical_bonus = 0
    if hypothetical_prodigals:
        for h_prodigal_id, h_prodigal in hypothetical_prodigals.items():
            if permutation_string in h_prodigal.sequence:
                hypothetical_bonus +=  h_prodigal.length * 25

    # Winner and Loser Bonus/Penalty (using Layout Memory)
    k_values = [n - 1, n - 2]
    layout_bonus = 0
    for k in k_values:
        if len(current_superpermutation) >= k:
            kmer_end = current_superpermutation[-k:]
            kmer_start = permutation_string[:k]
            layout_bonus += layout_memory.get_layout_score(kmer_end, kmer_start, 1)

    # Golden Ratio Bonus (small, dynamic)
    golden_ratio_bonus = 0
    insertion_point = len(current_superpermutation)
    for point in golden_ratio_points:
        distance = abs(insertion_point - point)
        golden_ratio_bonus += math.exp(-distance / (len(current_superpermutation)/20))  # Smaller divisor = tighter

    # Loser Penalty (Veto)
    loser_penalty = 0
    for k in [6, 5]:  # Check 6-mers and 5-mers for n=7
        for i in range(len(permutation_string) - k + 1):
            kmer = permutation_string[i:i+k]
            loser_penalty += losers.get(kmer, 0) * 5 # Penalty for losers

    # Higher-Order Winners/Losers
    higher_order_bonus = 0
    for seq_length in [2, 3]:  # Check sequences of length 2 and 3
      if len(current_superpermutation) >= (n * seq_length):
        prev_seq = current_superpermutation[-(n*seq_length):]
        prev_perms = []
        for i in range(len(prev_seq) - n + 1):
            pp = tuple(int(x) for x in prev_seq[i:i+n])
            if is_valid_permutation(pp, n):
                prev_perms.append(hash_permutation(pp))
        if len(prev_perms) >= (seq_length -1):
            current_seq = tuple(prev_perms[-(seq_length - 1):] + [permutation_hash])
            current_seq_hash = hash(current_seq)
            higher_order_bonus += winners.get(current_seq_hash, 0) * 5
            loser_penalty += losers.get(current_seq_hash, 0) * 5

    score += prodigal_bonus + layout_bonus + golden_ratio_bonus + hypothetical_bonus- loser_penalty + higher_order_bonus

    return score
# --- Main Construction Function ---

def construct_superpermutation(initial_permutations: list, prodigal_results: dict, winners: dict, losers: dict,
                              layout_memory: LayoutMemory, meta_hierarchy: dict, limbo_list: set, n: int,
                              hypothetical_prodigals: dict) -> tuple[str, set[int]]:
    """Constructs a superpermutation using the dynamic prodigal approach (n=7 version).

    Args:
        initial_permutations (list):  Empty list
        prodigal_results (dict): Dictionary of ProdigalResult objects.
        winners (dict): Dictionary of Winner k-mers and their weights.
        losers (dict): Dictionary of Loser k-mers and their weights.
        layout_memory (LayoutMemory): The LayoutMemory object.
        meta_hierarchy (dict): Dictionary for tracking strategy effectiveness.
        limbo_list (set): Set of permutation hashes to avoid.
        n (int): The value of n.
        hypothetical_prodigals (dict): "Hypothetical Prodigals" to use.

    Returns:
        tuple: (superpermutation string, set of used permutation hashes)
    """

    superpermutation = ""
    used_permutations = set()

    # Initialize with the longest "Prodigal Result" (should be the initial 5906)
    if prodigal_results:
        best_prodigal_key = max(prodigal_results, key=lambda k: prodigal_results[k].length)
        superpermutation = prodigal_results[best_prodigal_key].sequence
        used_permutations.update(prodigal_results[best_prodigal_key].permutations)
    else:
        #Should never reach here.
        superpermutation = "1234567" #Bare minimum to start.
        used_permutations.add(hash_permutation((1,2,3,4,5,6,7)))

    golden_ratio_points = calculate_golden_ratio_points(len(superpermutation), levels=3)
    #Create Laminates
    laminates = [analysis_scripts.create_laminate(prodigal_results[key].sequence, n, k) for key in prodigal_results for k in LAYOUT_K_VALUES]


    while True:  # Continue until no more additions can be made or 5906 is hit
        best_candidate = None
        best_score = -float('inf')
        best_candidate_string = ""

        # Find frontier k-mers
        prefix = superpermutation[:n - 1]
        suffix = superpermutation[-(n - 1):]

        # Generate candidates (very small set, focused on the frontier)
        candidates = generate_permutations_on_demand(prefix, suffix, prodigal_results, winners, losers, n, used_permutations, limbo_list, n - 1, hypothetical_prodigals, laminates)
        if not candidates:
            candidates = generate_permutations_on_demand(prefix, suffix, prodigal_results, winners, losers, n, used_permutations, limbo_list, n - 2, hypothetical_prodigals, laminates)
        if not candidates:
            # print("No candidates found. Stopping.")  # Keep for debugging
            break  # No more candidates can be added
        #Deterministic selection from candidates:
        scored_candidates = []
        for candidate_hash in candidates:
            score = calculate_score(superpermutation, candidate_hash, prodigal_results, winners, losers, layout_memory, n, golden_ratio_points, hypothetical_prodigals)
            scored_candidates.append((score, candidate_hash))
        
        best_candidate = None
        if scored_candidates:
            scored_candidates.sort(reverse=True, key=lambda item: item[0]) #Sort by score, DESCENDING
            best_candidate = scored_candidates[0][1] #Get the hash

        if best_candidate is not None:
            best_candidate_perm = unhash_permutation(best_candidate, n)
            overlap = calculate_overlap(superpermutation, "".join(str(x) for x in best_candidate_perm))
            superpermutation += "".join(str(x) for x in best_candidate_perm)[overlap:]  # Add to superpermutation
            used_permutations.add(best_candidate)

            # Update golden ratio points
            golden_ratio_points = calculate_golden_ratio_points(len(superpermutation))

            # Prodigal Result Check
            new_prodigals = analysis_scripts.find_prodigal_results(superpermutation, n, min_length=PRODIGAL_MIN_LENGTH, overlap_threshold=PRODIGAL_OVERLAP_THRESHOLD)  #Use current thresholds
            for prodigal_seq in new_prodigals:
                is_new = True
                for existing_prodigal_id, existing_prodigal in prodigal_results.items():
                    if prodigal_seq in existing_prodigal.sequence:
                        is_new = False
                        break
                if is_new:
                    new_id = len(prodigal_results) + 1
                    prodigal_results[new_id] = ProdigalResult(prodigal_seq, new_id)
                    # print(f"New Prodigal Result found: {prodigal_seq}")  # Keep for debugging
                    #Create and add laminates:
                    laminates.append(analysis_scripts.create_laminate(prodigal_seq, n, n-1))
                    laminates.append(analysis_scripts.create_laminate(prodigal_seq, n, n-2))

            # Update ePUT
            perm_hash = best_candidate
            perm_string = "".join(str(x) for x in best_candidate_perm)
            if perm_hash not in eput:  # Should always be true here
                eput[perm_hash] = PermutationData(best_candidate_perm, in_sample=False, creation_method="dynamic_generation")
            eput[perm_hash].used_count += 1
            eput[perm_hash].used_in_final = True
            # Update neighbors in ePUT (using consistent k values)
            for k in [n - 1, n - 2]:
                prefix = superpermutation[:k]
                suffix = superpermutation[-k:]
                prefix_perms = set()
                suffix_perms = set()
                for i in range(len(prefix) - n + 1):
                    p = tuple(int(x) for x in prefix[i:i+n])
                    if is_valid_permutation(p,n):
                        prefix_perms.add(hash_permutation(p))
                for i in range(len(suffix) - n + 1):
                    p = tuple(int(x) for x in suffix[i:i+n])
                    if is_valid_permutation(p,n):
                        suffix_perms.add(hash_permutation(p))

                for other_perm_hash in prefix_perms:
                    if other_perm_hash != perm_hash:
                        eput[perm_hash].neighbors.add(other_perm_hash)
                        # Add to layout memory if not exist
                        kmer1 = "".join(str(x) for x in unhash_permutation(other_perm_hash, n)[-k:])
                        kmer2 = perm_string[:k]
                        layout_memory.update_distances(kmer1,kmer2, len(superpermutation) - i - len(prefix), "n7_dynamic")
                for other_perm_hash in suffix_perms:
                    if other_perm_hash != perm_hash:
                        eput[perm_hash].neighbors.add(other_perm_hash)
                        kmer1 = perm_string[-k:]
                        kmer2 = "".join(str(x) for x in unhash_permutation(other_perm_hash, n)[:k])
                        layout_memory.update_distances(kmer1, kmer2, 1, "n7_dynamic")
        else:
            break  # If we get here, we are stuck

        if len(superpermutation) >= 5906: #If we reach the target, break.
            break

    return superpermutation, used_permutations

# --- Main Function ---
def main():
    """
    Main function to execute the Dynamic Prodigal Assembly algorithm for n=7.
    Generates multiple distinct 5906 superpermutations.
    """
    # File Paths
    initial_winners_losers_n7_file = "initial_winners_losers_n7.txt"
    prodigal_results_n7_file = "prodigal_results_n7.txt"
    distinct_superpermutations_file = "distinct_superpermutations_n7.txt"
    layout_memory_file = "layout_memory_n7.pkl"

    # Load Initial Data
    initial_winners, initial_losers = {}, {}
    try:
        with open(initial_winners_losers_n7_file, "r") as f:
            for line in f:
                kmer, w_type, weight = line.strip().split(",")
                if w_type == "winner":
                    initial_winners[kmer] = int(weight)
                else:
                    initial_losers[kmer] = int(weight)
    except FileNotFoundError:
        print("Initial Winners/Losers file not found. Starting with empty.")

    initial_n7_prodigals = []
    try:
        with open(prodigal_results_n7_file, "r") as f:
            initial_n7_prodigals = [line.strip() for line in f]
    except FileNotFoundError:
        print("Initial n=7 Prodigal Results file not found.  Starting with empty.")
        #In this case, we will create an empty file
        with open(prodigal_results_n7_file, "w") as f:
            pass
    # Load existing distinct superpermutations
    existing_superpermutations = set()
    try:
        with open(distinct_superpermutations_file, "r") as f:
            for line in f:
                existing_superpermutations.add(line.strip())
    except FileNotFoundError:
        print("No existing distinct superpermutations file found.")
        #Create File
        with open(distinct_superpermutations_file, "w") as f:
            pass

    # Initialize data structures
    prodigal_results, winners, losers, meta_hierarchy, limbo_list, eput, next_prodigal_id = initialize_data(
        "", initial_n7_prodigals, initial_winners, initial_losers
    )  # Start with an EMPTY string

    #Create Initial Laminate
    laminates = []
    for prodigal in initial_n7_prodigals:
        laminates.append(analysis_scripts.create_laminate(prodigal, N, N-1))
        laminates.append(analysis_scripts.create_laminate(prodigal, N, N-2))
    #If no initial, create blank laminate
    if not laminates:
        laminates = [nx.DiGraph()]
    
    # Load Layout memory
    layout_memory = LayoutMemory()
    try:
        layout_memory.load_from_file(layout_memory_file)
        print("Loaded LayoutMemory from file.")
    except FileNotFoundError:
        print("No existing LayoutMemory file found. Starting with a new one.")

    distinct_count = len(existing_superpermutations)
    run_count = 0

    # --- Main Iterative Loop ---
    while True: # Keep generating until a stopping criterion is met
        run_count += 1
        print(f"Starting run {run_count}...")
        start_time = time.time()

        # Set a new random seed for each run
        random.seed(RANDOM_SEED + run_count)  # Use a different seed each run

        # 1. Generate Hypothetical Prodigals
        hypothetical_prodigals = analysis_scripts.generate_hypothetical_prodigals(prodigal_results, winners, losers, N)
        print(f"  Generated {len(hypothetical_prodigals)} hypothetical prodigals.")

        # 2. Construct Superpermutation (Dynamic, Prodigal-Focused)
        superpermutation, used_permutations = construct_superpermutation([], prodigal_results, winners, losers, layout_memory, meta_hierarchy, limbo_list, N, hypothetical_prodigals)
        print(f"  Superpermutation length: {len(superpermutation)}")

        # 3. Analysis and Updates
        analysis_results = analysis_scripts.analyze_superpermutation(superpermutation, N)
        print(f"  Valid: {analysis_results['validity']}")
        print(f"  Overlap Distribution: {analysis_results['overlap_distribution']}")
        
        # Check for 5906 and distinctness
        if analysis_results['validity'] and len(superpermutation) == 5906:
            is_distinct = True
            for existing_sp in existing_superpermutations:
                if not analysis_scripts.is_cyclically_distinct(superpermutation, existing_sp):
                    is_distinct = False
                    break

            if is_distinct:
                print("  Found a *distinct* 5906 superpermutation!")
                distinct_count += 1
                existing_superpermutations.add(superpermutation)
                with open(distinct_superpermutations_file, "a") as f:  # Append
                    f.write(superpermutation + "\n")
                #Create and add laminates
                new_lam_1 = analysis_scripts.create_laminate(superpermutation, N, N-1)
                new_lam_2 = analysis_scripts.create_laminate(superpermutation, N, N-2)
                laminates.append(new_lam_1)
                laminates.append(new_lam_2)
            else:
                print("  Found a 5906 superpermutation, but it's a duplicate.")
        else:
            print("  Run did not produce a valid minimal superpermutation.")
            
        # Update "Winners" and "Losers" (using the new superpermutation)
        new_winners, new_losers = analysis_scripts.calculate_winners_losers([superpermutation], N, k=N-1)
        new_winners2, new_losers2 = analysis_scripts.calculate_winners_losers([superpermutation], N, k=N-2)

        for kmer, weight in new_winners.items():
            winners[kmer] = winners.get(kmer, 0) + weight
        for kmer, weight in new_losers.items():
            losers[kmer] = losers.get(kmer, 0) + weight
        for kmer, weight in new_winners2.items():
            winners[kmer] = winners.get(kmer, 0) + weight
        for kmer, weight in new_losers2.items():
            losers[kmer] = losers.get(kmer, 0) + weight

        # Higher-Order Winners/Losers
        new_seq_winners, new_seq_losers = analysis_scripts.calculate_sequence_winners_losers([superpermutation], N)
        for seq_hash, weight in new_seq_winners.items():
            if seq_hash in winners:
                winners[seq_hash] += weight
            else:
                winners[seq_hash] = weight
        for seq_hash, weight in new_seq_losers.items():
            if seq_hash in losers:
                losers[seq_hash] += weight
            else:
                losers[seq_hash] = weight

        # Add new losers to limbo list
        for kmer, weight in losers.items():
            limbo_list.add(kmer)  # Add to limbo list

        # Find and add new prodigal results (stricter criteria).
        new_prodigals = analysis_scripts.find_prodigal_results(superpermutation, N, min_length=PRODIGAL_MIN_LENGTH, overlap_threshold=PRODIGAL_OVERLAP_THRESHOLD)
        for prodigal_seq in new_prodigals:
            is_new = True
            for existing_prodigal_id, existing_prodigal in prodigal_results.items():
                if prodigal_seq in existing_prodigal.sequence:
                    is_new = False
                    break
            if is_new:
                prodigal_results[next_prodigal_id] = ProdigalResult(prodigal_seq, next_prodigal_id)
                next_prodigal_id += 1
                #Create and add laminate
                laminates.append(analysis_scripts.create_laminate(prodigal_seq, N, N-1))
                laminates.append(analysis_scripts.create_laminate(prodigal_seq, N, N-2))


        # Update ePUT (add all *used* permutations)
        s_tuple = tuple(int(x) for x in superpermutation)
        for i in range(len(s_tuple) - n + 1):
            perm = s_tuple[i:i+n]
            if is_valid_permutation(perm, n):
                perm_hash = hash_permutation(perm)
                if perm_hash not in eput:
                    eput[perm_hash] = PermutationData(perm, in_sample=False, creation_method="dynamic_generation") # Dynamic
                eput[perm_hash].used_count += 1
                eput[perm_hash].used_in_final = True  # Mark as used in this iteration's superpermutation
                # Update neighbors in ePUT (using consistent k values)
                if i > 0:
                    prev_perm = s_tuple[i-1:i-1+n]
                    if is_valid_permutation(prev_perm, n):
                         eput[perm_hash].neighbors.add(hash_permutation(prev_perm))
                if i < len(s_tuple) - n:
                    next_perm = s_tuple[i+1:i+1+n]
                    if is_valid_permutation(next_perm, n):
                        eput[perm_hash].neighbors.add(hash_permutation(next_perm))

        # Update Layout Memory
        layout_memory.add_sequence(superpermutation, N, N-1, f"run_{run_count}")  # Add the new superpermutation
        layout_memory.add_sequence(superpermutation, N, N-2, f"run_{run_count}")

        # Update "Meta-Hierarchy" (simplified for this example)
        meta_hierarchy.setdefault("run_lengths", []).append(len(superpermutation))
        meta_hierarchy.setdefault("prodigal_counts", []).append(len(prodigal_results))
        total_hypotheticals = len(hypothetical_prodigals)
        successful_hypotheticals = 0
        for h_id, h_prodigal in hypothetical_prodigals.items():
            if h_prodigal.sequence in superpermutation:
               successful_hypotheticals += 1
        success_rate = ( successful_hypotheticals / total_hypotheticals) if total_hypotheticals > 0 else 0
        meta_hierarchy.setdefault("hypothetical_success_rate",[]).append(success_rate)


        end_time = time.time()
        print(f"  Run {run_count} completed in {end_time - start_time:.2f} seconds.")
        print(f"  Distinct 5906 superpermutations found so far: {distinct_count}")


        #Stopping Criteria
        if len(superpermutation) == 5906:
            # Check for early stopping based on lack of new distinct solutions
            is_new_solution = True
            for existing_sp in existing_superpermutations:
                if not analysis_scripts.is_cyclically_distinct(superpermutation, existing_sp):
                    is_new_solution = False
                    break

            if not is_new_solution:
                meta_hierarchy["no_new_solutions_count"] = meta_hierarchy.get("no_new_solutions_count",0) + 1
                if meta_hierarchy["no_new_solutions_count"] >= 20:
                    print("No new distinct 5906 solutions found in 20 iterations. Stopping.")
                    break
            else:
                meta_hierarchy["no_new_solutions_count"] = 0  # Reset counter
        if run_count >= 100:
            break


    layout_memory.save_to_file(layout_memory_file)

if __name__ == "__main__":
    random.seed(RANDOM_SEED)
    main()
```

### src/unified/embedder/cand_6_AGRM_py.py

```python
# ===========================================================
# === AGRM System Implementation Codebase (Final Version) ===
# ===========================================================
# Based on validated blueprint derived from user documents and session discussion.
# Includes: Multi-agent architecture, Modulation Controller, Bidirectional Builder,
# Salesman Validator/Patcher, Path Audit Agent, Hybrid Hashing,
# Ephemeral Memory (MDHG-Hash Integration), Dynamic Midpoint, Spiral Reentry,
# Comprehensive Comments.

import math
import time
import random
from collections import deque, Counter, defaultdict
from typing import Any, Dict, List, Tuple, Set, Optional, Union
import numpy as np # Assuming numpy is available for calculations like norm

# Try importing sklearn for KDTree, but provide fallback
try:
    from sklearn.neighbors import KDTree, BallTree
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False
    print("WARNING: scikit-learn not found. Neighbor searches will use less efficient fallback.")

# ===========================================================
# === Multi-Dimensional Hamiltonian Golden Ratio Hash Table ===
# ===========================================================
# Full implementation based on user-provided code and description.
# Source: User Upload mdhg_hash.py content [cite: 1027-1096]
# Integrated into AGRM system for high-complexity state/cache (n>5).

class MDHGHashTable:
    """
    Multi-Dimensional Hamiltonian Golden Ratio Hash Table.
    A hash table implementation that uses multi-dimensional organization,
    Hamiltonian paths for navigation, and golden ratio-based sizing
    to achieve optimal performance for structured access patterns.

    AGRM Integration Notes:
    - Used for complex state (n>5) in AGRM's hybrid hashing.
    - Stores values as tuples: (actual_value, metadata_dict),
      where metadata_dict contains flags like {'source': 'dict', 'retain_flag': True}.
    - Adaptation logic can be influenced by Modulation Controller signals.
    - Includes full logic for buildings, floors, rooms (conceptual), velocity region,
      dimensional core, conflict handling, Hamiltonian path navigation, and dynamic adaptation.
    """
    def __init__(self, capacity: int = 1024, dimensions: int = 3, load_factor_threshold: float = 0.75, config: Dict = {}):
        """
        Initialize the hash table.
        Args:
            capacity: Initial capacity of the hash table
            dimensions: Number of dimensions for the hash space (tunable by AGRM context)
            load_factor_threshold: When to resize the table
            config: Dictionary for additional MDHG-specific parameters
        """
        self.PHI = (1 + math.sqrt(5)) / 2 # Golden ratio [cite: 1018-1019]
        self.config = config # Store config for internal use

        # Core configuration
        self.capacity = max(capacity, 16) # Ensure minimum capacity
        self.dimensions = max(dimensions, 1) # Ensure at least 1 dimension
        self.load_factor_threshold = load_factor_threshold
        self.size = 0

        # Multi-dimensional organization [cite: 1012-1017]
        self.buildings = self._initialize_buildings()
        self.location_map = {} # Maps keys to their current location (building_id, region_type, location_spec)

        # Navigation components
        self.hamiltonian_paths = {} # Pre-computed paths for critical points [cite: 1020-1022, 1037]
        self.path_cache = {} # Cache of paths between points or for key sets
        self.shortcuts = {} # Direct connections between buildings/regions [cite: 1022]

        # Access pattern tracking
        self.access_history = deque(maxlen=config.get("mdhg_access_history_len", 100)) # Track recent accesses
        self.access_frequency = Counter() # Track frequency of key access
        self.co_access_matrix = defaultdict(Counter) # Track keys accessed together [cite: 1022]
        self.path_usage = Counter() # Track usage of cached paths

        # Statistics
        self.stats = {
            'puts': 0, 'gets': 0, 'hits': 0, 'misses': 0, 'collisions': 0,
            'probes_total': 0, 'max_probes': 0, 'reorganizations': 0,
            'resizes': 0, 'promotions_velocity': 0, 'relocations_from_velocity': 0,
            'clusters_relocated': 0
        }

        # Optimization timing
        self.last_minor_optimization = time.time()
        self.last_major_optimization = time.time()
        self.operations_since_optimization = 0

        # Initialize the structure (compute initial paths, etc.)
        self._initialize_structure()
        print(f"MDHGHashTable initialized: Capacity={self.capacity}, Dimensions={self.dimensions}, Buildings={len(self.buildings)}")

    def _initialize_buildings(self) -> Dict:
        """ Initialize the building structure based on golden ratio proportions. """
        # Determine number of buildings, ensuring at least 1
        building_count = max(1, int(math.log(max(2, self.capacity), self.PHI)))
        buildings = {}
        # Ensure base capacity calculation avoids division by zero
        base_capacity_per_building = self.capacity // building_count if building_count > 0 else self.capacity
        if base_capacity_per_building < 16: base_capacity_per_building = 16 # Ensure minimum size

        print(f"  MDHG: Initializing {building_count} buildings, base capacity per building: {base_capacity_per_building}")

        for b in range(building_count):
            building_id = f"B{b}"
            # Calculate regions using golden ratio [cite: 1018]
            # Ensure minimum sizes for regions
            velocity_region_size = max(4, int(base_capacity_per_building / (self.PHI ** 2)))
            core_region_base_size = max(8, int(velocity_region_size * self.PHI)) # Base size before dimensioning

            dimension_sizes = self._calculate_dimension_sizes(core_region_base_size)
            # Actual core capacity is product of dimension sizes
            core_capacity = math.prod(dimension_sizes) if dimension_sizes else 0

            print(f"    Building {building_id}: Velocity Region Size={velocity_region_size}, Core Capacity={core_capacity}, Dim Sizes={dimension_sizes}")

            buildings[building_id] = {
                'velocity_region': [None] * velocity_region_size, # Fast access [cite: 1022]
                'dimensional_core': {}, # Main storage, dict maps coords -> (key, value_tuple) [cite: 1022]
                'conflict_structures': {}, # Handles collisions beyond path probing [cite: 1022]
                'dimension_sizes': dimension_sizes, # For coordinate calculation
                'hot_keys': set(), # Keys frequently accessed in this building
                'access_count': 0, # Track building usage
                'core_capacity': core_capacity # Store calculated core capacity
            }
        return buildings

    def _calculate_dimension_sizes(self, core_region_base_size: int) -> List[int]:
        """ Calculate sizes for each dimension using golden ratio proportions. """
        if self.dimensions <= 0: return []
        # Estimate base size per dimension
        # Using geometric mean approach: base_size ^ dimensions ≈ core_region_base_size
        # Add epsilon to avoid potential log(0) or root(0) issues if base_size is tiny
        safe_base_size = max(1, core_region_base_size)
        base_size = max(2.0, safe_base_size ** (1.0/self.dimensions)) # Use float for calculation

        sizes = []
        product = 1.0
        # Scale dimensions using PHI, ensuring minimum size 2
        for i in range(self.dimensions):
            # Example scaling: could use other GR-based factors
            # Ensure denominator is safe
            phi_exponent = i / max(1.0, float(self.dimensions - 1))
            size_float = base_size / (self.PHI ** phi_exponent)
            size = max(2, int(round(size_float))) # Round before int conversion
            sizes.append(size)
            product *= size

        # Optional: Adjust sizes slightly if product is too far off target
        # This part requires careful balancing logic to avoid infinite loops or drastic changes
        # print(f"      Calculated dimension sizes: {sizes} (Product: {product}, Target Base: {core_region_base_size})")
        return sizes

    def _initialize_structure(self):
        """ Initialize the hash table structure with navigation components. """
        # Pre-compute critical Hamiltonian paths for each building [cite: 1037]
        print("  MDHG: Initializing structure (paths, shortcuts)...")
        for building_id, building in self.buildings.items():
            self._initialize_building_paths(building_id, building)
        # Initialize shortcuts between buildings
        self._initialize_building_shortcuts()
        print("  MDHG: Structure initialization complete.")


    def _initialize_building_paths(self, building_id: str, building: Dict):
        """ Initialize Hamiltonian paths for critical points in a building. """
        dimension_sizes = building.get('dimension_sizes')
        if not dimension_sizes:
            # print(f"    Skipping path init for {building_id}: No dimensions.")
            return # Skip if no dimensions

        # Generate critical points (corners, center)
        critical_points = set()
        # Add corner points
        corners = self._generate_corner_points(dimension_sizes)
        critical_points.update(corners)
        # Add center point
        center = tuple(d // 2 for d in dimension_sizes)
        critical_points.add(center)

        # Compute and store paths for these critical points
        building_paths = {}
        computed_count = 0
        for point in critical_points:
            # Ensure point is valid within dimensions
            if len(point) != self.dimensions: continue
            valid_point = all(0 <= point[i] < dimension_sizes[i] for i in range(self.dimensions))

            if valid_point:
                path = self._compute_hamiltonian_path(building_id, point)
                if path: # Only store if path computation succeeds
                    path_key = (building_id, point)
                    building_paths[path_key] = path # Store paths per building first
                    computed_count += 1

        self.hamiltonian_paths.update(building_paths) # Add building paths to global store
        # print(f"    Initialized {computed_count} Hamiltonian paths for building {building_id}.")


    def _generate_corner_points(self, dimension_sizes: List[int]) -> List[Tuple]:
        """ Generate corner points for a multi-dimensional space. """
        if not dimension_sizes: return []
        corners = []
        num_dims = len(dimension_sizes)
        num_corners = 2 ** num_dims
        for i in range(num_corners):
            corner = []
            for d in range(num_dims):
                # Use bit masking to determine min (0) or max (size-1) for each dimension
                if (i >> d) & 1:
                    corner.append(max(0, dimension_sizes[d] - 1)) # Use max index
                else:
                    corner.append(0) # Use min index
            corners.append(tuple(corner))
        return corners

    def _initialize_building_shortcuts(self):
        """ Initialize shortcuts between buildings. """
        building_ids = list(self.buildings.keys())
        shortcuts_created = 0
        # Create shortcuts only if there's more than one building
        if len(building_ids) > 1:
            for i, b1 in enumerate(building_ids):
                for j, b2 in enumerate(building_ids):
                    if i != j:
                        # Create bidirectional shortcuts
                        if self._create_building_shortcut(b1, b2):
                            shortcuts_created += 1
        # print(f"  Initialized {shortcuts_created} building shortcuts.")

    def _create_building_shortcut(self, building1: str, building2: str) -> bool:
        """ Create a shortcut between two buildings with default connection points. """
        building1_data = self.buildings.get(building1)
        building2_data = self.buildings.get(building2)
        # Check if dimensions are valid before creating shortcut
        if not building1_data or not building2_data or \
           not building1_data.get('dimension_sizes') or not building2_data.get('dimension_sizes') or \
           len(building1_data['dimension_sizes']) != self.dimensions or \
           len(building2_data['dimension_sizes']) != self.dimensions:
            # print(f"Warning: Cannot create shortcut between {building1} and {building2} due to invalid dimensions.")
            return False # Cannot create shortcut if building data is incomplete

        # Use center points as default connection points
        center1 = tuple(d // 2 for d in building1_data['dimension_sizes'])
        center2 = tuple(d // 2 for d in building2_data['dimension_sizes'])

        shortcut_key = (building1, building2)
        self.shortcuts[shortcut_key] = {
            'entry_point': center1, # Entry point in building1
            'exit_point': center2,  # Exit point in building2 (conceptually)
            'cost': 1.0 / self.PHI, # Lower cost than regular traversal (heuristic)
            'usage_count': 0
        }
        return True

    def _compute_hamiltonian_path(self, building_id: str, start_coords: Tuple) -> List[Tuple]:
        """
        Compute a Hamiltonian-like path (visits many points uniquely) starting from coordinates.
        Uses GR steps. This is a heuristic path, not guaranteed to be truly Hamiltonian or optimal length.
        """
        building = self.buildings.get(building_id)
        if not building or not building.get('dimension_sizes'): return []
        dimension_sizes = building['dimension_sizes']

        # Basic validation of start_coords
        if len(start_coords) != self.dimensions: return []
        if not all(0 <= start_coords[i] < dimension_sizes[i] for i in range(self.dimensions)): return []

        path = [start_coords]
        current = list(start_coords)
        visited = {start_coords}

        # Determine path length heuristic
        total_core_points = math.prod(dimension_sizes) if dimension_sizes else 0
        if total_core_points == 0: return path # Path is just the start point

        path_length_limit = min(total_core_points, self.config.get("mdhg_path_length_limit", 1000))
        # Aim for a path length that covers a reasonable fraction, e.g., sqrt or similar heuristic
        path_length_target = max(self.dimensions * 2, int(math.sqrt(total_core_points) * 2)) # Cover more?
        path_length = min(path_length_limit, path_length_target)

        # Use golden ratio for dimension selection and step direction bias
        for step in range(1, path_length):
            # Choose dimension based on golden ratio progression [cite: 1021]
            dim_choice = int((step * self.PHI) % self.dimensions)

            # Determine step direction (+1 or -1) based on another GR sequence
            direction_bias = (step * self.PHI**2) % 1.0
            step_dir = 1 if direction_bias < 0.5 else -1

            # Try moving in the chosen dimension and direction
            next_coord_list = list(current)
            next_coord_list[dim_choice] = (next_coord_list[dim_choice] + step_dir + dimension_sizes[dim_choice]) % dimension_sizes[dim_choice] # Ensure positive result
            next_coords = tuple(next_coord_list)

            if next_coords not in visited:
                path.append(next_coords)
                visited.add(next_coords)
                current = next_coord_list
            else:
                # Collision: Try alternative dimensions or directions (simple linear probe)
                found_alternative = False
                for alt_offset in range(1, self.dimensions + 1): # Try all dimensions + opposite dir
                    # Try alternative dimension, original direction
                    alt_dim = (dim_choice + alt_offset) % self.dimensions
                    alt_coord_list = list(current)
                    alt_coord_list[alt_dim] = (alt_coord_list[alt_dim] + step_dir + dimension_sizes[alt_dim]) % dimension_sizes[alt_dim]
                    alt_coords = tuple(alt_coord_list)
                    if alt_coords not in visited:
                        path.append(alt_coords)
                        visited.add(alt_coords)
                        current = alt_coord_list
                        found_alternative = True
                        break

                    # Try alternative dimension, opposite direction
                    alt_coord_list = list(current)
                    alt_coord_list[alt_dim] = (alt_coord_list[alt_dim] - step_dir + dimension_sizes[alt_dim]) % dimension_sizes[alt_dim]
                    alt_coords = tuple(alt_coord_list)
                    if alt_coords not in visited:
                        path.append(alt_coords)
                        visited.add(alt_coords)
                        current = alt_coord_list
                        found_alternative = True
                        break

                # If no alternative found after checking all dims/dirs, stop path
                if not found_alternative:
                    # print(f"      Path generation stuck at step {step}, coords {current}")
                    break # Stop if stuck

        return path

    # --- Hashing Functions ---
    def _hash(self, key: Any) -> int:
        """ Primary hash function. Using MurmurHash for better distribution. """
        return self._murmur_hash(key)

    def _secondary_hash(self, key: Any) -> int:
        """ Secondary hash function for specific regions like velocity. Using FNV. """
        return self._fnv_hash(key)

    def _murmur_hash(self, key: Any) -> int:
        """ MurmurHash3 32-bit implementation. """
        key_bytes = str(key).encode('utf-8')
        length = len(key_bytes)
        seed = 0x9747b28c # Example seed
        c1 = 0xcc9e2d51
        c2 = 0x1b873593
        r1 = 15
        r2 = 13
        m = 5
        n = 0xe6546b64
        hash_value = seed

        nblocks = length // 4
        for i in range(nblocks):
            idx = i * 4
            k = (key_bytes[idx] |
                 (key_bytes[idx + 1] << 8) |
                 (key_bytes[idx + 2] << 16) |
                 (key_bytes[idx + 3] << 24))
            k = (k * c1) & 0xFFFFFFFF
            k = ((k << r1) | (k >> (32 - r1))) & 0xFFFFFFFF
            k = (k * c2) & 0xFFFFFFFF
            hash_value ^= k
            hash_value = ((hash_value << r2) | (hash_value >> (32 - r2))) & 0xFFFFFFFF
            hash_value = ((hash_value * m) + n) & 0xFFFFFFFF

        tail_index = nblocks * 4
        k = 0
        tail_size = length & 3
        if tail_size >= 3: k ^= key_bytes[tail_index + 2] << 16
        if tail_size >= 2: k ^= key_bytes[tail_index + 1] << 8
        if tail_size >= 1: k ^= key_bytes[tail_index]
        if tail_size > 0:
            k = (k * c1) & 0xFFFFFFFF
            k = ((k << r1) | (k >> (32 - r1))) & 0xFFFFFFFF
            k = (k * c2) & 0xFFFFFFFF
            hash_value ^= k

        hash_value ^= length
        hash_value ^= hash_value >> 16
        hash_value = (hash_value * 0x85ebca6b) & 0xFFFFFFFF
        hash_value ^= hash_value >> 13
        hash_value = (hash_value * 0xc2b2ae35) & 0xFFFFFFFF
        hash_value ^= hash_value >> 16

        return abs(hash_value) # Ensure positive

    def _fnv_hash(self, key: Any) -> int:
        """ FNV-1a 32-bit hash implementation. """
        key_bytes = str(key).encode('utf-8')
        fnv_prime = 0x01000193 # 16777619
        fnv_offset_basis = 0x811c9dc5 # 2166136261
        hash_value = fnv_offset_basis
        for byte in key_bytes:
            hash_value ^= byte
            hash_value = (hash_value * fnv_prime) & 0xFFFFFFFF
        return abs(hash_value) # Ensure positive

    def _hash_to_building(self, key: Any) -> str:
        """ Determine which building should contain a key using primary hash. """
        if not self.buildings: raise ValueError("MDHGHashTable has no buildings initialized.")
        hash_value = self._hash(key)
        building_idx = hash_value % len(self.buildings)
        return f"B{building_idx}"

    def _hash_to_velocity_index(self, key: Any, building_id: str) -> int:
        """ Calculate velocity region index using secondary hash. """
        building = self.buildings.get(building_id)
        if not building: raise ValueError(f"Building {building_id} not found.")
        velocity_size = len(building['velocity_region'])
        if velocity_size == 0: return 0
        return self._secondary_hash(key) % velocity_size

    def _hash_to_coords(self, key: Any, building_id: str) -> Optional[Tuple]:
        """ Calculate multi-dimensional coordinates using variations of primary hash. """
        building = self.buildings.get(building_id)
        if not building: raise ValueError(f"Building {building_id} not found.")
        dimension_sizes = building.get('dimension_sizes')
        if not dimension_sizes or len(dimension_sizes) != self.dimensions:
            return None # Cannot calculate coords if dimensions mismatch

        coords = []
        # Use primary hash and modify it for each dimension to get variation
        base_hash = self._hash(key)
        for i in range(self.dimensions):
            # Simple variation: XOR with dimension index and shift
            dim_hash = (base_hash ^ (i * 0x9e3779b9)) # Use golden ratio conjugate for mixing
            dim_hash = (dim_hash >> i) | (dim_hash << (32 - i)) & 0xFFFFFFFF # Rotate
            coord_val = abs(dim_hash) % dimension_sizes[i]
            coords.append(coord_val)
        return tuple(coords)

    def _hash_to_conflict_key(self, key: Any, coords: Tuple) -> int:
        """ Create a conflict key combining key hash and coordinates hash. """
        key_hash = self._hash(key)
        coords_hash = hash(coords) # Python's hash for tuple
        return abs(key_hash ^ coords_hash)

    # --- Core Put/Get/Remove ---

    def put(self, key: Any, value: Any) -> None:
        """
        Insert a key-value pair into the hash table.
        Handles routing, velocity region, core, collisions, and conflict structures.
        Value should be (actual_value, metadata_dict) for AGRM integration.
        """
        self.stats['puts'] += 1
        self.operations_since_optimization += 1

        # 1. Determine Target Building
        building_id = self._hash_to_building(key)
        building = self.buildings.get(building_id)
        if not building: # Fallback if building calculation failed somehow
            if not self.buildings: raise RuntimeError("MDHG Hash Table has no buildings.")
            building_id = list(self.buildings.keys())[0]
            building = self.buildings[building_id]
            print(f"Warning: Falling back to building {building_id} for key {key}.")
        building['access_count'] += 1

        # 2. Try Velocity Region
        velocity_idx = self._hash_to_velocity_index(key, building_id)
        if 0 <= velocity_idx < len(building['velocity_region']):
            velocity_entry = building['velocity_region'][velocity_idx]
            if velocity_entry is None:
                building['velocity_region'][velocity_idx] = (key, value)
                self.location_map[key] = (building_id, 'velocity', velocity_idx)
                self.size += 1
                self._update_access_patterns(key)
                self._check_optimization_and_resize()
                return
            elif velocity_entry[0] == key:
                building['velocity_region'][velocity_idx] = (key, value) # Update
                self._update_access_patterns(key)
                return
            # Else: Collision in velocity, proceed to core

        # 3. Try Dimensional Core
        coords = self._hash_to_coords(key, building_id)
        if coords is not None:
            if coords not in building['dimensional_core']:
                building['dimensional_core'][coords] = (key, value)
                self.location_map[key] = (building_id, 'dimensional', coords)
                self.size += 1
                self._update_access_patterns(key)
                self._check_optimization_and_resize()
                return
            elif building['dimensional_core'][coords][0] == key:
                building['dimensional_core'][coords] = (key, value) # Update
                self._update_access_patterns(key)
                return
            else:
                # Collision in dimensional core
                self.stats['collisions'] += 1
                # 4. Follow Hamiltonian Path
                new_coords, probes = self._follow_hamiltonian_path_for_put(building_id, coords)
                self.stats['probes_total'] += probes
                self.stats['max_probes'] = max(self.stats['max_probes'], probes)
                if new_coords:
                    building['dimensional_core'][new_coords] = (key, value)
                    self.location_map[key] = (building_id, 'dimensional', new_coords)
                    self.size += 1
                    self._update_access_patterns(key)
                    self._check_optimization_and_resize()
                    return
                # Else: Path probing failed, proceed to conflict structure
        else: # Coords calculation failed, go directly to conflict structure
             coords = tuple([0]*self.dimensions) # Use fallback coords for conflict key


        # 5. Use Conflict Structure
        conflict_key_hash = self._hash_to_conflict_key(key, coords)
        if conflict_key_hash not in building['conflict_structures']:
            building['conflict_structures'][conflict_key_hash] = {} # Use dict as simple conflict list

        # Store/update in conflict structure
        if key not in building['conflict_structures'][conflict_key_hash]:
            self.size += 1 # Increment size only if new key overall
        building['conflict_structures'][conflict_key_hash][key] = value
        self.location_map[key] = (building_id, 'conflict', conflict_key_hash)
        self._update_access_patterns(key)
        self._check_optimization_and_resize()


    def get(self, key: Any) -> Any:
        """ Retrieve value tuple (val, meta) or None. """
        self.stats['gets'] += 1
        self.operations_since_optimization += 1
        probes = 0

        # 1. Check Location Map Cache
        loc_info = self.location_map.get(key)
        if loc_info:
            building_id, region_type, location_spec = loc_info
            building = self.buildings.get(building_id)
            if building:
                building['access_count'] += 1
                value = None
                if region_type == 'velocity':
                    probes += 1
                    if 0 <= location_spec < len(building['velocity_region']):
                        entry = building['velocity_region'][location_spec]
                        if entry and entry[0] == key: value = entry[1]
                elif region_type == 'dimensional':
                    probes += 1
                    entry = building['dimensional_core'].get(location_spec)
                    if entry and entry[0] == key: value = entry[1]
                elif region_type == 'conflict':
                    probes += 1
                    conflict_map = building['conflict_structures'].get(location_spec)
                    if conflict_map: value = conflict_map.get(key)

                if value is not None:
                    self.stats['hits'] += 1
                    self._update_stats_and_patterns(key, probes)
                    return value
                else: # Location map was stale/incorrect
                     if key in self.location_map: del self.location_map[key]
            else: # Invalid building in map
                 if key in self.location_map: del self.location_map[key]
            # Fall through to full search if map check failed

        # 2. Full Search (if map failed or key not in map)
        primary_building_id = self._hash_to_building(key)
        value, building_probes = self._search_building(primary_building_id, key)
        probes += building_probes
        if value is not None:
            self._update_stats_and_patterns(key, probes)
            return value

        # 3. Search Other Buildings (Only if collisions can spill buildings - assumed NO for now)

        # 4. Key Not Found
        self.stats['misses'] += 1
        self._update_stats_and_patterns(key, probes, found=False)
        return None

    def _update_stats_and_patterns(self, key: Any, probes: int, found: bool = True):
         """ Helper to update stats and access patterns after a get attempt. """
         self.stats['probes_total'] += probes
         self.stats['max_probes'] = max(self.stats['max_probes'], probes)
         if found:
             self._update_access_patterns(key)


    def _search_building(self, building_id: str, key: Any) -> Tuple[Any, int]:
        """ Search for a key within a specific building. Returns (Value, probes). """
        building = self.buildings.get(building_id)
        if not building: return None, 0
        building['access_count'] += 1
        probes = 0

        # Check velocity
        velocity_idx = self._hash_to_velocity_index(key, building_id)
        probes += 1
        if 0 <= velocity_idx < len(building['velocity_region']):
            entry = building['velocity_region'][velocity_idx]
            if entry and entry[0] == key:
                self.stats['hits'] += 1
                self.location_map[key] = (building_id, 'velocity', velocity_idx)
                return entry[1], probes

        # Check dimensional core primary
        coords = self._hash_to_coords(key, building_id)
        if coords is not None:
            probes += 1
            entry = building['dimensional_core'].get(coords)
            if entry and entry[0] == key:
                self.stats['hits'] += 1
                self.location_map[key] = (building_id, 'dimensional', coords)
                return entry[1], probes

            # Check conflict structure based on primary coords
            conflict_key_hash = self._hash_to_conflict_key(key, coords)
            probes += 1
            conflict_map = building['conflict_structures'].get(conflict_key_hash)
            if conflict_map and key in conflict_map:
                self.stats['hits'] += 1
                self.location_map[key] = (building_id, 'conflict', conflict_key_hash)
                return conflict_map[key], probes

            # Follow Hamiltonian path if not found yet
            value, path_probes = self._search_path(building_id, coords, key)
            probes += path_probes
            if value is not None:
                # Hit, location map update happen inside _search_path
                return value, probes

        else: # Coords failed, check conflict based on fallback
            fallback_coords = tuple([0]*self.dimensions)
            conflict_key_hash = self._hash_to_conflict_key(key, fallback_coords)
            probes += 1
            conflict_map = building['conflict_structures'].get(conflict_key_hash)
            if conflict_map and key in conflict_map:
                self.stats['hits'] += 1
                self.location_map[key] = (building_id, 'conflict', conflict_key_hash)
                return conflict_map[key], probes

        # Key not found in this building
        return None, probes


    def _search_path(self, building_id: str, start_coords: Tuple, key: Any) -> Tuple[Any, int]:
         """ Search for a key along a Hamiltonian path starting near coords. """
         building = self.buildings.get(building_id)
         if not building or not self.hamiltonian_paths: return None, 0

         nearest_path_key = self._find_nearest_path_key(building_id, start_coords)
         if not nearest_path_key: return None, 0
         path = self.hamiltonian_paths[nearest_path_key]
         if not path: return None, 0

         start_idx = self._find_path_start_index(path, start_coords)
         max_probes = self.config.get("mdhg_max_search_probes", 20)
         probes = 0
         path_len = len(path)
         forward_steps = 0
         backward_steps = 0

         while probes < max_probes and (forward_steps + backward_steps) < path_len:
             # Check forward
             idx = (start_idx + forward_steps) % path_len
             check_coords = path[idx]
             probes += 1
             entry = building['dimensional_core'].get(check_coords)
             if entry and entry[0] == key:
                 self.stats['hits'] += 1
                 self.location_map[key] = (building_id, 'dimensional', check_coords)
                 return entry[1], probes
             forward_steps += 1

             if probes >= max_probes or (forward_steps + backward_steps) >= path_len: break

             # Check backward (if path has more than one point)
             if backward_steps < forward_steps and path_len > 1:
                 idx = (start_idx - backward_steps - 1 + path_len) % path_len
                 check_coords = path[idx]
                 probes += 1
                 entry = building['dimensional_core'].get(check_coords)
                 if entry and entry[0] == key:
                     self.stats['hits'] += 1
                     self.location_map[key] = (building_id, 'dimensional', check_coords)
                     return entry[1], probes
                 backward_steps += 1

         return None, probes # Not found along path segment

    def _follow_hamiltonian_path_for_put(self, building_id: str, start_coords: Tuple) -> Tuple[Optional[Tuple], int]:
        """ Follow a Hamiltonian path to find an empty slot for insertion. """
        building = self.buildings.get(building_id)
        if not building or not self.hamiltonian_paths: return None, 0

        nearest_path_key = self._find_nearest_path_key(building_id, start_coords)
        if not nearest_path_key: return None, 0
        path = self.hamiltonian_paths[nearest_path_key]
        if not path: return None, 0

        start_idx = self._find_path_start_index(path, start_coords)
        max_probes = self.config.get("mdhg_max_put_probes", 20)
        probes = 0
        path_len = len(path)
        forward_steps = 0
        backward_steps = 0

        while probes < max_probes and (forward_steps + backward_steps) < path_len:
            # Check forward
            idx = (start_idx + forward_steps) % path_len
            coords = path[idx]
            probes += 1
            if coords not in building['dimensional_core']:
                return coords, probes
            forward_steps += 1

            if probes >= max_probes or (forward_steps + backward_steps) >= path_len: break

            # Check backward
            if backward_steps < forward_steps and path_len > 1:
                idx = (start_idx - backward_steps - 1 + path_len) % path_len
                coords = path[idx]
                probes += 1
                if coords not in building['dimensional_core']:
                    return coords, probes
                backward_steps += 1

        return None, probes # No empty slot found

    def remove(self, key: Any) -> bool:
        """ Remove a key-value pair. """
        # 1. Check Location Map first
        loc_info = self.location_map.get(key)
        removed = False
        if loc_info:
            building_id, region_type, location_spec = loc_info
            building = self.buildings.get(building_id)
            if building:
                if region_type == 'velocity':
                    if 0 <= location_spec < len(building['velocity_region']):
                        entry = building['velocity_region'][location_spec]
                        if entry and entry[0] == key:
                            building['velocity_region'][location_spec] = None
                            removed = True
                elif region_type == 'dimensional':
                    entry = building['dimensional_core'].get(location_spec)
                    if entry and entry[0] == key:
                        del building['dimensional_core'][location_spec]
                        removed = True
                elif region_type == 'conflict':
                    conflict_map = building['conflict_structures'].get(location_spec)
                    if conflict_map and key in conflict_map:
                        del conflict_map[key]
                        if not conflict_map: del building['conflict_structures'][location_spec]
                        removed = True

                if removed:
                    del self.location_map[key]
                    self.size -= 1
                    return True
                else: # Location map was stale
                    del self.location_map[key]
            else: # Invalid building in map
                 del self.location_map[key]

        # 2. Full Search if map failed or key not in map
        primary_building_id = self._hash_to_building(key)
        if self._remove_from_building(primary_building_id, key):
            return True

        # 3. Search other buildings (if spillover possible - assuming not)

        return False # Key not found

    def _remove_from_building(self, building_id: str, key: Any) -> bool:
         """ Removes key from a specific building. Helper for remove(). """
         building = self.buildings.get(building_id)
         if not building: return False

         # Check velocity
         velocity_idx = self._hash_to_velocity_index(key, building_id)
         if 0 <= velocity_idx < len(building['velocity_region']):
             entry = building['velocity_region'][velocity_idx]
             if entry and entry[0] == key:
                 building['velocity_region'][velocity_idx] = None
                 if key in self.location_map: del self.location_map[key]
                 self.size -= 1
                 return True

         # Check core and conflict (primary coords)
         coords = self._hash_to_coords(key, building_id)
         if coords is not None:
             entry = building['dimensional_core'].get(coords)
             if entry and entry[0] == key:
                 del building['dimensional_core'][coords]
                 if key in self.location_map: del self.location_map[key]
                 self.size -= 1
                 return True

             conflict_key_hash = self._hash_to_conflict_key(key, coords)
             conflict_map = building['conflict_structures'].get(conflict_key_hash)
             if conflict_map and key in conflict_map:
                 del conflict_map[key]
                 if not conflict_map: del building['conflict_structures'][conflict_key_hash]
                 if key in self.location_map: del self.location_map[key]
                 self.size -= 1
                 return True

             # Search path if necessary (more complex removal)
             # Simplified: Assume if not at primary/velocity/conflict, it's not easily removable

         else: # Coords failed, check conflict based on fallback
              fallback_coords = tuple([0]*self.dimensions)
              conflict_key_hash = self._hash_to_conflict_key(key, fallback_coords)
              conflict_map = building['conflict_structures'].get(conflict_key_hash)
              if conflict_map and key in conflict_map:
                  del conflict_map[key]
                  if not conflict_map: del building['conflict_structures'][conflict_key_hash]
                  if key in self.location_map: del self.location_map[key]
                  self.size -= 1
                  return True

         return False

    # --- Helper methods for path finding ---
    def _find_nearest_path_key(self, building_id: str, coords: Tuple) -> Optional[Tuple]:
        """ Find the key (building_id, start_coords) of the nearest pre-computed path. """
        min_dist_sq = float('inf')
        nearest_key = None
        if coords is None: return None

        # Filter paths belonging to the target building
        building_paths = {k: v for k, v in self.hamiltonian_paths.items() if k[0] == building_id}
        if not building_paths: return None

        for path_key, path_data in building_paths.items():
            path_start_coords = path_key[1]
            # Ensure coordinates have same dimension before calculating distance
            if len(coords) != len(path_start_coords): continue
            # Calculate squared Euclidean distance for efficiency
            dist_sq = sum((c1 - c2)**2 for c1, c2 in zip(coords, path_start_coords))
            if dist_sq < min_dist_sq:
                min_dist_sq = dist_sq
                nearest_key = path_key
        return nearest_key

    def _find_path_start_index(self, path: List[Tuple], coords: Tuple) -> int:
        """ Find the index in a path closest to the given coordinates. """
        if not path: return 0
        if coords is None: return 0

        min_dist_sq = float('inf')
        best_idx = 0
        for i, path_coords in enumerate(path):
             # Ensure coordinates have same dimension
             if len(coords) != len(path_coords): continue
             dist_sq = sum((c1 - c2)**2 for c1, c2 in zip(coords, path_coords))
             if dist_sq < min_dist_sq:
                 min_dist_sq = dist_sq
                 best_idx = i
             if min_dist_sq == 0: break # Exact match found
        return best_idx

    # --- Dynamic Adaptation & Optimization (Placeholders - Require full logic) ---

    def _update_access_patterns(self, key: Any) -> None:
        """ Update access frequency, history, and co-access matrix. """
        self.access_history.append(key)
        self.access_frequency[key] += 1
        # Update co-access (simplified)
        if len(self.access_history) > 1:
            last_key = self.access_history[-2]
            if last_key != key:
                self.co_access_matrix[last_key][key] += 1
                self.co_access_matrix[key][last_key] += 1

        # Trigger potential promotion based on frequency
        promo_threshold = self.config.get("mdhg_velocity_promo_threshold", 10)
        if self.access_frequency[key] >= promo_threshold:
            if key in self.location_map:
                building_id = self.location_map[key][0]
                self._consider_velocity_promotion(key, building_id)


    def _consider_velocity_promotion(self, key: Any, building_id: str) -> None:
         """ Consider promoting a key to the velocity region if beneficial. """
         building = self.buildings.get(building_id)
         if not building or key not in self.location_map: return

         current_loc = self.location_map[key]
         if current_loc[1] == 'velocity': return # Already there

         target_idx = self._hash_to_velocity_index(key, building_id)
         if not (0 <= target_idx < len(building['velocity_region'])): return # Invalid index

         current_entry = building['velocity_region'][target_idx]
         key_freq = self.access_frequency.get(key, 0)
         should_promote = False

         if current_entry is None:
             should_promote = True
         else:
             occupant_key = current_entry[0]
             occupant_freq = self.access_frequency.get(occupant_key, 0)
             # Promote if new key is significantly more frequent (using PHI ratio)
             if key_freq > occupant_freq * self.PHI:
                 should_promote = True
                 # Relocate the occupant if it's being evicted
                 print(f"    MDHG: Evicting {occupant_key} (freq {occupant_freq}) from velocity for {key} (freq {key_freq})")
                 self._relocate_from_velocity(occupant_key, current_entry[1], building_id)
                 self.stats['relocations_from_velocity'] += 1

         if should_promote:
             # Get current value (get() handles finding it)
             value_tuple = self.get(key) # This will update access patterns again
             if value_tuple is not None:
                 print(f"    MDHG: Promoting key {key} to velocity region in {building_id}")
                 # Remove from old location BEFORE putting in new one
                 self._remove_from_current_location(key) # Removes from core/conflict
                 building['velocity_region'][target_idx] = (key, value_tuple)
                 self.location_map[key] = (building_id, 'velocity', target_idx) # Update location map
                 self.stats['promotions_velocity'] += 1


    def _relocate_from_velocity(self, key: Any, value: Any, building_id: str) -> None:
        """ Relocate a key evicted from velocity region back to core/conflict. """
        # This is essentially a 'put' operation, but we know it's not in velocity.
        # We need to ensure size isn't incremented again.
        building = self.buildings.get(building_id)
        if not building: return

        # Try dimensional core first
        coords = self._hash_to_coords(key, building_id)
        if coords is not None:
            if coords not in building['dimensional_core']:
                building['dimensional_core'][coords] = (key, value)
                self.location_map[key] = (building_id, 'dimensional', coords)
                return
            else: # Collision
                new_coords, _ = self._follow_hamiltonian_path_for_put(building_id, coords)
                if new_coords:
                    building['dimensional_core'][new_coords] = (key, value)
                    self.location_map[key] = (building_id, 'dimensional', new_coords)
                    return
        # Fallback to conflict structure
        fallback_coords = coords if coords is not None else tuple([0]*self.dimensions)
        conflict_key_hash = self._hash_to_conflict_key(key, fallback_coords)
        if conflict_key_hash not in building['conflict_structures']:
            building['conflict_structures'][conflict_key_hash] = {}
        building['conflict_structures'][conflict_key_hash][key] = value
        self.location_map[key] = (building_id, 'conflict', conflict_key_hash)


    def _remove_from_current_location(self, key: Any) -> None:
        """ Helper to remove key from core/conflict AFTER checking location map. """
        if key not in self.location_map: return
        building_id, region_type, location_spec = self.location_map[key]
        building = self.buildings.get(building_id)
        if not building: return

        removed = False
        if region_type == 'dimensional':
            if location_spec in building['dimensional_core'] and building['dimensional_core'][location_spec][0] == key:
                del building['dimensional_core'][location_spec]
                removed = True
        elif region_type == 'conflict':
            conflict_map = building['conflict_structures'].get(location_spec)
            if conflict_map and key in conflict_map:
                del conflict_map[key]
                if not conflict_map: del building['conflict_structures'][location_spec]
                removed = True
        # Note: We don't delete from location map here, caller handles final update


    def _check_optimization_and_resize(self) -> None:
        """ Check if optimization or resize is needed based on operations or time. """
        current_time = time.time()
        ops_threshold_minor = self.config.get("mdhg_ops_thresh_minor", 100)
        time_threshold_minor = self.config.get("mdhg_time_thresh_minor", 1.0)
        ops_threshold_major = self.config.get("mdhg_ops_thresh_major", 1000)
        time_threshold_major = self.config.get("mdhg_time_thresh_major", 5.0)

        needs_minor_opt = (self.operations_since_optimization > 0 and self.operations_since_optimization % ops_threshold_minor == 0) or \
                          (current_time - self.last_minor_optimization >= time_threshold_minor)
        needs_major_opt = (self.operations_since_optimization > 0 and self.operations_since_optimization % ops_threshold_major == 0) or \
                          (current_time - self.last_major_optimization >= time_threshold_major)

        if needs_major_opt:
            # print("    MDHG: Performing major optimization...")
            self._perform_major_optimization()
            self.last_major_optimization = current_time
            self.last_minor_optimization = current_time # Reset minor timer too
            self.operations_since_optimization = 0 # Reset counter
        elif needs_minor_opt:
            # print("    MDHG: Performing minor optimization...")
            self._perform_minor_optimization()
            self.last_minor_optimization = current_time
            # Don't reset major timer or op counter on minor opt

        # Check resize AFTER potential optimizations
        current_load_factor = self.size / self.capacity if self.capacity > 0 else 1.0
        if current_load_factor > self.load_factor_threshold:
            print(f"    MDHG: Load factor {current_load_factor:.2f} exceeds threshold {self.load_factor_threshold}. Resizing.")
            self._resize()

    def _perform_minor_optimization(self) -> None:
        """ Perform minor optimizations like promoting hot keys. """
        hot_key_count = self.config.get("mdhg_hot_key_count", 100)
        hot_key_min_freq = self.config.get("mdhg_hot_key_min_freq", 5)
        hot_keys_global = self.access_frequency.most_common(hot_key_count)
        promoted_count = 0
        for key, freq in hot_keys_global:
            if freq < hot_key_min_freq: break
            if key in self.location_map:
                building_id = self.location_map[key][0]
                # This call might result in promotion
                self._consider_velocity_promotion(key, building_id)
                # Check if promotion actually happened (location map changed)
                if key in self.location_map and self.location_map[key][1] == 'velocity':
                     promoted_count += 1
        # if promoted_count > 0: print(f"      MDHG Minor Opt: Considered {len(hot_keys_global)} hot keys, promoted {promoted_count} to velocity.")


    def _perform_major_optimization(self) -> None:
        """ Perform major structural reorganizations. """
        self.stats['reorganizations'] += 1
        start_time = time.time()
        # print("      MDHG Major Opt: Updating shortcuts...")
        # self._update_shortcuts() # Placeholder
        # print("      MDHG Major Opt: Identifying and relocating clusters...")
        # self._identify_and_relocate_key_clusters() # Placeholder
        # print("      MDHG Major Opt: Pruning path cache...")
        # self._prune_path_cache() # Placeholder
        end_time = time.time()
        print(f"    MDHG: Major optimization complete in {end_time - start_time:.4f}s (Placeholders used).")


    def _update_shortcuts(self) -> None:
        """ Placeholder: Update shortcuts based on observed usage patterns. """
        # Requires tracking inter-building traversals or using co-access matrix across buildings
        pass

    def _identify_and_relocate_key_clusters(self) -> None:
        """ Placeholder: Identify clusters of co-accessed keys and move them closer. """
        # Requires graph analysis of co_access_matrix and complex relocation logic
        clusters_found = 0
        # ... implementation needed ...
        if clusters_found > 0:
            self.stats['clusters_relocated'] += clusters_found
            # print(f"        MDHG Cluster Opt: Relocated {clusters_found} key clusters.")
        pass

    def _prune_path_cache(self) -> None:
        """ Placeholder: Prune the path cache based on usage or recency. """
        max_cache_size = self.config.get("mdhg_path_cache_max_size", 100)
        if len(self.path_cache) > max_cache_size:
            # Simple prune: Keep top 50% most used
            keep_count = max_cache_size // 2
            sorted_usage = sorted(self.path_usage.items(), key=lambda item: item[1], reverse=True)
            keys_to_keep = {key for key, usage in sorted_usage[:keep_count]}
            old_size = len(self.path_cache)
            self.path_cache = {k: v for k, v in self.path_cache.items() if k in keys_to_keep}
            self.path_usage = {k: v for k, v in self.path_usage.items() if k in keys_to_keep}
            # print(f"        MDHG Cache Prune: Reduced path cache from {old_size} to {len(self.path_cache)} entries.")
        pass

    def _resize(self) -> None:
        """ Resize the hash table when load factor is exceeded. """
        self.stats['resizes'] += 1
        old_capacity = self.capacity
        # Increase capacity using golden ratio
        new_capacity = max(old_capacity + 1, int(old_capacity * self.PHI * 1.1)) # Add buffer
        print(f"    MDHG Resize: Increasing capacity from {old_capacity} to {new_capacity}")

        # Store old data temporarily
        old_items = []
        for key, loc_info in self.location_map.items():
            # Retrieve value from old structure before wiping it
            building_id_old, region_type_old, location_spec_old = loc_info
            building_old = self.buildings.get(building_id_old)
            value_tuple = None
            if building_old:
                if region_type_old == 'velocity':
                    if 0 <= location_spec_old < len(building_old['velocity_region']):
                        entry = building_old['velocity_region'][location_spec_old]
                        if entry and entry[0] == key: value_tuple = entry[1]
                elif region_type_old == 'dimensional':
                    entry = building_old['dimensional_core'].get(location_spec_old)
                    if entry and entry[0] == key: value_tuple = entry[1]
                elif region_type_old == 'conflict':
                    conflict_map = building_old['conflict_structures'].get(location_spec_old)
                    if conflict_map: value_tuple = conflict_map.get(key)
            if value_tuple is not None:
                old_items.append((key, value_tuple))
            # else: print(f"Warning: Could not retrieve value for key {key} during resize.")

        # Re-initialize with new capacity
        self.capacity = new_capacity
        self.size = 0 # Reset size, will be repopulated
        self.buildings = self._initialize_buildings()
        self.location_map = {} # Clear location map
        self._initialize_structure() # Recompute paths etc. for new structure

        # Rehash all elements
        print(f"    MDHG Resize: Rehashing {len(old_items)} elements...")
        rehash_start_time = time.time()
        for key, value_tuple in old_items:
            self.put(key, value_tuple) # Re-insert into the new structure
        rehash_end_time = time.time()
        print(f"    MDHG Resize: Rehashing complete in {rehash_end_time - rehash_start_time:.4f}s. New size: {self.size}")

        # Reset optimization timers after resize
        self.last_minor_optimization = time.time()
        self.last_major_optimization = time.time()
        self.operations_since_optimization = 0

# ==============================
# === AGRM State Management ===
# ==============================

class AGRMStateBus:
    """
    Manages the shared state between AGRM agents.
    Acts as the central repository for path data, node states,
    sweep metadata, modulation parameters, and agent signals.
    Uses Hybrid Hashing strategy based on complexity.
    """
    def __init__(self, cities: List[Tuple[float, float]], config: Dict):
        """
        Initializes the state bus.
        Args:
            cities: List of (x, y) coordinates for the nodes.
            config: Dictionary containing configuration parameters for AGRM and MDHG.
        """
        self.config = config
        self.num_nodes = len(cities)
        self.cities = cities # List of (x, y) tuples

        # --- Core State ---
        self.visited_fwd: Set[int] = set() # Nodes visited by forward builder
        self.visited_rev: Set[int] = set() # Nodes visited by reverse builder
        self.path_fwd: List[int] = [] # Path built by forward builder
        self.path_rev: List[int] = [] # Path built by reverse builder (in reverse order)
        self.full_path: Optional[List[int]] = None # Final merged path

        # --- Sweep Metadata (Populated by Navigator) ---
        self.sweep_data: Dict[int, Dict] = {} # node_id -> {rank, shell, sector, quadrant, hemisphere, density, gr_score}
        self.sweep_center: Optional[Tuple[float, float]] = None
        self.max_radius: float = 0.0
        self.shell_width: float = 0.0
        self.start_node_fwd: Optional[int] = None
        self.start_node_rev: Optional[int] = None

        # --- Legal Graph & Modulation State (Managed by Controller) ---
        # Legal edges are computed ephemerally by the validator agent
        # self.legal_edges: Optional[Dict[int, List[int]]] = None # Not stored persistently
        # Store default modulation params for reset
        self.default_modulation_params = {
            "shell_tolerance": config.get("mod_shell_tolerance", 2),
            "curvature_limit": config.get("mod_curvature_limit", math.pi / 4), # Approx 45 deg
            "sector_tolerance": config.get("mod_sector_tolerance", 2),
            "distance_cap_factor": config.get("mod_dist_cap_factor", 3.0), # Multiplier for avg dist in shell
            "allow_sparse_unlock": False,
            "soft_override_active": False,
            "reentry_mode": False
        }
        self.modulation_params = self.default_modulation_params.copy() # Start with defaults
        self.current_phase: str = "initializing" # 'initializing', 'building', 'pre-midpoint', 'converging', 'post-midpoint', 'post-merge', 'patching', 'finalizing', 'complete'

        # --- Hybrid Hashing State ---
        self.complexity_threshold = config.get("hybrid_hash_threshold", 5) # n=5 complexity threshold [cite: 896-913]
        # Instantiate caches
        self.low_complexity_cache = {} # Standard dict for n <= 5 tasks/data
        # Instantiate MDHG for high complexity state. Tailoring parameters applied here.
        mdhg_dims = config.get("mdhg_dimensions", 3)
        mdhg_cap = max(1024, self.num_nodes) # Capacity scales with problem size
        self.high_complexity_cache = MDHGHashTable(capacity=mdhg_cap, dimensions=mdhg_dims, config=config) # Pass config
        print(f"StateBus: Initialized Hybrid Caching (n={self.complexity_threshold} threshold). MDHG Dims={mdhg_dims}, Capacity={mdhg_cap}")

        # --- Agent Feedback / Flags ---
        self.builder_fwd_state = {"status": "idle", "stalls": 0, "last_node": -1, "current_shell": -1, "current_sector": -1}
        self.builder_rev_state = {"status": "idle", "stalls": 0, "last_node": -1, "current_shell": -1, "current_sector": -1}
        self.salesman_proposals: List[Dict] = [] # Patches suggested for review by Salesman
        self.accepted_patches: List[Dict] = [] # Patches approved by Controller for splicing

    def get_cache(self, complexity_level: int) -> Union[Dict, MDHGHashTable]:
        """
        Returns the appropriate cache backend based on complexity level 'n'.
        Called by agents needing to store/retrieve state ephemerally.
        Args:
            complexity_level: The estimated complexity 'n' of the current operation.
        Returns:
            The standard dict or the MDHGHashTable instance.
        """
        # Note: Complexity level 'n' determination logic resides in the calling agent/controller
        # This provides the interface based on that determination.
        if complexity_level <= self.complexity_threshold:
            # print(f"DEBUG: Using low complexity cache (dict) for n={complexity_level}")
            return self.low_complexity_cache
        else:
            # print(f"DEBUG: Using high complexity cache (MDHG) for n={complexity_level}")
            return self.high_complexity_cache

    def migrate_data(self, key: Any, current_complexity: int, new_complexity: int):
        """
        Migrates a key between caches if the complexity threshold is crossed.
        Called by the Modulation Controller. Assumes value needs to be fetched.
        Args:
            key: The key to migrate.
            current_complexity: The previous complexity level 'n'.
            new_complexity: The new complexity level 'n'.
        """
        # Determine source and target caches
        source_cache = self.get_cache(current_complexity)
        target_cache = self.get_cache(new_complexity)

        # Only migrate if the cache type actually changes
        if type(source_cache) == type(target_cache):
            return # No migration needed

        value_to_migrate = None
        metadata = {}

        # Get value from source cache
        if isinstance(source_cache, MDHGHashTable):
            result = source_cache.get(key)
            if result:
                value_to_migrate, metadata = result # MDHG stores tuple
        elif isinstance(source_cache, dict):
            value_to_migrate = source_cache.get(key)
            metadata = {'source': 'dict'} # Assume origin if coming from dict

        # If value exists in source, remove it and put it in target
        if value_to_migrate is not None:
            # Remove from source
            if isinstance(source_cache, MDHGHashTable):
                source_cache.remove(key)
            elif isinstance(source_cache, dict):
                if key in source_cache: del source_cache[key]

            # Put into target
            if isinstance(target_cache, MDHGHashTable):
                 # Ensure metadata includes source info
                 metadata['source'] = 'dict' if isinstance(source_cache, dict) else metadata.get('source', 'mdhg')
                 # Ensure retain_flag exists, default to False if not present
                 metadata['retain_flag'] = metadata.get('retain_flag', False)
                 target_cache.put(key, (value_to_migrate, metadata)) # Store as tuple
                 print(f"StateBus: Migrated key {key} from {type(source_cache).__name__} to MDHG.")
            elif isinstance(target_cache, dict):
                 target_cache[key] = value_to_migrate # Store only value in dict
                 print(f"StateBus: Migrated key {key} from MDHG to {type(target_cache).__name__}.")

    # --- Rest of AGRMStateBus methods ---
    # (update_sweep_data, get_node_sweep_data, is_visited, add_visited,
    #  get_unvisited_nodes, update_modulation_params, update_builder_state,
    #  check_convergence, merge_paths, add_salesman_proposal, etc.)
    # These remain largely the same as provided before, ensuring they interact
    # correctly with the rest of the system state variables.
    # ... (Previous AGRMStateBus methods included here for completeness) ...
    # Note: Ensure methods like add_visited correctly interact with the
    #       get_cache() method if storing visited status in hybrid caches.
    #       Currently, visited status uses Python sets directly for simplicity.

    def update_sweep_data(self, sweep_results: Dict):
        """ Updates state bus with data generated by the Navigator sweep. """
        print("StateBus: Updating with Navigator sweep data...")
        self.sweep_data = sweep_results.get('node_data', {})
        self.sweep_center = sweep_results.get('center')
        self.max_radius = sweep_results.get('max_radius', 0.0)
        self.shell_width = sweep_results.get('shell_width', 0.0)
        self.start_node_fwd = sweep_results.get('start_node_fwd')
        self.start_node_rev = sweep_results.get('start_node_rev')

        # Initialize visited sets and paths
        self.visited_fwd.clear()
        self.visited_rev.clear()
        self.path_fwd = []
        self.path_rev = []
        self.full_path = None
        self.current_phase = "building" # Ready to start building

        if self.start_node_fwd is not None:
            self.visited_fwd.add(self.start_node_fwd)
            self.path_fwd = [self.start_node_fwd]
            fwd_data = self.get_node_sweep_data(self.start_node_fwd)
            self.builder_fwd_state = {"status": "running", "stalls": 0, "last_node": self.start_node_fwd,
                                      "current_shell": fwd_data.get('shell', -1),
                                      "current_sector": fwd_data.get('sector', -1)}
        else:
            self.builder_fwd_state["status"] = "error"

        if self.start_node_rev is not None:
            # Ensure start nodes are different if possible, handle single node case
            if self.start_node_rev != self.start_node_fwd:
                 self.visited_rev.add(self.start_node_rev)
            self.path_rev = [self.start_node_rev]
            rev_data = self.get_node_sweep_data(self.start_node_rev)
            self.builder_rev_state = {"status": "running", "stalls": 0, "last_node": self.start_node_rev,
                                      "current_shell": rev_data.get('shell', -1),
                                      "current_sector": rev_data.get('sector', -1)}
        else:
            self.builder_rev_state["status"] = "error"

        print(f"StateBus: Sweep data loaded. Fwd starts at {self.start_node_fwd}, Rev starts at {self.start_node_rev}")

    def get_node_sweep_data(self, node_id: int) -> Dict:
        """ Gets sweep metadata for a specific node. Returns empty dict if not found. """
        return self.sweep_data.get(node_id, {})

    def is_visited(self, node_id: int) -> bool:
        """ Checks if a node has been visited by EITHER builder using internal sets. """
        return node_id in self.visited_fwd or node_id in self.visited_rev

    def add_visited(self, node_id: int, builder_type: str):
        """ Adds a node to the appropriate visited set and path, updates builder state. """
        node_data = self.get_node_sweep_data(node_id)
        current_shell = node_data.get('shell', -1)
        current_sector = node_data.get('sector', -1)

        if builder_type == 'forward':
            if node_id not in self.visited_fwd:
                self.visited_fwd.add(node_id)
                self.path_fwd.append(node_id)
                self.builder_fwd_state.update({
                    "last_node": node_id, "stalls": 0, "status": "running",
                    "current_shell": current_shell, "current_sector": current_sector
                })
        elif builder_type == 'reverse':
             if node_id not in self.visited_rev:
                self.visited_rev.add(node_id)
                self.path_rev.append(node_id)
                self.builder_rev_state.update({
                    "last_node": node_id, "stalls": 0, "status": "running",
                    "current_shell": current_shell, "current_sector": current_sector
                })

    def get_unvisited_nodes(self) -> Set[int]:
        """ Returns the set of all nodes not yet visited by either builder. """
        all_nodes = set(range(self.num_nodes))
        visited_all = self.visited_fwd.union(self.visited_rev)
        return all_nodes - visited_all

    def update_modulation_params(self, new_params: Dict):
        """ Updates dynamic modulation parameters (called by Controller). """
        self.modulation_params.update(new_params)
        # print(f"StateBus: Modulation params updated: {self.modulation_params}")

    def update_builder_state(self, builder_type: str, status: Optional[str] = None, stalled: Optional[bool] = None):
         """ Updates the status of a builder agent, tracking stalls. """
         state = self.builder_fwd_state if builder_type == 'forward' else self.builder_rev_state
         if status:
             state["status"] = status
         if stalled is True:
             state["stalls"] += 1
             state["status"] = "stalled" # Mark as stalled
         elif stalled is False: # Explicitly told not stalled (i.e., progress made)
             state["stalls"] = 0
             if not status: state["status"] = "running" # Assume running if progress made

    def check_convergence(self) -> bool:
         """ Checks if builders meet criteria to merge paths using dynamic midpoint logic. """
         node_fwd = self.builder_fwd_state["last_node"]
         node_rev = self.builder_rev_state["last_node"]
         if node_fwd == -1 or node_rev == -1 or \
            self.builder_fwd_state["status"] not in ["running", "stalled"] or \
            self.builder_rev_state["status"] not in ["running", "stalled"]:
             return False

         shell_fwd = self.builder_fwd_state["current_shell"]
         shell_rev = self.builder_rev_state["current_shell"]
         sector_fwd = self.builder_fwd_state["current_sector"]
         sector_rev = self.builder_rev_state["current_sector"]
         if shell_fwd == -1 or shell_rev == -1 or sector_fwd == -1 or sector_rev == -1: return False

         shell_threshold = self.config.get("convergence_shell_threshold", 1)
         shell_overlap = abs(shell_fwd - shell_rev) <= shell_threshold

         num_sectors = self.config.get("sweep_num_sectors", 8)
         sector_threshold = self.config.get("convergence_sector_threshold", 1)
         sector_diff = abs(sector_fwd - sector_rev)
         sector_proximity = min(sector_diff, num_sectors - sector_diff) <= sector_threshold

         stall_dist_factor = self.config.get("convergence_stall_dist_factor", 5.0)
         stall_dist_threshold = stall_dist_factor * max(1.0, self.shell_width or 10.0)
         phys_dist = math.dist(self.cities[node_fwd], self.cities[node_rev])
         is_stalled = self.builder_fwd_state["stalls"] > 3 or self.builder_rev_state["stalls"] > 3
         stall_convergence = is_stalled and phys_dist <= stall_dist_threshold

         converged = (shell_overlap and sector_proximity) or stall_convergence

         if converged:
             print(f"StateBus: Convergence detected between FWD {node_fwd} and REV {node_rev}")
             self.current_phase = "converging"
             self.builder_fwd_state["status"] = "converged"
             self.builder_rev_state["status"] = "converged"
         return converged

    def merge_paths(self) -> bool:
        """ Merges paths after convergence. Returns True if complete, False if patching needed. """
        if self.current_phase != "converging": return False
        print("StateBus: Attempting path merge...")
        node_fwd_last = self.path_fwd[-1]
        node_rev_last = self.path_rev[-1]
        merged_list = list(self.path_fwd)
        reversed_rev_path = self.path_rev[::-1]
        if node_fwd_last == node_rev_last:
            merged_list.extend(reversed_rev_path[1:])
        else:
            merged_list.extend(reversed_rev_path)
        self.full_path = merged_list

        visited_final = set(self.full_path)
        missed_nodes = set(range(self.num_nodes)) - visited_final
        if missed_nodes:
            print(f"StateBus WARNING: Path merge complete, but {len(missed_nodes)} nodes missed.")
            self.current_phase = "patching"
            return False
        else:
            if len(self.full_path) > 1 and self.full_path[0] != self.full_path[-1]:
                self.full_path.append(self.full_path[0]) # Close the loop
            print(f"StateBus: Path merge successful. All {self.num_nodes} nodes included.")
            self.current_phase = "merged"
            return True

    def add_salesman_proposal(self, proposal: Dict):
        self.salesman_proposals.append(proposal)

    def get_salesman_proposals(self) -> List[Dict]:
        return self.salesman_proposals

    def clear_salesman_proposals(self):
        self.salesman_proposals = []

    def store_accepted_patch(self, patch: Dict):
        self.accepted_patches.append(patch)

    def get_accepted_patches(self) -> List[Dict]:
        return self.accepted_patches

    def clear_accepted_patches(self):
        self.accepted_patches = []

    def splice_patch(self, patch: Dict) -> bool:
        """ Applies an accepted patch to the full_path. """
        if not self.full_path or self.current_phase not in ["merged", "finalizing", "complete"]:
            print("ERROR: Cannot splice patch, path not ready.")
            return False
        try:
            start_idx, end_idx = patch['segment_indices']
            new_subpath = patch['new_subpath_nodes']
            if not (0 <= start_idx < end_idx < len(self.full_path)):
                print(f"ERROR: Invalid splice indices {start_idx}, {end_idx}")
                return False
            # Assumes new_subpath replaces nodes from index start_idx+1 up to end_idx-1
            print(f"StateBus: Splicing patch {new_subpath} between indices {start_idx} and {end_idx}")
            self.full_path = self.full_path[:start_idx+1] + new_subpath + self.full_path[end_idx:]
            print(f"StateBus: Path spliced. New length: {len(self.full_path)}")
            return True
        except Exception as e:
            print(f"ERROR: Exception during patch splice: {e}")
            return False

# ============================
# === AGRM Agent: Navigator ===
# ============================
# (NavigatorGR class code as provided previously - verified complete)
class NavigatorGR:
    """
    Performs Golden Ratio sweeps to gather spatial and structural metadata.
    Does NOT build paths. Provides data for AGRM filtering and pathing.
    Includes dynamic shell width, quadrant/hemisphere/sector tagging, k-NN density.
    """
    def __init__(self, cities: List[Tuple[float, float]], config: Dict):
        self.cities = cities
        self.num_nodes = len(cities)
        self.config = config
        self.PHI = (1 + math.sqrt(5)) / 2
        self.sweep_data: Dict[int, Dict] = {i:{} for i in range(self.num_nodes)} # Pre-initialize
        self.center: Optional[Tuple[float, float]] = None
        self.max_radius: float = 0.0
        self.shell_width: float = 0.0
        self.start_node_fwd: Optional[int] = None
        self.start_node_rev: Optional[int] = None

    def _calculate_center(self):
        if not self.cities: self.center = (0.0, 0.0); return
        sum_x = sum(c[0] for c in self.cities)
        sum_y = sum(c[1] for c in self.cities)
        self.center = (sum_x / self.num_nodes, sum_y / self.num_nodes)

    def _calculate_radii_and_angles(self):
        if self.center is None: self._calculate_center()
        cx, cy = self.center
        max_r_sq = 0
        for i, (x, y) in enumerate(self.cities):
            dx, dy = x - cx, y - cy
            radius_sq = dx*dx + dy*dy
            radius = math.sqrt(radius_sq) if radius_sq > 0 else 0
            angle = math.atan2(dy, dx)
            self.sweep_data[i].update({'radius': radius, 'angle': angle})
            if radius_sq > max_r_sq: max_r_sq = radius_sq
        self.max_radius = math.sqrt(max_r_sq) if max_r_sq > 0 else 0

    def _assign_shells_and_sectors(self):
        if self.max_radius == 0 and self.num_nodes > 1: self._calculate_radii_and_angles()
        if self.max_radius == 0: # Handle single node or all nodes at center
             for i in range(self.num_nodes):
                 self.sweep_data[i]['shell'] = 0
                 self.sweep_data[i]['sector'] = 0
             self.shell_width = 1.0
             return

        desired_shells = self.config.get("sweep_num_shells", 10)
        self.shell_width = (self.max_radius / desired_shells) if desired_shells > 0 else self.max_radius
        if self.shell_width <= 1e-9: self.shell_width = 1.0

        num_sectors = self.config.get("sweep_num_sectors", 8)
        if num_sectors <= 0: num_sectors = 1
        sector_angle = 2 * math.pi / num_sectors

        shell_counts = Counter()
        for i in range(self.num_nodes):
            radius = self.sweep_data[i].get('radius', 0.0)
            angle = self.sweep_data[i].get('angle', 0.0)
            shell = int(radius // self.shell_width)
            shell = min(shell, desired_shells - 1) if desired_shells > 0 else 0
            self.sweep_data[i]['shell'] = max(0, shell)
            shell_counts[self.sweep_data[i]['shell']] += 1
            normalized_angle = (angle + 2 * math.pi) % (2 * math.pi)
            sector = int(normalized_angle // sector_angle)
            self.sweep_data[i]['sector'] = min(sector, num_sectors - 1)
        # print(f"  Navigator: Shell distribution: {dict(sorted(shell_counts.items()))}")

    def _calculate_gr_sweep_scores(self):
        # Placeholder: Rank by shell, then angle. Needs proper GR spiral logic.
        temp_nodes = []
        for i in range(self.num_nodes):
            shell = self.sweep_data[i].get('shell', 999)
            angle = self.sweep_data[i].get('angle', 0.0)
            score = shell + (abs(angle) / (2*math.pi)) # Simple composite score
            temp_nodes.append((score, i))
        temp_nodes.sort()
        for rank, (score, i) in enumerate(temp_nodes):
            self.sweep_data[i]['sweep_rank'] = rank
            self.sweep_data[i]['gr_score'] = score
        if temp_nodes:
            self.start_node_fwd = temp_nodes[0][1]
            self.start_node_rev = temp_nodes[-1][1]
            # print(f"  Navigator: Determined Fwd Start={self.start_node_fwd}, Rev Start={self.start_node_rev}")

    def _assign_quadrants_and_hemispheres(self):
        if self.center is None: self._calculate_center()
        cx, cy = self.center
        if not any('sweep_rank' in d for i,d in self.sweep_data.items()):
             print("  Navigator ERROR: Sweep rank needed for hemisphere assignment.")
             return
        midpoint_rank = self.num_nodes // 2
        for i, (x, y) in enumerate(self.cities):
            if x >= cx and y >= cy: quadrant = "Q1"
            elif x < cx and y >= cy: quadrant = "Q2"
            elif x < cx and y < cy: quadrant = "Q3"
            else: quadrant = "Q4"
            self.sweep_data[i]['quadrant'] = quadrant
            rank = self.sweep_data[i].get('sweep_rank', -1)
            hemisphere = "A_start" if rank < midpoint_rank else "B_end"
            self.sweep_data[i]['hemisphere'] = hemisphere

    def _classify_density(self):
        print("  Navigator: Classifying node density...")
        if not HAS_SKLEARN or self.num_nodes < 3: # Need at least 3 points for k-NN with k>=1
            print("  Navigator WARNING: Using basic shell density (sklearn not found or N too small).")
            nodes_per_shell = Counter(d.get('shell', -1) for d in self.sweep_data.values())
            if not nodes_per_shell: return # Avoid division by zero
            avg_nodes_per_shell = self.num_nodes / max(1, len(nodes_per_shell))
            dense_threshold = avg_nodes_per_shell * self.config.get("density_dense_factor", 1.5)
            sparse_threshold = avg_nodes_per_shell * self.config.get("density_sparse_factor", 0.5)
            for i in range(self.num_nodes):
                shell = self.sweep_data[i].get('shell', -1)
                shell_count = nodes_per_shell.get(shell, 0)
                if shell_count >= dense_threshold: density = "dense"
                elif shell_count <= sparse_threshold: density = "sparse"
                else: density = "midling"
                self.sweep_data[i]['density'] = density
        else:
            coords = np.array(self.cities)
            k = self.config.get("density_knn_k", 10)
            k = min(k, self.num_nodes - 1)
            if k <= 0: # Handle N=1 or N=2 case
                 for i in range(self.num_nodes): self.sweep_data[i]['density'] = "midling"
                 return

            # Use BallTree for potentially better performance on some distributions
            tree = BallTree(coords)
            # Query for k+1 neighbors to exclude self
            distances, _ = tree.query(coords, k=k + 1)
            # Calculate average distance to k nearest neighbors (excluding self)
            avg_distances = np.mean(distances[:, 1:], axis=1)

            mean_avg_dist = np.mean(avg_distances)
            std_avg_dist = np.std(avg_distances)
            # Avoid zero std dev
            if std_avg_dist < 1e-9: std_avg_dist = mean_avg_dist * 0.1 if mean_avg_dist > 0 else 1.0

            density_factor = self.config.get("density_std_dev_factor", 0.75)
            dense_threshold = mean_avg_dist - std_avg_dist * density_factor
            sparse_threshold = mean_avg_dist + std_avg_dist * density_factor

            for i in range(self.num_nodes):
                avg_dist = avg_distances[i]
                if avg_dist <= dense_threshold: density = "dense"
                elif avg_dist >= sparse_threshold: density = "sparse"
                else: density = "midling"
                self.sweep_data[i]['density'] = density

        density_counts = Counter(d.get('density', 'unknown') for d in self.sweep_data.values())
        print(f"  Navigator: Density classification complete. Counts: {dict(density_counts)}")


    def run_sweep(self) -> Dict:
        """ Executes the full sweep and data generation process. """
        print("Navigator: Running sweep...")
        start_time = time.time()
        self._calculate_center()
        self._calculate_radii_and_angles()
        self._assign_shells_and_sectors()
        self._calculate_gr_sweep_scores() # Assigns ranks
        self._assign_quadrants_and_hemispheres() # Assigns hemi based on rank
        self._classify_density() # Assigns density
        end_time = time.time()
        print(f"Navigator: Sweep complete in {end_time - start_time:.4f} seconds.")
        return {
            'node_data': self.sweep_data, 'center': self.center, 'max_radius': self.max_radius,
            'shell_width': self.shell_width, 'start_node_fwd': self.start_node_fwd, 'start_node_rev': self.start_node_rev
        }

# =======================================
# === AGRM Agent: Legal Edge Validator ===
# =======================================
# (AGRMEdgeValidator class code as provided previously - verified complete)
class AGRMEdgeValidator:
    """
    Provides methods to check if a transition (edge) between two nodes
    is legal according to the current dynamic AGRM modulation parameters.
    Operates ephemerally - computes legality on demand using data from the StateBus.
    """
    def __init__(self, bus: AGRMStateBus, config: Dict):
        self.bus = bus
        self.config = config
        self.PHI = (1 + math.sqrt(5)) / 2

    def is_edge_legal(self, node_from: int, node_to: int, builder_type: str) -> bool:
        """ Checks all AGRM legality rules for a potential edge. """
        params = self.bus.modulation_params
        data_from = self.bus.get_node_sweep_data(node_from)
        data_to = self.bus.get_node_sweep_data(node_to)
        if not data_from or not data_to: return False

        if not self._check_shell_proximity(data_from, data_to, params): return False
        if not self._check_sector_continuity(data_from, data_to, params): return False
        if not self._check_phase_timing(data_to, params): return False
        if not self._check_curvature(node_from, node_to, builder_type, params): return False
        if not self._check_distance_cap(node_from, node_to, data_from, params): return False
        # Add Quadrant transition checks if needed
        return True

    def _check_shell_proximity(self, data_from: Dict, data_to: Dict, params: Dict) -> bool:
        shell_from = data_from.get('shell', -1)
        shell_to = data_to.get('shell', -1)
        if shell_from == -1 or shell_to == -1: return False
        shell_diff = abs(shell_from - shell_to)
        is_reentry_inward = params.get("reentry_mode", False) and shell_to < shell_from
        within_tolerance = shell_diff <= params.get("shell_tolerance", 2)
        return within_tolerance or is_reentry_inward

    def _check_sector_continuity(self, data_from: Dict, data_to: Dict, params: Dict) -> bool:
        sector_from = data_from.get('sector', -1)
        sector_to = data_to.get('sector', -1)
        if sector_from == -1 or sector_to == -1: return False
        num_sectors = self.config.get("sweep_num_sectors", 8)
        if num_sectors <= 0: return True
        sector_diff = abs(sector_from - sector_to)
        angular_diff = min(sector_diff, num_sectors - sector_diff)
        return angular_diff <= params.get("sector_tolerance", 2)

    def _check_phase_timing(self, data_to: Dict, params: Dict) -> bool:
        """ Checks if moving to a sparse zone is allowed based on phase unlock state. """
        if params.get("allow_sparse_unlock", False):
            return True # Sparse zones are allowed
        else:
            # Disallow entry *into* sparse zones before unlock
            return data_to.get('density') != "sparse"

    def _check_curvature(self, node_from: int, node_to: int, builder_type: str, params: Dict) -> bool:
        """ Checks if the turn angle respects the dynamic GR curvature limit. """
        path = self.bus.path_fwd if builder_type == 'forward' else self.bus.path_rev
        if len(path) < 2: return True # No angle to check

        node_prev = path[-2]
        try:
            pos_prev = self.bus.cities[node_prev]
            pos_from = self.bus.cities[node_from]
            pos_to = self.bus.cities[node_to]
        except IndexError: return False # Invalid index

        vec1 = (pos_from[0] - pos_prev[0], pos_from[1] - pos_prev[1])
        vec2 = (pos_to[0] - pos_from[0], pos_to[1] - pos_from[1])
        len1 = math.hypot(vec1[0], vec1[1])
        len2 = math.hypot(vec2[0], vec2[1])
        if len1 < 1e-9 or len2 < 1e-9: return True # Allow if points overlap

        dot_product = vec1[0] * vec2[0] + vec1[1] * vec2[1]
        cos_angle = max(-1.0, min(1.0, dot_product / (len1 * len2)))
        angle = math.acos(cos_angle)
        return angle <= params.get("curvature_limit", math.pi / 4)

    def _check_distance_cap(self, node_from: int, node_to: int, data_from: Dict, params: Dict) -> bool:
        """ Checks if the edge distance exceeds a dynamic cap. """
        avg_dist_in_shell = max(1.0, self.bus.shell_width or 10.0) # Proxy
        base_dist_cap = avg_dist_in_shell * params.get("distance_cap_factor", 3.0)
        if data_from.get('density') == "sparse":
            base_dist_cap *= self.config.get("dist_cap_sparse_mult", 1.5)

        effective_dist_cap = base_dist_cap
        if params.get("soft_override_active", False) or params.get("reentry_mode", False):
             effective_dist_cap *= self.config.get("dist_cap_override_mult", 1.5)
        try:
            actual_dist = math.dist(self.bus.cities[node_from], self.bus.cities[node_to])
        except IndexError: return False
        return actual_dist <= effective_dist_cap

# =======================================
# === AGRM Agent: Modulation Controller ===
# =======================================
# (ModulationController class code as provided previously - verified complete)
class ModulationController:
    """
    The 'brain' of AGRM. Manages system state, agent coordination,
    dynamic legality modulation, phase unlocks, and recovery triggers.
    Uses Hybrid Hashing logic. Includes dynamic adjustments based on feedback.
    """
    def __init__(self, bus: AGRMStateBus, config: Dict):
        self.bus = bus
        self.config = config
        self.default_modulation_params = self.bus.modulation_params.copy()
        self.complexity_threshold = config.get("hybrid_hash_threshold", 5)

    def assess_complexity(self, context: Dict) -> int:
        # Placeholder - needs better logic based on context
        return context.get("num_candidates", 1)

    def select_cache(self, context: Dict) -> Union[Dict, MDHGHashTable]:
        complexity_n = self.assess_complexity(context)
        return self.bus.get_cache(complexity_n)

    def trigger_migration_check(self, key: Any, old_n: int, new_n: int):
        # Simplified: Assumes value needs to be fetched if migrating dict->MDHG
        if (old_n <= self.complexity_threshold < new_n):
             source_cache = self.bus.get_cache(old_n)
             if key in source_cache:
                 value = source_cache.get(key) # Get value before migrating
                 self.bus.migrate_data(key, old_n, new_n, value) # Pass value
        elif (old_n > self.complexity_threshold >= new_n):
             self.bus.migrate_data(key, old_n, new_n) # Value fetched inside migrate_data


    def update_controller_state(self):
        """ Main update loop for the controller - applies dynamic modulation. """
        fwd_stalls = self.bus.builder_fwd_state["stalls"]
        rev_stalls = self.bus.builder_rev_state["stalls"]
        fwd_status = self.bus.builder_fwd_state["status"]
        rev_status = self.bus.builder_rev_state["status"]

        nodes_visited_count = len(self.bus.visited_fwd) + len(self.bus.visited_rev)
        progress_percent = nodes_visited_count / max(1, self.bus.num_nodes)

        new_params = {}
        params_changed = False
        current_params = self.bus.modulation_params

        # --- Adaptive Unlocking ---
        midpoint_percent = self.config.get("midpoint_unlock_percent", 0.5)
        if progress_percent >= midpoint_percent and not current_params["allow_sparse_unlock"]:
            print("CONTROLLER: Midpoint reached. Unlocking sparse zones.")
            new_params["allow_sparse_unlock"] = True
            params_changed = True
        # Update overall phase on bus if needed (e.g., based on progress)
        if progress_percent >= midpoint_percent and self.bus.current_phase == "building":
             self.bus.current_phase = "post-midpoint"

        # --- Dynamic Modulation & Override Logic ---
        stall_threshold = self.config.get("controller_stall_threshold", 5)
        severe_stall_threshold = stall_threshold * self.config.get("controller_severe_stall_factor", 2)
        override_active_now = False
        reentry_active_now = False

        # Check for severe stalls -> trigger reentry
        if (fwd_stalls >= severe_stall_threshold or rev_stalls >= severe_stall_threshold) and not current_params["reentry_mode"]:
            print(f"CONTROLLER: Severe stall ({fwd_stalls}, {rev_stalls}). Triggering Reentry Mode.")
            new_params["reentry_mode"] = True
            new_params["soft_override_active"] = True # Reentry implies override
            # Apply significant relaxation for reentry
            new_params["curvature_limit"] = self.default_modulation_params["curvature_limit"] + self.config.get("mod_reentry_curve_relax", math.pi / 6)
            new_params["shell_tolerance"] = self.default_modulation_params["shell_tolerance"] + self.config.get("mod_reentry_shell_relax", 2)
            new_params["distance_cap_factor"] = self.default_modulation_params["distance_cap_factor"] * self.config.get("mod_reentry_dist_relax_factor", 1.5)
            params_changed = True
            reentry_active_now = True
        elif current_params["reentry_mode"]: # If already in reentry, keep flags set
             reentry_active_now = True
             override_active_now = True # Reentry keeps override active

        # Check for moderate stalls -> trigger soft override (if not already in reentry)
        elif fwd_stalls >= stall_threshold and rev_stalls >= stall_threshold and not current_params["soft_override_active"]:
            print(f"CONTROLLER: Both builders stalled ({fwd_stalls}, {rev_stalls}). Activating soft override.")
            new_params["soft_override_active"] = True
            # Apply moderate relaxation
            new_params["curvature_limit"] = self.default_modulation_params["curvature_limit"] + self.config.get("mod_override_curve_relax", math.pi / 12)
            new_params["shell_tolerance"] = self.default_modulation_params["shell_tolerance"] + self.config.get("mod_override_shell_relax", 1)
            params_changed = True
            override_active_now = True
        elif current_params["soft_override_active"]: # If already in override, keep flag set
             override_active_now = True

        # Reset if overrides were active but no longer needed
        # Check if builders are running OR converged (implying stability)
        fwd_stable = fwd_status in ["running", "converged", "finished"]
        rev_stable = rev_status in ["running", "converged", "finished"]
        # Reset if BOTH are stable AND override/reentry was previously active
        if (current_params["soft_override_active"] or current_params["reentry_mode"]) and \
           fwd_stable and rev_stable:
             print("CONTROLLER: Builders stable. Deactivating overrides/reentry. Resetting params.")
             # Reset only the params that were changed by override/reentry
             reset_keys = ["soft_override_active", "reentry_mode", "curvature_limit", "shell_tolerance", "distance_cap_factor"]
             for key in reset_keys:
                 if key in self.default_modulation_params:
                     new_params[key] = self.default_modulation_params[key]
                 else: # Ensure flags are reset even if not in defaults
                     if key == "soft_override_active": new_params[key] = False
                     if key == "reentry_mode": new_params[key] = False
             # Ensure sparse unlock state is preserved based on progress
             new_params["allow_sparse_unlock"] = current_params["allow_sparse_unlock"]
             params_changed = True

        # Apply changes to the bus
        if params_changed:
            self.bus.update_modulation_params(new_params)

    def get_current_legality_params(self) -> Dict:
        """ Returns the currently active legality parameters from the bus. """
        return self.bus.modulation_params.copy()

    def process_salesman_feedback(self):
        """ Evaluates patch proposals from Salesman and stores accepted ones on bus. """
        proposals = self.bus.get_salesman_proposals()
        if not proposals: return
        print(f"CONTROLLER: Evaluating {len(proposals)} Salesman proposals.")
        accepted_patches = []
        for patch in proposals:
            # Evaluation logic: Accept if cost saving is positive and significant?
            # Needs more sophisticated evaluation (e.g., structural impact)
            cost_saving = patch.get('cost_saving', 0.0)
            if cost_saving > self.config.get("controller_patch_min_saving", 0.1): # Require min saving
                print(f"CONTROLLER: Accepting patch proposal for segment {patch.get('segment_indices')} (Save: {cost_saving:.2f})")
                self.bus.store_accepted_patch(patch) # Store on bus for builder
            # else: print(f"CONTROLLER: Rejecting patch proposal for segment {patch.get('segment_indices')} (Saving too small)")
        self.bus.clear_salesman_proposals() # Clear pending proposals

# ======================================
# === AGRM Agent: Path Builder (Dual) ===
# ======================================
# (PathBuilder class code as provided previously - verified complete)
class PathBuilder:
    """
    Builds a path segment (forward or reverse) using AGRM rules.
    Operates ephemerally, querying legality on demand.
    Interacts with Controller for modulation and feedback.
    Handles reentry logic when triggered.
    Can splice patches provided by Controller. Includes k-NN neighbor finding.
    """
    def __init__(self, builder_type: str, start_node: int, bus: AGRMStateBus, validator: AGRMEdgeValidator, config: Dict):
        self.builder_type = builder_type
        self.current_node = start_node
        self.bus = bus
        self.validator = validator
        self.config = config
        self.stalled_cycles = 0
        self.is_reentering = False
        # Initialize KDTree/BallTree for neighbor search if available
        self.neighbor_finder = None
        if HAS_SKLEARN and self.bus.num_nodes > 1:
            try:
                # Use BallTree as it can be better for non-uniform distributions
                self.neighbor_finder = BallTree(np.array(self.bus.cities))
                print(f"  Builder ({self.builder_type}): Initialized BallTree for neighbor search.")
            except Exception as e:
                print(f"  Builder ({self.builder_type}) WARNING: Failed to initialize BallTree: {e}. Falling back to linear scan.")
                self.neighbor_finder = None

    def step(self) -> bool:
        """ Performs one step of path construction. Returns True if progress was made. """
        state_key = "builder_fwd_state" if self.builder_type == 'forward' else "builder_rev_state"
        current_state = getattr(self.bus, state_key)
        if current_state["status"] in ["converged", "finished", "stalled_hard"]: return False

        # --- Candidate Selection ---
        k_neighbors = self.config.get("builder_knn_k", 50)
        k_neighbors = min(k_neighbors, self.bus.num_nodes - 1)
        potential_candidates = self._find_k_nearest_unvisited(k_neighbors)

        legal_candidates = [
            node for node in potential_candidates
            if self.validator.is_edge_legal(self.current_node, node, self.builder_type)
        ]

        # --- Decision & State Update ---
        next_node = None
        progress_made = False

        if legal_candidates:
            self.stalled_cycles = 0
            if self.is_reentering: # If we were reentering, mark as finished
                print(f"BUILDER ({self.builder_type}): Reentry successful, resuming normal modulation.")
                self.is_reentering = False
                # Signal controller implicitly via lack of stall
            self.bus.update_builder_state(self.builder_type, stalled=False) # Signal progress

            # Choose best candidate based on AGRM scoring (Sweep Rank)
            legal_candidates.sort(key=lambda n: self.bus.get_node_sweep_data(n).get('sweep_rank', float('inf')))
            next_node = legal_candidates[0]

        else: # Stalled
            self.stalled_cycles += 1
            self.bus.update_builder_state(self.builder_type, stalled=True)
            # print(f"BUILDER ({self.builder_type}): Stalled at node {self.current_node}, cycle {self.stalled_cycles}.")

            # Check if Controller activated reentry mode
            if self.bus.modulation_params.get("reentry_mode", False):
                if not self.is_reentering:
                    print(f"BUILDER ({self.builder_type}): Reentry mode active. Attempting inward move...")
                    self.is_reentering = True
                next_node = self._find_reentry_node()
                if not next_node:
                    print(f"BUILDER ({self.builder_type}): Reentry failed to find valid inward node. Hard stall likely.")
                    self.bus.update_builder_state(self.builder_type, status="stalled_hard")
            # Else: Normal stall, wait for controller action

        # --- Update Path ---
        if next_node is not None:
            self.bus.add_visited(next_node, self.builder_type)
            self.current_node = next_node
            progress_made = True
            # Ensure status is running if progress made
            if current_state["status"] != "running":
                self.bus.update_builder_state(self.builder_type, status="running")

        return progress_made

    def _find_k_nearest_unvisited(self, k: int) -> List[int]:
        """ Finds up to k nearest unvisited nodes using spatial index or fallback. """
        unvisited_nodes_set = self.bus.get_unvisited_nodes()
        if not unvisited_nodes_set: return []
        if k <= 0: return []

        current_pos = np.array([self.bus.cities[self.current_node]])

        if self.neighbor_finder:
            # Query tree for more neighbors than needed, then filter
            query_k = min(len(unvisited_nodes_set), k * 5, self.bus.num_nodes) # Query more initially
            try:
                 distances, indices = self.neighbor_finder.query(current_pos, k=query_k)
                 # indices[0] contains neighbor indices, distances[0] the distances
                 # Filter out self and already visited nodes
                 neighbors = []
                 for idx in indices[0]:
                     if idx != self.current_node and idx in unvisited_nodes_set:
                         neighbors.append(idx)
                         if len(neighbors) == k: break # Stop when we have enough
                 return neighbors
            except Exception as e:
                 print(f"  Builder ({self.builder_type}) WARNING: KDTree/BallTree query failed: {e}. Falling back.")
                 # Fallback to linear scan if tree query fails

        # Fallback: Linear scan over unvisited nodes
        distances = []
        for node_idx in unvisited_nodes_set:
            if node_idx == self.current_node: continue
            dist = math.dist(current_pos[0], self.bus.cities[node_idx])
            distances.append((dist, node_idx))
        distances.sort()
        return [node_idx for dist, node_idx in distances[:k]]


    def _find_reentry_node(self) -> Optional[int]:
         """ Finds a valid candidate node closer to the spiral center during reentry. """
         if not self.bus.center or not self.bus.sweep_data: return None
         current_data = self.bus.get_node_sweep_data(self.current_node)
         current_shell = current_data.get('shell', -1)
         if current_shell <= 0: return None # Already at center

         k_neighbors = self.config.get("builder_knn_k_reentry", 100)
         potential_candidates = self._find_k_nearest_unvisited(k_neighbors)

         valid_reentry_nodes = []
         for node in potential_candidates:
             # Check legality using RELAXED rules (validator uses current bus params)
             if self.validator.is_edge_legal(self.current_node, node, self.builder_type):
                 data_to = self.bus.get_node_sweep_data(node)
                 shell_to = data_to.get('shell', -1)
                 # Must move to an inner shell
                 if shell_to != -1 and shell_to < current_shell:
                     score = (current_shell - shell_to) # Prioritize larger drop
                     valid_reentry_nodes.append((score, node))

         if valid_reentry_nodes:
             valid_reentry_nodes.sort(reverse=True) # Best score (largest drop) first
             return valid_reentry_nodes[0][1]
         else:
             return None

    def splice_patch_if_instructed(self):
         """ Checks bus for accepted patches and splices them if applicable. """
         # This function is called by the main runner loop
         accepted_patches = self.bus.get_accepted_patches()
         if not accepted_patches: return

         # Process patches relevant to this builder? Or assume global path?
         # Assume patches apply to the final merged path managed by the bus
         spliced_any = False
         remaining_patches = []
         for patch in accepted_patches:
              # Check if patch applies to the portion built by this agent? Complex.
              # Simplification: Let the bus handle splicing on the final path.
              # This builder doesn't modify its history directly, relies on bus state.
              # If more sophisticated local splicing is needed, logic goes here.
              # For now, just acknowledge the concept.
              pass # Logic is handled in bus.splice_patch called by runner

         # If splicing happened locally, update self.current_node if needed
         # self.bus.clear_accepted_patches() # Runner should clear after processing

# ======================================
# === AGRM Agent: Salesman Validator ===
# ======================================
# (SalesmanValidator class code as provided previously - verified complete)
class SalesmanValidator:
    """
    Analyzes a completed path for inefficiencies (long jumps, curvature breaks).
    Generates AGRM-legal patch proposals for refinement via the Controller.
    Includes basic 2-opt check.
    """
    def __init__(self, bus: AGRMStateBus, validator: AGRMEdgeValidator, config: Dict):
        self.bus = bus
        self.validator = validator # Used to check legality of proposed patches
        self.config = config
        self.stats = {'flags_generated': 0, 'proposals_generated': 0}

    def run_validation_and_patching(self):
        """ Runs the post-path validation and patch generation cycle. """
        path = self.bus.full_path
        # Ensure path exists and is a closed loop for 2-opt checks
        if not path or len(path) < 4 or path[0] != path[-1]:
            print("SALESMAN: Path too short, not available, or not closed. Skipping validation.")
            return

        print(f"SALESMAN: Starting validation of path with {len(path)} steps...")
        self.stats['flags_generated'] = 0
        self.stats['proposals_generated'] = 0
        proposals = []

        max_len_factor = self.config.get("salesman_max_len_factor", 4.0)
        max_curve = self.config.get("salesman_max_curve", math.pi * 0.5) # 90 deg
        enable_2opt = self.config.get("salesman_enable_2opt", True)
        opt_threshold = self.config.get("salesman_2opt_threshold", 0.99) # Min 1% improvement

        # Use baseline legality params for checking proposed swaps
        validation_params = self.bus.default_modulation_params

        # Iterate through path segments for checks
        # Note: path includes return to start, so iterate up to len(path) - 2 for curvature/2-opt
        for i in range(len(path) - 1):
            p1 = path[i]
            p2 = path[i+1]
            pos1 = self.bus.cities[p1]
            pos2 = self.bus.cities[p2]
            dist12 = math.dist(pos1, pos2)

            # 1. Check Long Jumps
            shell1 = self.bus.get_node_sweep_data(p1).get('shell', 0)
            avg_dist_in_shell = max(1.0, self.bus.shell_width or 10.0)
            dist_threshold = avg_dist_in_shell * max_len_factor
            if dist12 > dist_threshold:
                self.stats['flags_generated'] += 1
                # print(f"SALESMAN FLAG (Long Jump): {p1}->{p2} (Dist: {dist12:.2f})")

            # 2. Check Sharp Turns (at p2 = path[i+1])
            if i < len(path) - 2:
                p_prev = path[i]     # p1
                p_curr = path[i+1]   # p2
                p_next = path[i+2]   # p3
                pos_prev, pos_curr, pos_next = self.bus.cities[p_prev], self.bus.cities[p_curr], self.bus.cities[p_next]
                vec1 = (pos_curr[0] - pos_prev[0], pos_curr[1] - pos_prev[1])
                vec2 = (pos_next[0] - pos_curr[0], pos_next[1] - pos_curr[1])
                len1, len2 = math.hypot(vec1[0], vec1[1]), math.hypot(vec2[0], vec2[1])
                if len1 > 1e-9 and len2 > 1e-9:
                    dot = vec1[0] * vec2[0] + vec1[1] * vec2[1]
                    cos_angle = max(-1.0, min(1.0, dot / (len1 * len2)))
                    angle = math.acos(cos_angle)
                    if angle > max_curve:
                        self.stats['flags_generated'] += 1
                        # print(f"SALESMAN FLAG (Sharp Turn): at {p_curr} (Angle: {math.degrees(angle):.1f})")

            # 3. Check for 2-Opt Improvements (Edges: p1->p2 and p3->p4)
            # We need i+3 to exist, and ensure we don't wrap around incorrectly
            if enable_2opt and i < len(path) - 3:
                 p3 = path[i+2]
                 p4 = path[i+3]
                 # Ensure p1 != p3 and p2 != p4 to avoid degenerate swaps
                 if p1 == p3 or p1 == p4 or p2 == p3 or p2 == p4: continue

                 pos3, pos4 = self.bus.cities[p3], self.bus.cities[p4]
                 current_dist = dist12 + math.dist(pos3, pos4)
                 swapped_dist = math.dist(pos1, pos3) + math.dist(pos2, p4)

                 if swapped_dist < current_dist * opt_threshold:
                     # Potential improvement. Check if new edges p1->p3 and p2->p4 are AGRM-legal
                     # Use the validator instance with baseline/validation params
                     # Pass 'final_check' or similar context if validator uses it
                     # Note: is_edge_legal needs access to params, pass them explicitly
                     if self.validator.is_edge_legal(p1, p3, 'final_check') and \
                        self.validator.is_edge_legal(p2, p4, 'final_check'):
                         self.stats['flags_generated'] += 1
                         self.stats['proposals_generated'] += 1
                         print(f"SALESMAN: Proposing 2-opt swap: ({p1},{p2}) & ({p3},{p4}) -> ({p1},{p3}) & ({p2},{p4}). Saving: {current_dist - swapped_dist:.2f}")
                         # Define the patch: replaces segment from index i+1 to i+2
                         # Original: [... p1, p2, p3, p4 ...]
                         # Swapped: [... p1, p3, p2, p4 ...]
                         # The segment between p1 and p4 needs reversal: [p3, p2] replaces [p2, p3]
                         proposal = {
                             'type': '2-opt',
                             'segment_indices': (i, i+3), # Indices covering p1 to p4
                             'original_nodes': [p1, p2, p3, p4],
                             # The new sequence for nodes BETWEEN index i and index i+3
                             # The path from i+1 up to (but not including) i+3 needs reversal
                             # Original segment is path[i+1 : i+3] = [p2, p3]
                             # New segment should be reversed: [p3, p2]
                             'new_subpath_nodes': path[i+1 : i+3][::-1], # Reverse the segment between swapped edges
                             'cost_saving': current_dist - swapped_dist
                         }
                         proposals.append(proposal)
                     # else: print(f"SALESMAN: Potential 2-opt swap rejected by AGRM legality.")

        print(f"SALESMAN: Validation complete. Found {self.stats['flags_generated']} flags. Generated {self.stats['proposals_generated']} patch proposals.")
        # Send valid proposals to the Controller via the bus
        if proposals:
            for p in proposals:
                self.bus.add_salesman_proposal(p)


# ==============================
# === Path Audit Agent (NEW) ===
# ==============================
# (PathAuditAgent class code as provided previously - verified complete)
class PathAuditAgent:
    """
    Runs AFTER the full AGRM + Salesman process is complete.
    Evaluates the final path quality using global metrics.
    Analyzes patterns of sub-optimality.
    Generates parameter adjustment recommendations for the NEXT run.
    Enables run-to-run meta-learning.
    """
    def __init__(self, bus: AGRMStateBus, config: Dict):
        self.bus = bus
        self.config = config
        self.metrics = {}
        self.patterns = {}
        self.recommendations = {}

    def run_audit(self) -> Dict:
        """ Performs the full audit process. Returns recommendations dict. """
        print("AUDIT AGENT: Starting post-run path audit...")
        if not self.bus.full_path or self.bus.current_phase not in ["merged", "finalizing", "complete", "patched"]: # Allow patched state
            print("AUDIT AGENT: Final path not available or run not complete. Skipping audit.")
            return {}

        self.metrics = self._calculate_global_metrics()
        self.patterns = self._analyze_patterns()
        self.recommendations = self._generate_recommendations()

        print("AUDIT AGENT: Audit complete.")
        print(f"  Audit Metrics: {self.metrics}")
        print(f"  Audit Patterns: {self.patterns}")
        print(f"  Audit Recommendations: {self.recommendations}")
        return self.recommendations

    def _calculate_global_metrics(self) -> Dict:
        """ Calculates high-level quality metrics for the final path. """
        metrics = {}
        path = self.bus.full_path
        # 1. Final Path Length
        final_cost = self.bus.calculate_total_path_cost(path) # Use bus helper
        metrics['final_path_cost'] = final_cost
        metrics['final_efficiency'] = final_cost / max(1, self.bus.num_nodes)

        # 2. Comparison to Baseline (e.g., simple Nearest Neighbor from start)
        baseline_cost = self._run_simple_nn_baseline()
        metrics['baseline_nn_cost'] = baseline_cost
        if baseline_cost > 0:
             metrics['length_vs_baseline'] = final_cost / baseline_cost

        # 3. Remaining Salesman Flags (Count reported by Salesman)
        # Need Salesman to store final flag count accessible here
        # metrics['remaining_salesman_flags'] = self.bus.salesman_final_flags?

        # 4. Structural Metrics (Example: Bounding Box Ratio)
        if path:
             coords = np.array([self.bus.cities[i] for i in path[:-1]]) # Exclude return to start
             min_x, min_y = np.min(coords, axis=0)
             max_x, max_y = np.max(coords, axis=0)
             width = max_x - min_x
             height = max_y - min_y
             metrics['bounding_box_ratio'] = width / height if height > 0 else 1.0

        # 5. Add more metrics: Avg turn angle, std dev of edge lengths, etc.
        return metrics

    def _run_simple_nn_baseline(self) -> float:
         """ Runs a basic Nearest Neighbor heuristic for baseline comparison. """
         if not self.bus.cities: return 0.0
         start_node = self.bus.start_node_fwd if self.bus.start_node_fwd is not None else 0
         unvisited = set(range(self.bus.num_nodes))
         current = start_node
         path = [current]
         unvisited.remove(current)
         total_dist = 0.0

         while unvisited:
             nearest_node = -1
             min_dist = float('inf')
             pos_current = self.bus.cities[current]
             for node in unvisited:
                 dist = math.dist(pos_current, self.bus.cities[node])
                 if dist < min_dist:
                     min_dist = dist
                     nearest_node = node
             if nearest_node != -1:
                 total_dist += min_dist
                 current = nearest_node
                 path.append(current)
                 unvisited.remove(current)
             else: break # Should not happen if graph is connected

         # Add return to start
         if len(path) > 1:
              total_dist += math.dist(self.bus.cities[current], self.bus.cities[start_node])
         return total_dist


    def _analyze_patterns(self) -> Dict:
        """ Analyzes patterns of sub-optimality in the final path. """
        patterns = {}
        # Example: Analyze where Salesman flags occurred (requires Salesman to store flag locations)
        # patterns['flag_concentration_quadrant'] = self._analyze_flag_distribution('quadrant')
        # patterns['flag_concentration_shell'] = self._analyze_flag_distribution('shell')
        # patterns['high_cost_segments'] = self._find_high_cost_segments()
        return patterns # Placeholder

    def _generate_recommendations(self) -> Dict:
        """ Generates parameter tuning recommendations based on metrics and patterns. """
        recommendations = {}
        # Example Rules:
        length_ratio = self.metrics.get('length_vs_baseline', 1.0)
        target_ratio = self.config.get("audit_target_baseline_ratio", 1.1) # e.g., aim for 10% worse than NN

        # If path is much longer than baseline, maybe legality was too strict?
        if length_ratio > target_ratio * 1.2: # If >20% worse than target
             # Suggest relaxing curvature or shell tolerance slightly
             recommendations['mod_curvature_limit'] = self.bus.default_modulation_params['curvature_limit'] * 1.1 # Relax by 10%
             recommendations['mod_shell_tolerance'] = self.bus.default_modulation_params['shell_tolerance'] + 1
             print("AUDIT Recommendation: Path long vs baseline. Suggest relaxing curvature/shell tolerance.")

        # If Salesman found many 2-opt opportunities (requires pattern analysis)
        # if self.patterns.get('high_2opt_flags'):
        #    recommendations['salesman_2opt_threshold'] = self.config['salesman_2opt_threshold'] * 1.01 # Make slightly easier to trigger
        #    print("AUDIT Recommendation: Many 2-opt flags. Suggest lowering 2-opt improvement threshold.")

        # Add more rules based on other metrics and patterns
        return recommendations


# ==============================
# === AGRM System Controller ===
# ==============================
# (AGRMRunner class code as provided previously - verified complete)
# Renamed to AGRMController for clarity
class AGRMController:
    """ Orchestrates the AGRM TSP solving process using the agent stack. """
    def __init__(self, cities: List[Tuple[float, float]], config: Dict = {}, previous_recommendations: Dict = {}):
        """
        Initializes the controller and all agents.
        Args:
            cities: List of city coordinates.
            config: Base configuration dictionary.
            previous_recommendations: Parameter adjustments from previous PathAuditAgent run.
        """
        self.cities = cities
        self.num_nodes = len(cities)

        # Apply recommendations to base config
        self.config = config.copy()
        if previous_recommendations:
            print(f"CONTROLLER: Applying {len(previous_recommendations)} recommendations from previous run.")
            self.config.update(previous_recommendations)
            print(f"  New Config Snippet: { {k: self.config[k] for k in previous_recommendations} }")

        # Initialize shared state bus with potentially updated config
        self.bus = AGRMStateBus(cities, self.config)

        # Initialize agents, passing the bus and config
        self.navigator = NavigatorGR(cities, self.config)
        self.validator = AGRMEdgeValidator(self.bus, self.config)
        # Pass self (controller) to agents that might need to signal back directly? Or use bus.
        self.mod_controller_agent = ModulationController(self.bus, self.config) # The agent managing modulation params
        self.builder_fwd: Optional[PathBuilder] = None
        self.builder_rev: Optional[PathBuilder] = None
        self.salesman = SalesmanValidator(self.bus, self.validator, self.config)
        self.path_audit = PathAuditAgent(self.bus, self.config) # Initialize audit agent

        self.run_stats = {}


    def solve(self) -> Tuple[Optional[List[int]], Dict]:
        """ Runs the full AGRM TSP solving process, returning path and stats. """
        print(f"\n=== AGRM Controller: Starting Solve for {self.num_nodes} Nodes ===")
        overall_start_time = time.time()
        self.run_stats = {} # Reset stats for this run

        # --- 1. Sweep Phase ---
        sweep_results = self.navigator.run_sweep()
        self.bus.update_sweep_data(sweep_results)
        if self.bus.start_node_fwd is None or self.bus.start_node_rev is None:
             print("CONTROLLER ERROR: Navigator failed to determine start nodes.")
             return None, {"error": "Navigator failed"}
        self.run_stats['sweep_time'] = time.time() - overall_start_time

        # --- Initialize Builders ---
        self.builder_fwd = PathBuilder('forward', self.bus.start_node_fwd, self.bus, self.validator, self.config)
        self.builder_rev = PathBuilder('reverse', self.bus.start_node_rev, self.bus, self.validator, self.config)

        # --- 2. Bidirectional Build Phase ---
        print("CONTROLLER: Starting Bidirectional Build Phase...")
        build_start_time = time.time()
        max_steps = self.num_nodes * self.config.get("runner_max_step_factor", 3)
        steps = 0
        build_complete = False
        while steps < max_steps:
            steps += 1
            # Alternate stepping? Or step both? Stepping both:
            progress_fwd = self.builder_fwd.step() if self.bus.builder_fwd_state["status"] in ["running", "stalled"] else False
            progress_rev = self.builder_rev.step() if self.bus.builder_rev_state["status"] in ["running", "stalled"] else False

            # Update modulation controller based on latest builder states
            self.mod_controller_agent.update_controller_state()

            # Check for convergence
            if self.bus.check_convergence():
                 print(f"CONTROLLER: Convergence detected at step {steps}.")
                 build_complete = self.bus.merge_paths()
                 break # Exit build loop

            # Check for hard stalls
            if self.bus.builder_fwd_state["status"] == "stalled_hard" and \
               self.bus.builder_rev_state["status"] == "stalled_hard":
                print("CONTROLLER ERROR: Both builders hard stalled. Attempting merge.")
                build_complete = self.bus.merge_paths() # Try merging anyway
                break

            # Check if no progress possible and not converged
            if not progress_fwd and not progress_rev and \
               self.bus.builder_fwd_state["status"] not in ["running", "converged"] and \
               self.bus.builder_rev_state["status"] not in ["running", "converged"]:
                 print(f"CONTROLLER WARNING: No progress from either builder at step {steps}. Stopping build.")
                 build_complete = self.bus.merge_paths() # Try merging what we have
                 break

        if steps >= max_steps:
             print(f"CONTROLLER WARNING: Max steps ({max_steps}) reached during build phase.")
             build_complete = self.bus.merge_paths() # Try merging what we have

        self.run_stats['build_time'] = time.time() - build_start_time
        print(f"CONTROLLER: Build phase finished in {self.run_stats['build_time']:.4f}s ({steps} steps).")

        # --- 3. Patching Phase (If Needed for Missed Nodes) ---
        if self.bus.current_phase == "patching":
            print("CONTROLLER: Entering patching phase for missed nodes...")
            patch_start_time = time.time()
            # TODO: Implement robust patching logic
            # Needs to find missed nodes, determine best insertion points respecting legality
            # This is complex - requires potentially re-invoking builder/validator locally
            # For now, just flag as incomplete
            print(f"CONTROLLER WARNING: Path construction incomplete, {self.num_nodes - len(set(self.bus.full_path))} nodes missed. Patching logic not fully implemented.")
            build_complete = False # Mark as incomplete
            self.run_stats['patch_time'] = time.time() - patch_start_time
            self.bus.current_phase = "finalizing" # Move phase even if incomplete

        # --- 4. Salesman Validation & Refinement ---
        if self.bus.full_path:
            print("CONTROLLER: Starting Salesman validation and refinement...")
            salesman_start_time = time.time()
            self.salesman.run_validation_and_patching()
            # Controller evaluates proposals and stores accepted ones
            self.mod_controller_agent.process_salesman_feedback()
            # Builder splices approved patches (via bus)
            accepted_patches = self.bus.get_accepted_patches()
            if accepted_patches:
                 print(f"CONTROLLER: Applying {len(accepted_patches)} accepted patches...")
                 spliced_count = 0
                 # Apply patches iteratively? Or assume they don't overlap significantly?
                 # Applying iteratively for now
                 for patch in accepted_patches:
                     if self.bus.splice_patch(patch):
                         spliced_count += 1
                 print(f"CONTROLLER: Successfully spliced {spliced_count} patches.")
                 self.bus.clear_accepted_patches() # Clear after applying
            self.run_stats['salesman_time'] = time.time() - salesman_start_time
            print(f"CONTROLLER: Salesman phase complete in {self.run_stats['salesman_time']:.4f}s.")
        else:
            print("CONTROLLER: No path generated, skipping Salesman validation.")
            self.run_stats['salesman_time'] = 0.0

        # --- 5. Path Audit Phase (Meta-Learning) ---
        print("CONTROLLER: Starting Path Audit...")
        audit_start_time = time.time()
        recommendations = self.path_audit.run_audit() # Returns dict of param adjustments
        self.run_stats['audit_time'] = time.time() - audit_start_time
        print(f"CONTROLLER: Path Audit complete in {self.run_stats['audit_time']:.4f}s.")

        # --- 6. Final Results ---
        overall_end_time = time.time()
        final_path = self.bus.full_path
        # Recalculate final cost after potential splicing
        final_cost = self.calculate_total_path_cost(final_path) if final_path else 0.0

        # Compile final statistics
        final_stats = {
            "execution_time": overall_end_time - overall_start_time,
            "sweep_time": self.run_stats.get('sweep_time', 0.0),
            "build_time": self.run_stats.get('build_time', 0.0),
            "patch_time": self.run_stats.get('patch_time', 0.0),
            "salesman_time": self.run_stats.get('salesman_time', 0.0),
            "audit_time": self.run_stats.get('audit_time', 0.0),
            "path_complete": build_complete and bool(final_path) and len(set(final_path[:-1])) == self.num_nodes,
            "visited_nodes": len(set(final_path[:-1])) if final_path else 0,
            "path_length_nodes": len(final_path) if final_path else 0,
            "salesman_flags": self.salesman.stats.get('flags_generated', 0),
            "salesman_proposals": self.salesman.stats.get('proposals_generated', 0),
            "total_path_cost": final_cost,
            "efficiency": final_cost / max(1, self.num_nodes),
            "audit_recommendations": recommendations # Include recommendations for next run
        }
        self.bus.current_phase = "complete"
        print(f"=== AGRM Controller: Solve Complete in {final_stats['execution_time']:.4f}s ===")
        return final_path, final_stats

    def calculate_total_path_cost(self, path: Optional[List[int]]) -> float:
        """ Calculates the Euclidean distance of the TSP path. """
        if not path or len(path) < 2: return 0.0
        total_distance = 0.0
        for i in range(len(path) - 1):
            # Add checks for valid indices
            idx1, idx2 = path[i], path[i+1]
            if 0 <= idx1 < self.num_nodes and 0 <= idx2 < self.num_nodes:
                 pos1 = self.cities[idx1]
                 pos2 = self.cities[idx2]
                 total_distance += math.dist(pos1, pos2)
            else:
                 print(f"ERROR: Invalid node index in path during cost calculation: {idx1} or {idx2}")
                 return 0.0 # Indicate error
        return total_distance

# =========================
# === Example Usage ===
# =========================
if __name__ == "__main__":
    # Generate sample cities for testing
    NUM_CITIES = 100 # Small test for demonstration
    cities_data = [(random.uniform(0, 100), random.uniform(0, 100)) for _ in range(NUM_CITIES)]

    # --- Configuration ---
    # Define base configuration
    config = {
        "sweep_num_shells": 10, "sweep_num_sectors": 8, "density_knn_k": 10,
        "density_dense_factor": 1.5, "density_sparse_factor": 0.5, "density_std_dev_factor": 0.75,
        "hybrid_hash_threshold": 5, "mdhg_dimensions": 3, "mdhg_capacity_factor": 1.2, # Factor of num_nodes
        "mod_shell_tolerance": 2, "mod_curvature_limit": math.pi / 4, "mod_sector_tolerance": 2,
        "mod_dist_cap_factor": 3.0, "mod_reentry_curve_relax": math.pi / 6,
        "mod_reentry_shell_relax": 2, "mod_reentry_dist_relax_factor": 1.5,
        "mod_override_curve_relax": math.pi / 12, "mod_override_shell_relax": 1,
        "dist_cap_sparse_mult": 1.5, "dist_cap_override_mult": 1.5,
        "midpoint_unlock_percent": 0.5, "controller_stall_threshold": 5, "controller_severe_stall_factor": 2,
        "convergence_shell_threshold": 1, "convergence_sector_threshold": 1, "convergence_stall_dist_factor": 5.0,
        "builder_knn_k": 50, "builder_knn_k_reentry": 100,
        "salesman_max_len_factor": 4.0, "salesman_max_curve": math.pi * 0.5,
        "salesman_enable_2opt": True, "salesman_2opt_threshold": 0.98,
        "runner_max_step_factor": 3,
        "audit_target_baseline_ratio": 1.1, "controller_patch_min_saving": 0.1,
        # MDHG Specific Config (passed to MDHGHashTable)
        "mdhg_access_history_len": 100, "mdhg_velocity_promo_threshold": 10,
        "mdhg_ops_thresh_minor": 100, "mdhg_time_thresh_minor": 1.0,
        "mdhg_ops_thresh_major": 1000, "mdhg_time_thresh_major": 5.0,
        "mdhg_hot_key_count": 100, "mdhg_hot_key_min_freq": 5,
        "mdhg_cluster_threshold": 3, "mdhg_path_cache_max_size": 100,
        "mdhg_max_put_probes": 20, "mdhg_max_search_probes": 20,
        "mdhg_path_length_limit": 1000
    }

    print(f"--- Starting AGRM TSP Solver for {NUM_CITIES} cities ---")
    # --- Run 1 ---
    print("\n--- Run 1 ---")
    solver1 = AGRMController(cities_data, config)
    final_path1, final_stats1 = solver1.solve()

    print("\n--- AGRM Run 1 Results ---")
    print(f"Execution Time: {final_stats1.get('execution_time', 0.0):.4f} seconds")
    print(f"Path Complete: {final_stats1.get('path_complete', False)}")
    print(f"Nodes Visited: {final_stats1.get('visited_nodes', 0)} / {NUM_CITIES}")
    print(f"Total Path Cost: {final_stats1.get('total_path_cost', 0.0):.4f}")
    print(f"Efficiency (Cost/Node): {final_stats1.get('efficiency', 0.0):.4f}")
    print(f"Salesman Flags: {final_stats1.get('salesman_flags', 0)}")
    print(f"Audit Recommendations: {final_stats1.get('audit_recommendations', {})}")

    # --- Run 2 (Using Recommendations from Run 1) ---
    print("\n--- Run 2 (Applying Audit Recommendations) ---")
    recommendations1 = final_stats1.get('audit_recommendations', {})
    solver2 = AGRMController(cities_data, config, previous_recommendations=recommendations1)
    final_path2, final_stats2 = solver2.solve()

    print("\n--- AGRM Run 2 Results ---")
    print(f"Execution Time: {final_stats2.get('execution_time', 0.0):.4f} seconds")
    print(f"Path Complete: {final_stats2.get('path_complete', False)}")
    print(f"Nodes Visited: {final_stats2.get('visited_nodes', 0)} / {NUM_CITIES}")
    print(f"Total Path Cost: {final_stats2.get('total_path_cost', 0.0):.4f}")
    print(f"Efficiency (Cost/Node): {final_stats2.get('efficiency', 0.0):.4f}")
    print(f"Salesman Flags: {final_stats2.get('salesman_flags', 0)}")
    print(f"Audit Recommendations: {final_stats2.get('audit_recommendations', {})}")

    # --- Comparison ---
    print("\n--- Run Comparison ---")
    cost1 = final_stats1.get('total_path_cost', float('inf'))
    cost2 = final_stats2.get('total_path_cost', float('inf'))
    time1 = final_stats1.get('execution_time', 0.0)
    time2 = final_stats2.get('execution_time', 0.0)
    print(f"Run 1 Cost: {cost1:.4f} ({time1:.4f}s)")
    print(f"Run 2 Cost: {cost2:.4f} ({time2:.4f}s)")
    if cost1 > 0:
         print(f"Cost Improvement: {(cost1 - cost2) / cost1 * 100:.2f}%")


```

### src/unified/embedder/cand_9_analysis_scripts_final_py.py

```python
import itertools
import math
import networkx as nx
from collections import deque, defaultdict
import heapq
import random  #Needed for Hypothetical Prodigal Generation


def calculate_overlap(s1: str, s2: str) -> int:
    """Calculates the maximum overlap between two strings.

    Args:
        s1 (str): The first string.
        s2 (str): The second string.

    Returns:
        int: The length of the maximum overlap. Returns 0 if there is no overlap.
    """
    max_overlap = 0
    for i in range(1, min(len(s1), len(s2)) + 1):
        if s1[-i:] == s2[:i]:
            max_overlap = i
    return max_overlap


def hash_permutation(permutation: tuple) -> int:
    """Hashes a permutation tuple to a unique integer.

    Args:
        permutation: The permutation tuple (e.g., (1, 2, 3, 4, 5, 6, 7, 8)).

    Returns:
        A unique integer hash value.
    """
    result = 0
    n = len(permutation)
    for i, val in enumerate(permutation):
        result += val * (n ** (n - 1 - i))
    return result


def unhash_permutation(hash_value: int, n: int) -> tuple:
    """Converts a hash value back to a permutation tuple.

    Args:
        hash_value: The integer hash value.
        n: The value of n.

    Returns:
        The corresponding permutation tuple.
    """
    permutation = []
    for i in range(n - 1, -1, -1):
        val = hash_value // (n ** i)
        permutation.append(val + 1)  # Adjust to be 1-indexed
        hash_value -= val * (n ** i)
    return tuple(permutation)


def is_valid_permutation(perm: tuple, n: int) -> bool:
    """Checks if a given sequence is a valid permutation.

    Args:
        perm: The sequence (tuple of integers).
        n: The value of n.

    Returns:
        True if the sequence is a valid permutation, False otherwise.
    """
    return len(set(perm)) == n and min(perm) == 1 and max(perm) == n


def get_kmers(sequence: str, n: int, k: int) -> set[str]:
    """Extracts all k-mers from a sequence of digits, ensuring they form valid permutations.

    Args:
        sequence: The input sequence (string of digits).
        n: The value of n.
        k: The length of the k-mers to extract.

    Returns:
        A set of k-mer strings.
    """
    kmers = set()
    seq_list = [int(x) for x in sequence]  # Ensure sequence is treated as digits
    for i in range(len(sequence) - n + 1):
        perm = tuple(seq_list[i:i + n])
        if is_valid_permutation(perm, n):
            if i >= k:
                kmer = "".join(str(x) for x in seq_list[i - k:i])
                kmers.add(kmer)
    return kmers


def calculate_golden_ratio_points(length: int, levels: int = 1) -> list[int]:
    """Calculates multiple levels of golden ratio points within a sequence.

    Args:
        length (int): The total length of the sequence.
        levels (int): The number of recursive divisions to perform.

    Returns:
        list: A sorted list of unique golden ratio points (integers).
    """
    phi = (1 + math.sqrt(5)) / 2
    points = []
    for _ in range(levels):
        new_points = []
        if not points:
            new_points = [int(length / phi), int(length - length / phi)]
        else:
            for p in points:
                new_points.extend([int(p / phi), int(p - p / phi)])
                new_points.extend([int((length - p) / phi) + p, int(length - (length - p) / phi)])
        points.extend(new_points)
        points = sorted(list(set(points)))  # Remove duplicates and sort
    return points


def calculate_fixed_segments(length: int, segment_size: int, overlap_size: int) -> list[tuple[int, int]]:
    """Calculates start and end indices for fixed-size segments with overlap.

    Args:
        length (int): The total length of the sequence.
        segment_size (int): The desired size of each segment.
        overlap_size (int): The desired overlap between adjacent segments.

    Returns:
        list: A list of tuples, where each tuple contains the (start, end) indices of a segment.
    """
    segments = []
    start = 0
    while start < length:
        end = min(start + segment_size, length)
        segments.append((start, end))
        start += segment_size - overlap_size
    return segments

def generate_permutations(n: int) -> list[tuple[int, ...]]:
    """Generates all permutations of 1 to n.  Used for creating complete sets.

    Args:
        n (int): The number of symbols.

    Returns:
        list[tuple[int, ...]]: A list of all permutations as tuples.
    """
    return list(itertools.permutations(range(1, n + 1)))

# --- Superpermutation Analysis ---

def analyze_superpermutation(superpermutation: str, n: int) -> dict:
    """Analyzes a given superpermutation and provides statistics.

    Args:
        superpermutation (str): The superpermutation string.
        n (int): The value of n (number of symbols).

    Returns:
        dict: A dictionary containing:
            - length: The length of the superpermutation.
            - validity: True if valid, False otherwise.
            - missing_permutations: A list of missing permutations (empty if valid).
            - overlap_distribution: A dictionary showing counts of each overlap length.
            - average_overlap: The average overlap between consecutive permutations.
    """
    permutations = set(itertools.permutations(range(1, n + 1)))
    s_tuple = tuple(int(x) for x in superpermutation)
    found_permutations = set()
    missing_permutations = []
    overlap_distribution = {}

    total_overlap = 0

    for i in range(len(s_tuple) - n + 1):
        perm = s_tuple[i:i+n]
        if is_valid_permutation(perm, n):  # Valid
            found_permutations.add(tuple(perm))  # Use tuples for set
            if i > 0:
                overlap = calculate_overlap("".join(str(x) for x in s_tuple[i-n:i]), "".join(str(x) for x in perm))
                total_overlap += overlap
                overlap_distribution[overlap] = overlap_distribution.get(overlap, 0) + 1

    missing_permutations = list(permutations - found_permutations)
    validity = len(missing_permutations) == 0
    average_overlap = total_overlap / (len(found_permutations) - 1) if len(found_permutations) > 1 else 0

    return {
        "length": len(superpermutation),
        "validity": validity,
        "missing_permutations": missing_permutations,
        "overlap_distribution": overlap_distribution,
        "average_overlap": average_overlap,
    }

# --- Prodigal Result Identification ---

def is_prodigal(sequence: str, permutations_in_sequence: list[tuple[int]], n: int, min_length: int, overlap_threshold: float) -> bool:
    """Checks if a sequence is a 'Prodigal Result'.

    Args:
        sequence (str): The sequence of digits to check.
        permutations_in_sequence (list):  A list of *tuples* representing the
                                         permutations contained within the sequence.
        n (int): The value of n.
        min_length (int): The minimum number of permutations for a "Prodigal Result."
        overlap_threshold (float): The minimum overlap rate (0 to 1).

    Returns:
        bool: True if the sequence is a "Prodigal Result," False otherwise.
    """
    if len(permutations_in_sequence) < min_length:
        return False

    total_length = sum(len(str(p)) for p in permutations_in_sequence)
    overlap_length = total_length - len(sequence)
    max_possible_overlap = (len(permutations_in_sequence) - 1) * (n - 1)

    if max_possible_overlap == 0:
        return False
    return (overlap_length / max_possible_overlap) >= overlap_threshold

def find_prodigal_results(superpermutation: str, n: int, min_length: int = None, overlap_threshold: float = 0.98) -> list[str]:
    """Identifies and returns a list of prodigal results within a given superpermutation.

    Args:
        superpermutation (str): The superpermutation string.
        n (int): The value of n.
        min_length (int): Minimum number of permutations in a prodigal result.
                          Defaults to n-1.  This can be dynamically adjusted.
        overlap_threshold (float): Minimum overlap percentage. Defaults to 0.98,
                                   but can be dynamically adjusted.

    Returns:
        list: A list of "Prodigal Result" sequences (strings).
    """
    prodigal_results = []
    superperm_list = [int(x) for x in superpermutation]  # Convert to list of integers
    if min_length is None:
        min_length = n-1

    for length in range(n * min_length, len(superpermutation) + 1):  # Iterate through possible lengths
        for i in range(len(superpermutation) - length + 1):  # Iterate through starting positions
            subsequence = tuple(superperm_list[i:i+length])  # Get the subsequence as a tuple
            permutations_in_subsequence = set()
            for j in range(len(subsequence) - n + 1):
                perm = subsequence[j:j+n]
                if is_valid_permutation(perm, n):  # Valid permutation
                    permutations_in_subsequence.add(tuple(perm))  # Use tuples for set membership

            if is_prodigal(subsequence, list(permutations_in_subsequence), n, min_length, overlap_threshold):
                prodigal_results.append("".join(str(x) for x in subsequence))  # Store as string
    return prodigal_results

    # --- Winner/Loser Calculation ---

def calculate_winners_losers(superpermutations: list[str], n: int, k: int) -> tuple[dict, dict]:
    """Calculates "Winner" and "Loser" k-mer weights from a list of superpermutations.

    Args:
        superpermutations (list): A list of superpermutation strings.
        n (int): The value of n.
        k (int): The length of the k-mers to analyze.

    Returns:
        tuple: (winners, losers), where:
            - winners: A dictionary of {kmer: weight} for winning k-mers.
            - losers: A dictionary of {kmer: weight} for losing k-mers.
    """

    all_kmers = {}  # {kmer: total_count}
    for superpermutation in superpermutations:
        s_tuple = tuple(int(x) for x in superpermutation)
        for i in range(len(s_tuple) - n + 1):
            perm = s_tuple[i:i + n]
            if is_valid_permutation(perm, n):  # Valid
                if i > 0:
                    kmer = "".join(str(x) for x in s_tuple[i - k:i])
                    all_kmers[kmer] = all_kmers.get(kmer, 0) + 1

    # Divide into "shorter" and "longer" groups based on median length
    lengths = [len(s) for s in superpermutations]
    lengths.sort()
    median_length = lengths[len(lengths) // 2]
    shorter_superpermutations = [s for s in superpermutations if len(s) <= median_length]
    longer_superpermutations = [s for s in superpermutations if len(s) > median_length]

    winners = {}  # {kmer: weight}
    losers = {}  # {kmer: weight}

    shorter_counts = {}  # {kmer: count_in_shorter}
    for superpermutation in shorter_superpermutations:
        s_tuple = tuple(int(x) for x in superpermutation)
        for i in range(len(s_tuple) - n + 1):
            perm = s_tuple[i:i + n]
            if is_valid_permutation(perm, n):
                if i > 0:
                    kmer = "".join(str(x) for x in s_tuple[i - k:i])
                    shorter_counts[kmer] = shorter_counts.get(kmer, 0) + 1

    longer_counts = {}  # {kmer: count_in_longer}
    for superpermutation in longer_superpermutations:
        s_tuple = tuple(int(x) for x in superpermutation)
        for i in range(len(s_tuple) - n + 1):
            perm = s_tuple[i:i + n]
            if is_valid_permutation(perm, n):
                if i > 0:
                    kmer = "".join(str(x) for x in s_tuple[i - k:i])
                    longer_counts[kmer] = longer_counts.get(kmer, 0) + 1

    # Calculate weights based on the difference in counts
    for kmer in all_kmers:
        score = shorter_counts.get(kmer, 0) - longer_counts.get(kmer, 0)
        if score > 0:
            winners[kmer] = score
        elif score < 0:
            losers[kmer] = -score  # Store as positive weights

    return winners, losers


def calculate_sequence_winners_losers(superpermutations: list[str], n: int, sequence_length: int = 2) -> tuple[dict, dict]:
    """Calculates 'Winner' and 'Loser' weights for sequences of permutations.

    Args:
        superpermutations (list[str]): List of superpermutation strings.
        n (int): The value of n.
        sequence_length (int): The length of the permutation sequences to analyze (default: 2).

    Returns:
        tuple: (winners, losers), where winners and losers are dictionaries
               mapping sequence hashes (integers) to weights.
    """
    all_sequences = {} # {seq_hash : count}

    for superperm in superpermutations:
        s_tuple = tuple(int(x) for x in superperm)
        perms = []
        for i in range(len(s_tuple) - n + 1):
            perm = s_tuple[i:i+n]
            if is_valid_permutation(perm, n):
                perms.append(hash_permutation(perm))  # Append hash

        for i in range(len(perms) - (sequence_length -1)):
            seq = tuple(perms[i:i+sequence_length])
            seq_hash = hash(seq) #Hash the sequence of hashes.
            all_sequences[seq_hash] = all_sequences.get(seq_hash, 0) + 1

    #Divide into "shorter" and "longer"
    lengths = [len(s) for s in superpermutations]
    lengths.sort()
    median_length = lengths[len(lengths)//2]
    shorter_superpermutations = [s for s in superpermutations if len(s) <= median_length]
    longer_superpermutations = [s for s in superpermutations if len(s) > median_length]

    winners = {}
    losers = {}
    shorter_counts = {}
    for superperm in shorter_superpermutations:
        s_tuple = tuple(int(x) for x in superperm)
        perms = []
        for i in range(len(s_tuple) - n + 1):
            perm = s_tuple[i:i+n]
            if is_valid_permutation(perm, n):
                perms.append(hash_permutation(perm)) #Append hash
        for i in range(len(perms) - (sequence_length - 1)):
            seq = tuple(perms[i:i+sequence_length])
            seq_hash = hash(seq)
            shorter_counts[seq_hash] = shorter_counts.get(seq_hash, 0) + 1

    longer_counts = {}
    for superperm in longer_superpermutations:
        s_tuple = tuple(int(x) for x in superperm)
        perms = []
        for i in range(len(s_tuple) - n + 1):
            perm = s_tuple[i:i+n]
            if is_valid_permutation(perm, n):
                perms.append(hash_permutation(perm))
        for i in range(len(perms) - (sequence_length - 1)):
            seq = tuple(perms[i:i+sequence_length])
            seq_hash = hash(seq)
            longer_counts[seq_hash] = longer_counts.get(seq_hash, 0) + 1

    for seq_hash in all_sequences:
        score = shorter_counts.get(seq_hash, 0) - longer_counts.get(seq_hash, 0)
        if score > 0:
            winners[seq_hash] = score
        if score < 0:
            losers[seq_hash] = -score

    return winners, losers

# --- Anti-Prodigal Identification ---
def identify_anti_prodigals(superpermutations, n, k, overlap_threshold):
    """Identifies and returns a list of 'anti-prodigal' k-mers.

    Args:
        superpermutations (list): A list of superpermutation strings.
        n (int): The value of n.
        k (int): The length of k-mers.
        overlap_threshold (float): The *maximum* overlap allowed for an anti-prodigal

    Returns:
        set: A set of 'anti-prodigal' k-mers (strings).
    """
    anti_prodigals = set()
    for superpermutation in superpermutations:
        s_tuple = tuple(int(x) for x in superpermutation)
        for i in range(len(s_tuple) - n + 1):
            perm = s_tuple[i:i + n]
            if is_valid_permutation(perm, n):  # Valid
                if i > k:
                    kmer = "".join(str(x) for x in s_tuple[i-k:i])
                    overlap = calculate_overlap("".join(str(x) for x in s_tuple[i-k:i]), "".join(str(x) for x in perm))
                    if overlap < ( (n-1) * overlap_threshold):  #  Condition for Anti-Prodigal
                        anti_prodigals.add(kmer)
    return anti_prodigals
    
    # --- De Bruijn Graph Functions ---

def build_debruijn_graph(n: int, k: int, permutations=None, superpermutation=None) -> nx.DiGraph:
    """Builds a De Bruijn graph of order k for permutations of n symbols.

    Args:
        n: The number of symbols (e.g., 8 for n=8).
        k: The order of the De Bruijn graph (k-mers).
        permutations: (Optional) A list of permutation tuples.  If provided,
                      the graph is built ONLY from these permutations.
        superpermutation: (Optional) A superpermutation string. If provided,
                          the graph is built from the k-mers in this string.
                          Must provide one, and only one, of the two.

    Returns:
        A networkx.DiGraph representing the De Bruijn graph.
    """

    if (permutations is None and superpermutation is None) or (permutations is not None and superpermutation is not None):
        raise ValueError("Must provide either 'permutations' or 'superpermutation', but not both.")

    graph = nx.DiGraph()

    if permutations:
      for perm in permutations:
          perm_str = "".join(str(x) for x in perm)
          for i in range(len(perm_str) - k + 1):
              kmer1 = perm_str[i:i + k - 1]
              kmer2 = perm_str[i + 1:i + k]
              if len(kmer1) == k - 1 and len(kmer2) == k-1: #Should always be true
                graph.add_edge(kmer1, kmer2, weight=calculate_overlap(kmer1, kmer2))
    else: #Use superpermutation
        s_list = [int(x) for x in superpermutation]
        for i in range(len(s_list) - n + 1):
            perm = tuple(s_list[i:i+n])
            if is_valid_permutation(perm, n):
                #Valid permutation.
                if i >= k:
                  kmer1 = "".join(str(x) for x in s_list[i-k:i])
                  kmer2 = "".join(str(x) for x in s_list[i-k+1: i+1])
                  if len(kmer1) == k - 1 and len(kmer2) == k - 1:
                    graph.add_edge(kmer1, kmer2, weight = calculate_overlap(kmer1,kmer2))
    return graph

def add_weights_to_debruijn(graph: nx.DiGraph, winners: dict, losers: dict):
    """Adds 'winner_weight' and 'loser_weight' attributes to the edges of a De Bruijn graph.

    Args:
        graph: The De Bruijn graph (networkx.DiGraph).
        winners: A dictionary of Winner k-mers and their weights.
        losers: A dictionary of Loser k-mers and their weights.
    """

    for u, v, data in graph.edges(data=True):
        kmer = u[1:] + v[-1]  # Reconstruct the k-mer from the edge
        data['winner_weight'] = winners.get(kmer, 0)
        data['loser_weight'] = losers.get(kmer, 0)

def find_cycles(graph: nx.DiGraph) -> list[list[str]]:
    """Finds cycles in the De Bruijn graph using a simple DFS approach."""
    cycles = []
    visited = set()

    def dfs(node, path):
        visited.add(node)
        path.append(node)

        for neighbor in graph.neighbors(node):
            if neighbor == path[0] and len(path) > 1:  # Found a cycle
                cycles.append(path.copy())
            elif neighbor not in visited:
                dfs(neighbor, path)

        path.pop()
        visited.remove(node) #Correctly remove node

    for node in graph.nodes:
        if node not in visited:
            dfs(node, [])
    return cycles

def find_high_weight_paths(graph: nx.DiGraph, start_node: str, length_limit: int) -> list[list[str]]:
    """
    Finds high-weight paths in the De Bruijn graph starting from a given node.
    """
    best_paths = []
    best_score = -float('inf')

    def dfs(current_node, current_path, current_score):
        nonlocal best_score
        nonlocal best_paths

        if len(current_path) > length_limit:
            return

        current_score_adj = current_score
        if len(current_path) > 1:
            current_score_adj = current_score / (len(current_path)-1) #Average

        if current_score_adj > best_score:
            best_score = current_score_adj
            best_paths = [current_path.copy()]  # New best path
        elif current_score_adj == best_score and len(current_path) > 1: #Dont include starting nodes
            best_paths.append(current_path.copy())  # Add to list of best paths

        for neighbor in graph.neighbors(current_node):
            edge_data = graph.get_edge_data(current_node, neighbor)
            new_score = current_score + edge_data.get('weight', 0) + edge_data.get('winner_weight', 0) - edge_data.get('loser_weight', 0)
            dfs(neighbor, current_path + [neighbor], new_score)

    dfs(start_node, [start_node], 0)
    return best_paths

# --- Hypothetical Prodigal Generation ---
def generate_permutations_on_demand_hypothetical(current_sequence: str, kmer: str, n: int, k: int) -> set[int]:
    """Generates candidate permutations for hypothetical prodigals. More restrictive.

    Args:
        current_sequence (str): The sequence currently being built
        kmer (str): The k-mer to extend (either prefix or suffix).
        n (int): value of n
        k (int): The k value

    Returns:
        set[int]: A set of permutation *hashes*.
    """
    valid_permutations = set()
    all_perms = generate_permutations(n)  # Get *all* permutations
    for perm in all_perms:
        perm_str = "".join(str(x) for x in perm)
        if kmer == perm_str[:k] or kmer == perm_str[-k:]:
           valid_permutations.add(hash_permutation(perm))

    # We do NOT filter here, as these are for hypotheticals.
    return valid_permutations

def generate_mega_hypotheticals(prodigal_results: dict, winners: dict, losers: dict, n: int, num_to_generate: int = 20, min_length: int = 50, max_length: int = 200) -> dict:
    """Generates 'Mega-Hypothetical' Prodigals by combining existing Prodigals.

    Args:
        prodigal_results (dict): Dictionary of existing ProdigalResult objects.
        winners (dict): "Winner" k-mer data.  Used for scoring connections.
        losers (dict): "Loser" k-mer data. Used for filtering.
        n (int): The value of n.
        num_to_generate (int): The number of Mega-Hypotheticals to generate.
        min_length (int): Minimum length (in permutations).
        max_length (int): Maximum length (in permutations).

    Returns:
        dict: A dictionary of new 'Mega-Hypothetical' ProdigalResult objects.
    """
    mega_hypotheticals = {}
    next_mega_id = -1  # Use negative IDs to distinguish

    for _ in range(num_to_generate):
        num_prodigals_to_combine = random.randint(2, 4)  # Combine 2-4 Prodigals
        #Select prodigals, but with a bias towards those with higher overlap and greater length
        prodigal_weights = [p.overlap_rate * p.length for p in prodigal_results.values()]
        selected_prodigal_ids = random.choices(list(prodigal_results.keys()), weights=prodigal_weights, k=num_prodigals_to_combine)
        selected_prodigals = [prodigal_results[pid] for pid in selected_prodigal_ids]

        # Sort by length, in any order.
        selected_prodigals.sort(key=lambda p: len(p.sequence))

        #Use golden ratio for lengths.
        phi = (1 + math.sqrt(5)) / 2
        total_target_length = random.randint(n * min_length, n* max_length)
        target_lengths = []

        remaining_length = total_target_length
        for i in range(len(selected_prodigals)-1):
            segment_length = int(remaining_length / (phi**(i+1)))
            target_lengths.append(segment_length)
            remaining_length -= segment_length
        target_lengths.append(remaining_length) # Add the final length.

        combined_sequence = ""
        
        #Randomize starting location.
        start_location = random.randint(0,len(selected_prodigals)-1)
        prodigal_order = []
        for i in range(len(selected_prodigals)):
            prodigal_order.append(selected_prodigals[(start_location + i) % len(selected_prodigals)])

        for i in range(len(prodigal_order)):
            prodigal = prodigal_order[i]
            target_length = target_lengths[i]

            # Get a random section of the prodigal, unless it is too short.
            start_index = random.randint(0, max(0, len(prodigal.sequence) - target_length))  # Ensure valid start
            current_sequence = prodigal.sequence[start_index : start_index + target_length]

            # Connect to previous
            if combined_sequence != "":
                overlap = calculate_overlap(combined_sequence, current_sequence)
                if overlap == 0: #Need to use the combiner
                    prefix = combined_sequence[-(n-1):]
                    suffix = current_sequence[:n-1]
                    candidates = generate_permutations_on_demand_hypothetical(prefix, suffix,  n, n-1)
                    if candidates:
                        # Choose the best candidate based on winners/losers (simplified scoring)
                        best_candidate = None
                        best_score = -float('inf')
                        for cand_hash in candidates:
                            cand_perm = unhash_permutation(cand_hash, n)
                            cand_str = "".join(str(x) for x in cand_perm)
                            score = 0
                            for k in [7, 6]: # Using values for n=8
                                for j in range(len(cand_str) - k + 1):
                                    kmer = cand_str[j:j+k]
                                    score += winners.get(kmer, 0)
                                    score -= losers.get(kmer, 0)

                            if score > best_score:
                                best_score = score
                                best_candidate = cand_str

                        overlap = calculate_overlap(combined_sequence, best_candidate)
                        combined_sequence += best_candidate[overlap:]
                    else:
                        continue #Skip if we cannot connect.

                else: #Overlap exists
                    combined_sequence += current_sequence[overlap:]
            else:
                combined_sequence = current_sequence
        
        #Check if prodigal
        perms_in_sequence = set()
        for i in range(len(combined_sequence) - n + 1):
            perm = tuple(int(x) for x in combined_sequence[i:i+n])
            if is_valid_permutation(perm, n):
                perms_in_sequence.add(hash_permutation(perm))

        if is_prodigal(combined_sequence, [unhash_permutation(x,n) for x in perms_in_sequence], n, min_length=min_length, overlap_threshold=0.95) and len(perms_in_sequence) > 0:
            mega_hypotheticals[next_mega_id] = ProdigalResult(combined_sequence, next_mega_id)
            next_mega_id -= 1

    return mega_hypotheticals
```

### src/unified/embedder/custom.py

```python
import numpy as np
def _hash_seed(payload: dict) -> int:
    import hashlib, json
    h = hashlib.sha256(json.dumps(payload, sort_keys=True).encode('utf-8')).digest()
    return int.from_bytes(h[:4], 'big')
def get_embed(dim: int = 64):
    def _f(payload: dict):
        rng = np.random.RandomState(_hash_seed(payload))
        v = rng.randn(dim).astype(np.float32)
        v /= (np.linalg.norm(v)+1e-12)
        return v
    return _f

```

### src/unified/embedder/registry.py

```python
from __future__ import annotations
from typing import Callable, Dict, Optional
import importlib, os
import numpy as np

_REG: Dict[str, Callable[[int], Callable[[dict], np.ndarray]]] = {}
_DEFAULT: Optional[str] = None

def register_embedder(name: str, factory: Callable[[int], Callable[[dict], np.ndarray]]):
    _REG[name] = factory

def list_embedders():
    return sorted(_REG.keys())

def set_default(name: str):
    global _DEFAULT
    if name not in _REG:
        raise KeyError(f'Unknown embedder: {name}')
    _DEFAULT = name

def load_embedder(name: Optional[str]=None, dim: int=64):
    target = name or os.getenv('UNIFIED_EMBEDDER') or _DEFAULT
    if target and target in _REG:
        return _REG[target](dim)
    # fallback to winner or custom
    try:
        from unified.embedder.winner import get_embed as _get
        return _get(dim)
    except Exception:
        from unified.embedder.custom import get_embed as _get
        return _get(dim)

# Auto-discovery of local candidates (cand_*.py modules)
def _discover_local():
    import pkgutil
    from unified.embedder.autotune import find_embed_callables, to_embed
    pkg = importlib.import_module('unified.embedder')
    mods = []
    for _, modname, _ in pkgutil.iter_modules(pkg.__path__):
        if modname.startswith('cand_') or modname in ('winner','custom'):
            mods.append(f'unified.embedder.{modname}')
    funcs = find_embed_callables(mods)
    for mn, name, fn in funcs:
        alias = '{}::{}'.format(mn.split('.')[-1], name)
        def _factory(dim, _fn=fn):
            from unified.embedder.autotune import to_embed
            return to_embed(_fn, dim)
        register_embedder(alias, _factory)

_discover_local()

```

### src/unified/embedder/winner.py

```python
import numpy as np
from importlib import import_module
MN='baseline'
FN='baseline'
def get_embed(dim: int = 64):
    M = import_module(MN)
    f = getattr(M, FN)
    def _wrap(payload: dict):
        v = f(payload)
        v = np.asarray(v, dtype=np.float32).reshape(-1)
        if v.size != dim:
            rng = np.random.RandomState(abs(hash(FN)) % (2**32))
            P = rng.randn(v.size, dim).astype(np.float32)
            v = v @ P
        n = np.linalg.norm(v)+1e-12
        return (v/n).astype(np.float32)
    return _wrap

```

### src/unified/integration/__init__.py

```python

```

### src/unified/integration/agrm_adapter.py

```python

"""Adapter: expose existing AGRM runner as a SNAP Builder with normalized snapshots."""
from __future__ import annotations
from typing import Any, Dict, Optional, List
import time
from unified.agrm import runner as _runner
from unified.agrm import snapshots as _snaps
from unified.agrm import utils as _utils

def _stable_hash(d: Dict[str, Any]) -> str:
    return _utils.stable_hash(repr(sorted(d.items())))

class AGRMBuilder:
    def build(self, instance: Dict[str, Any], *, seed: Optional[int]=None) -> Dict[str, Any]:
        params = instance.get('params', {})
        res = _runner.run_once(params, seed=seed)
        # Collect snapshots from the existing snapshot system (best-effort)
        snaps_raw: List[Dict[str, Any]] = list(_snaps.list_snapshots())
        norm_snaps = []
        now = time.time()
        for i, s in enumerate(snaps_raw):
            payload = {
                'route': s.get('route'),
                'steps': s.get('steps'),
                'score': s.get('score'),
                'metrics': s.get('metrics', {}),
            }
            sid = s.get('id') or f"snap-{seed or 0}-{i}"
            snap = {
                'id': sid,
                'seed': seed,
                'ts': s.get('ts', now),
                'params': params,
                'payload': payload,
            }
            snap['hash'] = _stable_hash({'id': sid, 'payload': payload})
            norm_snaps.append(snap)
        return {'result': res, 'snapshots': norm_snaps}

```

### src/unified/integration/agrm_best_adapter.py

```python
from __future__ import annotations
from typing import Any, Dict, Optional
import time
from unified.agrm_best import runner as _runner

def AGRM_result_to_snapshot(res: Dict[str, Any], params: Dict[str, Any], seed: Optional[int]) -> Dict[str, Any]:
    route = res.get('route', [])
    payload = {'route': route, 'steps': [], 'score': float(res.get('score', len(route))), 'metrics': {}}
    sid = 'best-' + str(seed or 0) + '-0'
    snap = {'id': sid, 'seed': seed, 'ts': time.time(), 'params': params, 'payload': payload}
    snap['hash'] = str(hash(str(snap['id']) + str(snap['payload'])))
    return snap

class AGRMBuilderBest:
    def build(self, instance: Dict[str, Any], *, seed: Optional[int]=None) -> Dict[str, Any]:
        params = instance.get('params', {})
        res = _runner.run_once(params, seed=seed)
        snap = AGRM_result_to_snapshot(res, params, seed)
        return {'result': res, 'snapshots': [snap]}

```

### src/unified/integration/agrm_unified_adapter.py

```python
from __future__ import annotations
from typing import Any, Dict, Optional, List
import time
from unified.agrm_unified import runner as _runner
from unified.config.shaper import DEFAULT_FAMILIES, DEFAULT_KS

def AGRM_result_to_snapshot(res: Dict[str, Any], params: Dict[str, Any], seed: Optional[int], idx: int) -> Dict[str, Any]:
    route = res.get('route', [])
    payload = {'route': route, 'steps': [], 'score': float(res.get('score', len(route))), 'metrics': {}, 'family': params.get('family'), 'k': params.get('k')}
    sid = 'unified-' + str(seed or 0) + '-' + str(idx)
    snap = {'id': sid, 'seed': seed, 'ts': time.time(), 'params': params, 'payload': payload}
    snap['hash'] = str(hash(str(snap['id']) + str(snap['payload'])))
    return snap

class AGRMBuilderUnified:
    def build(self, instance: Dict[str, Any], *, seed: Optional[int]=None) -> Dict[str, Any]:
        base = instance.get('params', {})
        families = base.get('families', DEFAULT_FAMILIES)
        ks = base.get('ks', DEFAULT_KS)
        snaps: List[Dict[str, Any]] = []
        idx = 0
        for fam in families:
            for k in ks:
                params = dict(base); params['family']=fam; params['k']=k
                for j in range(6):
                    res = _runner.run_once(params, seed=(seed or 0)+idx)
                    snaps.append(AGRM_result_to_snapshot(res, params, seed, idx))
                    idx += 1
        return {'result': {'ok': True}, 'snapshots': snaps}

```

### src/unified/integration/agrm_v10_adapter.py

```python
from __future__ import annotations
from typing import Any, Dict, Optional
import time
from unified.agrm_v10 import runner as _runner

def AGRM_result_to_snapshot(res: Dict[str, Any], params: Dict[str, Any], seed: Optional[int]) -> Dict[str, Any]:
    route = res.get('route', [])
    payload = {'route': route, 'steps': [], 'score': float(res.get('score', len(route))), 'metrics': {}}
    sid = 'v10-' + str(seed or 0) + '-0'
    snap = {'id': sid, 'seed': seed, 'ts': time.time(), 'params': params, 'payload': payload}
    snap['hash'] = str(hash(str(snap['id']) + str(snap['payload'])))
    return snap

class AGRMBuilderV10:
    def build(self, instance: Dict[str, Any], *, seed: Optional[int]=None) -> Dict[str, Any]:
        params = instance.get('params', {})
        res = _runner.run_once(params, seed=seed)
        snap = AGRM_result_to_snapshot(res, params, seed)
        return {'result': res, 'snapshots': [snap]}

```

### src/unified/integration/agrm_v1_adapter.py

```python
from __future__ import annotations
from typing import Any, Dict, Optional
import time
from unified.agrm_v1 import runner as _runner

def AGRM_result_to_snapshot(res: Dict[str, Any], params: Dict[str, Any], seed: Optional[int]) -> Dict[str, Any]:
    route = res.get('route', [])
    payload = {'route': route, 'steps': [], 'score': float(res.get('score', len(route))), 'metrics': {}}
    sid = 'v1-' + str(seed or 0) + '-0'
    snap = {'id': sid, 'seed': seed, 'ts': time.time(), 'params': params, 'payload': payload}
    snap['hash'] = str(hash(str(snap['id']) + str(snap['payload'])))
    return snap

class AGRMBuilderV1:
    def build(self, instance: Dict[str, Any], *, seed: Optional[int]=None) -> Dict[str, Any]:
        params = instance.get('params', {})
        res = _runner.run_once(params, seed=seed)
        snap = AGRM_result_to_snapshot(res, params, seed)
        return {'result': res, 'snapshots': [snap]}

```

### src/unified/integration/agrm_v2_adapter.py

```python
from __future__ import annotations
from typing import Any, Dict, Optional
import time
from unified.agrm_v2 import runner as _runner

def AGRM_result_to_snapshot(res: Dict[str, Any], params: Dict[str, Any], seed: Optional[int]) -> Dict[str, Any]:
    route = res.get('route', [])
    payload = {'route': route, 'steps': [], 'score': float(res.get('score', len(route))), 'metrics': {}}
    sid = 'v2-' + str(seed or 0) + '-0'
    snap = {'id': sid, 'seed': seed, 'ts': time.time(), 'params': params, 'payload': payload}
    snap['hash'] = str(hash(str(snap['id']) + str(snap['payload'])))
    return snap

class AGRMBuilderV2:
    def build(self, instance: Dict[str, Any], *, seed: Optional[int]=None) -> Dict[str, Any]:
        params = instance.get('params', {})
        res = _runner.run_once(params, seed=seed)
        snap = AGRM_result_to_snapshot(res, params, seed)
        return {'result': res, 'snapshots': [snap]}

```

### src/unified/integration/agrm_v3_adapter.py

```python
from __future__ import annotations
from typing import Any, Dict, Optional
import time
from unified.agrm_v3 import runner as _runner

def AGRM_result_to_snapshot(res: Dict[str, Any], params: Dict[str, Any], seed: Optional[int]) -> Dict[str, Any]:
    route = res.get('route', [])
    payload = {'route': route, 'steps': [], 'score': float(res.get('score', len(route))), 'metrics': {}}
    sid = 'v3-' + str(seed or 0) + '-0'
    snap = {'id': sid, 'seed': seed, 'ts': time.time(), 'params': params, 'payload': payload}
    snap['hash'] = str(hash(str(snap['id']) + str(snap['payload'])))
    return snap

class AGRMBuilderV3:
    def build(self, instance: Dict[str, Any], *, seed: Optional[int]=None) -> Dict[str, Any]:
        params = instance.get('params', {})
        res = _runner.run_once(params, seed=seed)
        snap = AGRM_result_to_snapshot(res, params, seed)
        return {'result': res, 'snapshots': [snap]}

```

### src/unified/integration/agrm_v4_adapter.py

```python
from __future__ import annotations
from typing import Any, Dict, Optional
import time
from unified.agrm_v4 import runner as _runner

def AGRM_result_to_snapshot(res: Dict[str, Any], params: Dict[str, Any], seed: Optional[int]) -> Dict[str, Any]:
    route = res.get('route', [])
    payload = {'route': route, 'steps': [], 'score': float(res.get('score', len(route))), 'metrics': {}}
    sid = 'v4-' + str(seed or 0) + '-0'
    snap = {'id': sid, 'seed': seed, 'ts': time.time(), 'params': params, 'payload': payload}
    snap['hash'] = str(hash(str(snap['id']) + str(snap['payload'])))
    return snap

class AGRMBuilderV4:
    def build(self, instance: Dict[str, Any], *, seed: Optional[int]=None) -> Dict[str, Any]:
        params = instance.get('params', {})
        res = _runner.run_once(params, seed=seed)
        snap = AGRM_result_to_snapshot(res, params, seed)
        return {'result': res, 'snapshots': [snap]}

```

### src/unified/integration/agrm_v5_adapter.py

```python
from __future__ import annotations
from typing import Any, Dict, Optional
import time
from unified.agrm_v5 import runner as _runner

def AGRM_result_to_snapshot(res: Dict[str, Any], params: Dict[str, Any], seed: Optional[int]) -> Dict[str, Any]:
    route = res.get('route', [])
    payload = {'route': route, 'steps': [], 'score': float(res.get('score', len(route))), 'metrics': {}}
    sid = 'v5-' + str(seed or 0) + '-0'
    snap = {'id': sid, 'seed': seed, 'ts': time.time(), 'params': params, 'payload': payload}
    snap['hash'] = str(hash(str(snap['id']) + str(snap['payload'])))
    return snap

class AGRMBuilderV5:
    def build(self, instance: Dict[str, Any], *, seed: Optional[int]=None) -> Dict[str, Any]:
        params = instance.get('params', {})
        res = _runner.run_once(params, seed=seed)
        snap = AGRM_result_to_snapshot(res, params, seed)
        return {'result': res, 'snapshots': [snap]}

```

### src/unified/integration/agrm_v6_adapter.py

```python
from __future__ import annotations
from typing import Any, Dict, Optional
import time
from unified.agrm_v6 import runner as _runner

def AGRM_result_to_snapshot(res: Dict[str, Any], params: Dict[str, Any], seed: Optional[int]) -> Dict[str, Any]:
    route = res.get('route', [])
    payload = {'route': route, 'steps': [], 'score': float(res.get('score', len(route))), 'metrics': {}}
    sid = 'v6-' + str(seed or 0) + '-0'
    snap = {'id': sid, 'seed': seed, 'ts': time.time(), 'params': params, 'payload': payload}
    snap['hash'] = str(hash(str(snap['id']) + str(snap['payload'])))
    return snap

class AGRMBuilderV6:
    def build(self, instance: Dict[str, Any], *, seed: Optional[int]=None) -> Dict[str, Any]:
        params = instance.get('params', {})
        res = _runner.run_once(params, seed=seed)
        snap = AGRM_result_to_snapshot(res, params, seed)
        return {'result': res, 'snapshots': [snap]}

```

### src/unified/integration/agrm_v7_adapter.py

```python
from __future__ import annotations
from typing import Any, Dict, Optional
import time
from unified.agrm_v7 import runner as _runner

def AGRM_result_to_snapshot(res: Dict[str, Any], params: Dict[str, Any], seed: Optional[int]) -> Dict[str, Any]:
    route = res.get('route', [])
    payload = {'route': route, 'steps': [], 'score': float(res.get('score', len(route))), 'metrics': {}}
    sid = 'v7-' + str(seed or 0) + '-0'
    snap = {'id': sid, 'seed': seed, 'ts': time.time(), 'params': params, 'payload': payload}
    snap['hash'] = str(hash(str(snap['id']) + str(snap['payload'])))
    return snap

class AGRMBuilderV7:
    def build(self, instance: Dict[str, Any], *, seed: Optional[int]=None) -> Dict[str, Any]:
        params = instance.get('params', {})
        res = _runner.run_once(params, seed=seed)
        snap = AGRM_result_to_snapshot(res, params, seed)
        return {'result': res, 'snapshots': [snap]}

```

### src/unified/integration/agrm_v8_adapter.py

```python
from __future__ import annotations
from typing import Any, Dict, Optional
import time
from unified.agrm_v8 import runner as _runner

def AGRM_result_to_snapshot(res: Dict[str, Any], params: Dict[str, Any], seed: Optional[int]) -> Dict[str, Any]:
    route = res.get('route', [])
    payload = {'route': route, 'steps': [], 'score': float(res.get('score', len(route))), 'metrics': {}}
    sid = 'v8-' + str(seed or 0) + '-0'
    snap = {'id': sid, 'seed': seed, 'ts': time.time(), 'params': params, 'payload': payload}
    snap['hash'] = str(hash(str(snap['id']) + str(snap['payload'])))
    return snap

class AGRMBuilderV8:
    def build(self, instance: Dict[str, Any], *, seed: Optional[int]=None) -> Dict[str, Any]:
        params = instance.get('params', {})
        res = _runner.run_once(params, seed=seed)
        snap = AGRM_result_to_snapshot(res, params, seed)
        return {'result': res, 'snapshots': [snap]}

```

### src/unified/integration/agrm_v9_adapter.py

```python
from __future__ import annotations
from typing import Any, Dict, Optional
import time
from unified.agrm_v9 import runner as _runner

def AGRM_result_to_snapshot(res: Dict[str, Any], params: Dict[str, Any], seed: Optional[int]) -> Dict[str, Any]:
    route = res.get('route', [])
    payload = {'route': route, 'steps': [], 'score': float(res.get('score', len(route))), 'metrics': {}}
    sid = 'v9-' + str(seed or 0) + '-0'
    snap = {'id': sid, 'seed': seed, 'ts': time.time(), 'params': params, 'payload': payload}
    snap['hash'] = str(hash(str(snap['id']) + str(snap['payload'])))
    return snap

class AGRMBuilderV9:
    def build(self, instance: Dict[str, Any], *, seed: Optional[int]=None) -> Dict[str, Any]:
        params = instance.get('params', {})
        res = _runner.run_once(params, seed=seed)
        snap = AGRM_result_to_snapshot(res, params, seed)
        return {'result': res, 'snapshots': [snap]}

```

### src/unified/integration/bridge.py

```python
from __future__ import annotations
from typing import Dict, Any, Iterable
import numpy as np
from unified.embedder.registry import load_embedder
from unified.integration.bridge_converters import snap_to_agrm, agrm_to_snap
from unified.agrm_unified import runner as _runner
from unified.e8.glyphs import make_glyph

def snapshot_to_glyphs(snapshot: Dict[str, Any], *, embed=None) -> Iterable[Dict[str, Any]]:
    """Convert a shaped snapshot to glyphs. If embed is None, use registry loader.
    1) Convert SNAP-ish payload to AGRM params
    2) Run AGRM once
    3) Convert AGRM result back to SNAP payload
    4) Embed payload into a glyph
    """
    payload = snapshot.get('payload', {})
    params = snap_to_agrm(payload)
    seed = snapshot.get('seed', 0)
    res = _runner.run_once(params, seed=seed)
    new_payload = agrm_to_snap(res, params=params)
    emb = embed or load_embedder(64)
    vec = emb(new_payload)
    g = make_glyph(snapshot.get('id'), vec, meta={'payload': new_payload, 'params': params})
    yield g

```

### src/unified/integration/bridge_converters.py

```python
from __future__ import annotations
from typing import Dict, Any

def snap_to_agrm(payload: Dict[str, Any]) -> Dict[str, Any]:
    """Convert a SNAP glyph payload to AGRM params.
    Strategy: pass through core keys and remap known aliases. Keep extras in 'extras'.
    """
    params: Dict[str, Any] = {}
    # Pass-through
    for k in ('route','score','family','k'):
        if k in payload:
            params[k] = payload[k]
    # Aliases
    if 'steps' in payload:
        params['history'] = payload['steps']
    # Extras
    extras = {k:v for k,v in payload.items() if k not in params and k not in ('steps',)}
    if extras:
        params['extras'] = extras
    return params

def agrm_to_snap(result: Dict[str, Any], *, params: Dict[str, Any]) -> Dict[str, Any]:
    """Convert an AGRM result to a SNAP-friendly payload.
    Required keys: 'route'. Optional: 'score'. Attach params context.
    """
    route = result.get('route', [])
    score = float(result.get('score', len(route)))
    payload = {
        'route': route,
        'steps': result.get('steps', []),
        'score': score,
        'metrics': result.get('metrics', {}),
        'family': params.get('family'),
        'k': params.get('k'),
    }
    return payload

```

### src/unified/integration/e8_adapter.py

```python

"""Adapter shell for E8 lattice under SNAP Lattice Protocol.
This remains minimal until concrete E8 modules are implemented.
"""
from typing import Any, Dict, Optional, Sequence
class NoopLattice:
    def __init__(self): self._items = []
    def ingest(self, items: Sequence[Dict[str, Any]]) -> None: self._items.extend(items)
    def promote(self) -> Dict[str, Any]: return {'promoted': 0, 'reason': 'E8 not yet implemented'}
    def query(self, q: Dict[str, Any], *, budget: int=10) -> Sequence[Dict[str, Any]]: return []

```

### src/unified/integration/mdhg_adapter.py

```python

"""Adapter: expose MDHG index as a SNAP Index."""
from typing import Any, Dict, Optional
from unified.mdhg.index import MDHGIndex

class MDHGIndexAdapter:
    def __init__(self):
        self._idx = MDHGIndex()

    def put(self, key: str, payload: Dict[str, Any]) -> None:
        self._idx.put(key, payload)

    def get(self, key: str) -> Optional[Dict[str, Any]]:
        return self._idx.get(key)

    def exists(self, key: str) -> bool:
        return self._idx.exists(key)

```

### src/unified/persistence/export_run.py

```python
import json, sqlite3
from pathlib import Path
from .schema import ensure_schema
def export_run(db_path: str, out_path: str):
    con = sqlite3.connect(db_path)
    ensure_schema(con)
    cur = con.cursor()
    dump = {'meta': {}, 'shells': [], 'members': [], 'glyphs': []}
    try:
        for k,v in cur.execute('SELECT key, value FROM meta'):
            dump['meta'][k] = v
    except Exception:
        pass
    try:
        for row in cur.execute('SELECT sid, center, promoted, meta FROM shells'):
            dump['shells'].append({'sid': row[0], 'center': row[1], 'promoted': row[2], 'meta': row[3]})
    except Exception:
        pass
    try:
        for row in cur.execute('SELECT sid, gid FROM shell_members'):
            dump['members'].append({'sid': row[0], 'gid': row[1]})
    except Exception:
        pass
    try:
        for row in cur.execute('SELECT gid, payload FROM glyphs LIMIT 5000'):
            dump['glyphs'].append({'gid': row[0], 'payload': row[1]})
    except Exception:
        pass
    Path(out_path).write_text(json.dumps(dump, indent=2), encoding='utf-8')

```

### src/unified/persistence/import_run.py

```python
import json, sqlite3
from .schema import ensure_schema
def import_run(db_path: str, json_path: str):
    con = sqlite3.connect(db_path)
    ensure_schema(con)
    cur = con.cursor()
    data = json.loads(open(json_path, 'r', encoding='utf-8').read())
    # meta
    for k,v in data.get('meta', {}).items():
        cur.execute('INSERT OR REPLACE INTO meta(key,value) VALUES(?,?)', (k, str(v)))
    # simple upserts; ignore conflicts if schema differs
    try:
        for s in data.get('shells', []):
            cur.execute('INSERT OR IGNORE INTO shells(sid,center,promoted,meta) VALUES(?,?,?,?)', (s.get('sid'), s.get('center'), s.get('promoted'), s.get('meta')))
    except Exception:
        pass
    try:
        for m in data.get('members', []):
            cur.execute('INSERT OR IGNORE INTO shell_members(sid,gid) VALUES(?,?)', (m.get('sid'), m.get('gid')))
    except Exception:
        pass
    con.commit()

```

### src/unified/persistence/prune.py

```python
import sqlite3
def prune(db_path: str, keep_last: int = 10000):
    con = sqlite3.connect(db_path)
    cur = con.cursor()
    try:
        cur.execute('DELETE FROM glyphs WHERE gid NOT IN (SELECT gid FROM glyphs ORDER BY gid DESC LIMIT ?)', (keep_last,))
        con.commit()
    except Exception:
        pass

```

### src/unified/persistence/schema.py

```python
SCHEMA_VERSION = 1
def ensure_schema(con):
    cur = con.cursor()
    cur.execute('CREATE TABLE IF NOT EXISTS meta (key TEXT PRIMARY KEY, value TEXT)')
    # record schema version
    cur.execute('INSERT OR REPLACE INTO meta(key,value) VALUES(?,?)', ('schema_version', str(SCHEMA_VERSION)))
    con.commit()

```

### src/unified/pipeline.py

```python
from __future__ import annotations
import os, json, time, uuid, datetime
from pathlib import Path
from typing import Dict, Any, Optional
from unified.integration.agrm_unified_adapter import AGRMBuilderUnified
from unified.integration.mdhg_adapter import MDHGIndexAdapter
from unified.integration import bridge as br
from unified.e8.persistence import db
from unified.persistence.schema import ensure_schema
from unified.e8.jobs.queue import sweep_promotion
import sqlite3, json as _json

def build_stamp(params: Dict[str, Any]) -> Dict[str, Any]:
    ts = datetime.datetime.utcnow().isoformat() + 'Z'
    return {
        'timestamp': ts,
        'run_id': str(uuid.uuid4()),
        'params': params,
    }


def _write_meta(stamp: dict):
    import sqlite3, os
    db_path = os.environ.get('UNIFIED_DB')
    con = sqlite3.connect(db_path)
    ensure_schema(con)
    cur = con.cursor()
    for k,v in stamp.items():
        cur.execute('INSERT OR REPLACE INTO meta(key,value) VALUES(?,?)', (k, json.dumps(v)))
    con.commit()
def _log_line(log_path: Path, event: str, **fields):
    rec = {'ts': time.time(), 'event': event}
    rec.update(fields)
    with log_path.open('a', encoding='utf-8') as f:
        f.write(json.dumps(rec) + '\n')

def run_pipeline(*, params: Dict[str, Any], seed: Optional[int]=None, reports_dir: Path=None, db_path: Optional[Path]=None) -> Dict[str, Any]:
    reports_dir = reports_dir or Path.cwd() / 'reports'
    reports_dir.mkdir(parents=True, exist_ok=True)
    log_path = reports_dir / 'run.log.jsonl'
    stamp = build_stamp(params)
    _write_meta(stamp)
    _log_line(log_path, 'start', stamp=stamp)
    # DB setup
    if db_path is None:
        db_path = Path.cwd() / f"run_{stamp['run_id']}.db"
    os.environ['UNIFIED_DB'] = str(db_path)
    db.init()
    # Build
    _log_line(log_path, 'build.begin', params=params)
    builder = AGRMBuilderUnified()
    out = builder.build({'params': params}, seed=seed)
    snaps = out['snapshots']
    _log_line(log_path, 'build.end', count=len(snaps))
    # Index
    _log_line(log_path, 'index.begin')
    idx = MDHGIndexAdapter()
    ing = 0
    for s in snaps:
        for g in br.snapshot_to_glyphs(s):
            db.put_glyph(g); ing += 1
    _log_line(log_path, 'index.end', ingested=ing)
    # Sweep
    _log_line(log_path, 'sweep.begin')
    promo = sweep_promotion(bits=2, seed=seed or 0)
    _log_line(log_path, 'sweep.end', promo=promo)
    # Summary
    e8 = _aggregate_e8(str(db_path))
    summary = {
        'stamp': stamp,
        'ingested': ing,
        'promotion': promo,
        'e8_shells': e8,
        'db_path': str(db_path),
    }
    (reports_dir / 'run_summary.json').write_text(json.dumps(summary, indent=2), encoding='utf-8')
    _log_line(log_path, 'done', summary_path=str(reports_dir / 'run_summary.json'))
    return summary


def _aggregate_e8(db_path: str):
    con = sqlite3.connect(db_path)
    cur = con.cursor()
    try:
        shells = cur.execute('SELECT sid FROM shells').fetchall()
    except Exception:
        return {}
    out = {}
    for (sid,) in shells:
        try:
            gids = [r[0] for r in cur.execute('SELECT gid FROM shell_members WHERE sid=?', (sid,)).fetchall()]
            e8s = []
            for gid in gids:
                row = cur.execute('SELECT meta FROM glyphs WHERE gid=?', (gid,)).fetchone()
                if not row: continue
                try:
                    meta = _json.loads(row[0]) if isinstance(row[0], str) else row[0]
                except Exception:
                    continue
                e8 = (meta or {}).get('e8') or {}
                summ = e8.get('summary')
                if isinstance(summ, dict):
                    e8s.append(summ)
            if e8s:
                avg = lambda k: sum(d.get(k,0.0) for d in e8s)/len(e8s)
                out[str(sid)] = {
                    'count': len(e8s),
                    'avg_norm': avg('e8_norm'),
                    'avg_min': avg('e8_min'),
                    'avg_max': avg('e8_max'),
                    'avg_mean': avg('e8_mean'),
                }
        except Exception:
            continue
    return out

```

---
## CLI scripts (bin/)

### bin/run_tests

```bash
#!/usr/bin/env python3
import importlib.util, sys, pathlib
from pathlib import Path
root = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(root / 'src'))
tests = list((root / 'tests').glob('test_*.py'))
failed = 0
for t in tests:
    spec = importlib.util.spec_from_file_location(t.stem, t)
    mod = importlib.util.module_from_spec(spec)
    try:
        spec.loader.exec_module(mod)
        # run any test_* callables
        for name in dir(mod):
            if name.startswith('test_') and callable(getattr(mod, name)):
                getattr(mod, name)()
        print('[PASS]', t.name)
    except Exception as e:
        print('[FAIL]', t.name, e)
        failed += 1
print('Failures:', failed)
sys.exit(1 if failed else 0)

```

### bin/unified

```bash
#!/usr/bin/env python3
import argparse, json
from pathlib import Path
from unified.api import run

def main():
    ap = argparse.ArgumentParser(description='Unified pipeline runner')
    ap.add_argument('--params', type=str, default='{}', help='JSON dict of params')
    ap.add_argument('--seed', type=int, default=10101)
    ap.add_argument('--out', type=str, default='reports')
    args = ap.parse_args()
    params = json.loads(args.params)
    rd = Path(args.out); rd.mkdir(parents=True, exist_ok=True)
    summary = run(params=params, seed=args.seed, reports_dir=rd)
    print(json.dumps(summary, indent=2))

if __name__ == '__main__':
    main()

```

### bin/unified-e8

```bash
#!/usr/bin/env python3
import argparse, json
from unified.e8_harness import map_to_e8, map_from_e8
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument('--payload', type=str, default='{}', help='JSON payload to map')
    args = ap.parse_args()
    p = json.loads(args.payload)
    coords = map_to_e8(p)
    summ = map_from_e8(coords)
    print(json.dumps({'coords': coords.tolist(), 'summary': summ}, indent=2))
if __name__=='__main__': main()

```

### bin/unified-embedder

```bash
#!/usr/bin/env python3
import argparse, json, os
from unified.embedder.registry import list_embedders, load_embedder, set_default

def main():
    ap = argparse.ArgumentParser(description='Embedder registry CLI')
    ap.add_argument('--list', action='store_true')
    ap.add_argument('--use', type=str, default=None, help='Name of embedder to use')
    ap.add_argument('--dim', type=int, default=64)
    ap.add_argument('--dry-run', action='store_true')
    args = ap.parse_args()
    if args.list:
        print(json.dumps({'embedders': list_embedders()}, indent=2))
        return
    if args.use:
        os.environ['UNIFIED_EMBEDDER'] = args.use
        if args.dry_run:
            f = load_embedder(args.use, dim=args.dim)
            v = f({'hello':'world'})
            print(json.dumps({'used': args.use, 'dim': len(v)}, indent=2))
            return
        # just setting env is enough; pipeline will pick it up
        print(json.dumps({'used': args.use}, indent=2))
        return
    print(json.dumps({'embedders': list_embedders()}, indent=2))

if __name__=='__main__': main()

```

### bin/unified-export

```bash
#!/usr/bin/env python3
import argparse
from unified.persistence.export_run import export_run
def main():
    ap = argparse.ArgumentParser(description='Export a run DB to JSON')
    ap.add_argument('db')
    ap.add_argument('out')
    args = ap.parse_args()
    export_run(args.db, args.out)
if __name__=='__main__': main()

```

### bin/unified-glyph

```bash
#!/usr/bin/env python3
import argparse, json
from unified.integration import bridge as br
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument('--payload', type=str, default='{}')
    ap.add_argument('--id', type=str, default='cli-0')
    args = ap.parse_args()
    snap = {'id': args.id, 'seed': 0, 'payload': json.loads(args.payload), 'params':{}}
    glyphs = list(br.snapshot_to_glyphs(snap))
    # print only meta summary to keep it light
    out = []
    for g in glyphs:
        out.append({'id': g.get('id'), 'dim': len(g.get('vec', [])), 'meta_keys': list((g.get('meta') or {}).keys())})
    print(json.dumps({'glyphs': out}, indent=2))
if __name__=='__main__': main()

```

### bin/unified-import

```bash
#!/usr/bin/env python3
import argparse
from unified.persistence.import_run import import_run
def main():
    ap = argparse.ArgumentParser(description='Import a JSON export into a run DB')
    ap.add_argument('db')
    ap.add_argument('json')
    args = ap.parse_args()
    import_run(args.db, args.json)
if __name__=='__main__': main()

```

### bin/unified-prune

```bash
#!/usr/bin/env python3
import argparse
from unified.persistence.prune import prune
def main():
    ap = argparse.ArgumentParser(description='Prune glyphs in a DB, keeping most recent N')
    ap.add_argument('db')
    ap.add_argument('--keep', type=int, default=10000)
    args = ap.parse_args()
    prune(args.db, args.keep)
if __name__=='__main__': main()

```

---
## Reports (filenames only)
- reports/agrm_best_selection.json
- reports/agrm_candidates.json
- reports/agrm_variant_eval.json
- reports/agrm_variant_eval_full.json
- reports/batch_after_franken.json
- reports/batch_after_merge.json
- reports/batch_rebuild_report.json
- reports/batch_report.json
- reports/block_merge_summary.json
- reports/confirm_after_ext_merges.json
- reports/deep_dive_unclear.json
- reports/deep_dive_unclear.md
- reports/embedder_aggressive_report.json
- reports/embedder_autotune_report.json
- reports/embedder_hunt.json
- reports/embedder_weighted_ensemble_report.json
- reports/familyA_stable_final_report.json
- reports/franken_merge_summary.json
- reports/franken_refined_merge_report.json
- reports/goldens.json
- reports/goldens_expanded.json
- reports/harvest_index.json
- reports/inclusion_plan.json
- reports/inclusion_plan.promoted.json
- reports/inclusion_promotions.json
- reports/knowledge_gaps.json
- reports/overall_stability_confirm.json
- reports/run.log.jsonl
- reports/run_summary.json
- reports/stability_diagnostics.json
- reports/tests_after_prune.txt
- reports/tests_output.txt
- reports/unclear_blocks.json
- reports/weighted_ensemble_search.json