Section 20 â€” Data Planes & Evidence Stores
Make every CQE run auditable, portable, and replayableâ€”without ever leaking meaning you didnâ€™t intend to share.
ğŸ”¹ What youâ€™re building (one screen)
â€¢ Evidence Plane â€” immutable, content-addressed objects (artifacts, metrics, diffs, manifests).
â€¢ Receipts Plane â€” compact per-view receipts + job commit-4 rows (and optional 8/64-bit rollups).
â€¢ Ledger Plane â€” append-only timeline of commits, strict-ladder advances, promotions, and rollbacks.
â€¢ Query Plane â€” fast indices over octet faces, mirror deltas, Î”-lift vocab, automorphism classes.
â€¢ Redaction Plane â€” policy filters; only hashed/aggregated evidence crosses trust boundaries.
Everything hangs off a single Merkle root per job so any third party can verify â€œthis row equals that evidence.â€
ğŸ”¹ Data model (minimal, sufficient)
1) Artifact bundle (content-addressed)
/evidence/ {sha256}/ tokens.json # meaning pack used (or its salted hash if redacted) preconditions.json # unit guards, bounds, S-target octet_plan.json # H1..H8 mappings, 8Ã—8 local, 4Ã— surround spec viewer_outputs/ H1/.../metrics.json # per-face metrics (e.g., MAE, SC, drift hist) H1/.../mirror.json # forwardâ†”inverse agreements ... playbook_deltas/ H1/delta.patch # Î”-lift applied (canonical diff or op-list) ... strict_gate.json # thresholds before/after (ratchet evidence) runtime/ versions.lock # PB/VIEW/SC build hashes cost.json # time/energy/$ tallies provenance/ env.sig # attestation, signer, clock, S-level 
2) Receipts (per view & job)
{ "job_id": "J/2025-09-20/â€¦", "anchor_form": "LEECH_PATCH_A1", "automorphism": {"group":"M24","element":"Ïƒ_173842"}, "view_id": "H3", "metrics": {"MAE":0.021, "SC":0.984, "Î”E":-0.37}, "mirror": {"pass":true, "drift":0.0098}, "delta_applied": "ghoststrip:Î”#14", "strict": {"gate":"WFE_nm<=22","status":"held"}, "commit4": "1011", // job-level repeats on every view receipt "evidence_root": "merkle:6b1fâ€¦" } 
3) Ledger row (append-only)
ts, job_id, anchor, automorphism_id, S_level, commit4, evidence_root, octet_coverage(8/8), mirror_pass(8/8), strict_event(Â±), cost, actor, notes 
ğŸ”¹ Merkle everything (one reason: trust)
â€¢ Leafs: hashes of each concrete file (metrics, mirror, delta patch, plan, versions).
â€¢ Inner nodes: deterministic concatenation hashes.
â€¢ Root: evidence_root stored in the ledger + signed (optional transparency log).
â€¢ Replay: Anyone can regenerate the root from a shared artifact bundle and confirm it equals the ledger.
Implication: receipts are tiny; proofs are cheap; debates are about thresholds, not â€œwhat ran.â€
ğŸ”¹ Shard by automorphism (scale without drift)
Partition storage and indices along the geometry that already governs you:
/forms/ {anchor_form}/ M24/ Ïƒ_173842/ jobs/ 2025-09-20Tâ€¦/evidence_rootâ†’bundle Ïƒ_009514/ ... Monster/ class-2A/â€¦ 
Benefits:
â€¢ Natural canarying and A/B across isomorphic forms.
â€¢ Fast retrieval: â€œshow me all Ïƒ_173842 runs with commit4=1011 in S3 last 30 days.â€
â€¢ Easy garbage collection by orbit if a form is deprecated.
ğŸ”¹ Indices that matter
â€¢ Octet Index: (job_id, face) â†’ pass/metrics/cost.
â€¢ Mirror Delta Index: histogram buckets of drift (find hot seams fast).
â€¢ Î”-lift Vocabulary Index: counts & contexts of each delta recipe (see which repairs actually win).
â€¢ Strict Ladder Index: gate movement timeline per anchor/S-level.
â€¢ Automorphism Index: orbit health; oddities confined to a specific element or across the class?
All indices point to the evidence_root; nothing is denormalized that would break verification.
ğŸ”¹ Redaction & safety (first-class, not bolt-on)
â€¢ Semantic firewall: Viewers/sidecars may handle sensitive domain inputs, but only emit hashed descriptors + stats beyond the trust boundary.
â€¢ Two-tier tokens: tokens.json â‡’ either full content (private projects) or tokens.redacted.json with salted token-glyph hashes (shared projects).
â€¢ Receipts-only sharing: For external audits, ship receipts.ndjson + merkle.proof. External parties verify math and process without raw data.
â€¢ Policy table: Domain â†’ allowed fields, rounding, minimum k-anonymity, salt rotation schedule.
ğŸ”¹ APIs (clean, boring, reliable)
Ingest
POST /api/v1/jobs { tokens_ref, anchor, automorphism, target_S, octet_plan_ref, policy_ref } â†’ { job_id } 
Push evidence
PUT /api/v1/jobs/{job_id}/evidence/{path} (streamed; content-addressed) 
Close & commit
POST /api/v1/jobs/{job_id}/close â†’ { commit4, evidence_root, ledger_row } 
Query
GET /api/v1/receipts?anchor=LEECH_PATCH_A1&commit4=1011&since=30d GET /api/v1/hotspots?kind=mirror&top=50 GET /api/v1/deltas?name=ghoststrip:Î”#14&context=H3 GET /api/v1/proof/{evidence_root} â†’ merkle.proof 
Replay (deterministic)
POST /api/v1/replay { evidence_root, sandbox=true } â†’ re-run & compare receipts bit-for-bit 
ğŸ”¹ Storage classes & retention
â€¢ Active (SSD): last 7â€“30 days S3+ jobs for quick diff & rollback.
â€¢ Warm (object store): 12â€“24 months of bundles; indices always hot.
â€¢ Cold (erasure-coded): beyond 24 months; receipts/ledgers stay hot forever.
â€¢ Non-Working Album: tiny slips (reason, minimal context, evidence_root); retained long-term for learning.
Retention gates tie to S-level and regulatory domain.
ğŸ”¹ Reliability & self-heal
â€¢ Background verifiers: sample 1% of roots/hour; recompute Merkle; alert on mismatch.
â€¢ Checksum-on-read: every artifact fetch is hashed; silent bit-rot canâ€™t pass.
â€¢ Dual writes: ledger replicated (append-only), evidence on multi-AZ object stores.
â€¢ Idempotent ingest: same content, same address; safe to retry.
ğŸ”¹ What it looks like (three scenes)
Audit in 90 seconds (Regulated S4)
â€¢ Auditor requests: â€œShow all S4 commits with WFE gate tightened last quarter.â€
â€¢ Query plane returns 17 rows; each links to evidence_root.
â€¢ Auditor pulls merkle.proof for 3 random jobs, verifies roots locally.
â€¢ One job shows mirror drift spike on face H6 with rollback within 2 minutesâ€”visible, explained, receipts clean.
Outcome: Trust with no raw data leaving your boundary.
Portability to a partner lab (Wet-lab S2 â†’ S3)
â€¢ You ship receipts.ndjson + two redacted bundles (salts rotated).
â€¢ Partner replays with their viewers; Merkle roots match; only thresholds differ.
â€¢ Disagreements are about tolerances, not â€œwhat was runâ€ or â€œwhat data was used.â€
Instant forensic on a hot seam (Prod S3)
â€¢ â€œWhy did commit-4 flip 1011â†’1101 on H3 last night?â€
â€¢ Mirror index shows a burst of phase retrieval failures clustered under Ïƒ_009514.
â€¢ Î”-lift index reveals a new Î”#27 appeared only in that orbit.
â€¢ Rollback already executed; strict not advanced; PB pinned.
You fix Î”#27 or gate it to S1; system stayed honest.
ğŸ”¹ Operator dashboards
â€¢ Octet board: faces Ã— pass/Î”E/cost, sparklines; click-through to evidence.
â€¢ Mirror radar: faces by drift sigma; hotspot list with automorphism tags.
â€¢ Strict ladder: gate timelines, who/what advanced, receipts links.
â€¢ Delta atlas: top deltas by win-rate and domain; retirement candidates.
â€¢ Cost view: expected vs realized per PB; governor decisions with rationale.
ğŸ”¹ Hands-on exercises
â€¢ Build a bundle: Run a tiny S2 job end-to-end; create evidence_root; verify locally.
â€¢ Redact & prove: Swap to redacted tokens; share receipts+proof to a teammate; have them verify root equality.
â€¢ Shard stress: Re-run the same task across 4 automorphism elements; compare orbit health.
â€¢ Replay check: Pull an old bundle (cold store), replay in sandbox; assert byte-for-byte receipt match.