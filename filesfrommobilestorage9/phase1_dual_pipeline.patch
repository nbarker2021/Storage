diff --git a/retrieval/scorer.py b/retrieval/scorer.py\nnew file mode 100644\nindex 0000000..9864b61\n--- /dev/null\n+++ b/retrieval/scorer.py\n@@ -0,0 +1,40 @@\n+\n+from __future__ import annotations\n+from dataclasses import dataclass\n+from typing import Protocol, Iterable, Tuple\n+import numpy as np\n+\n+class Geodesic(Protocol):\n+    def distance(self, a: np.ndarray, b: np.ndarray) -> float: ...\n+\n+@dataclass(slots=True)\n+class ScorerConfig:\n+    alpha: float = 1.0  # cosine weight\n+    beta: float = 0.0   # geodesic weight\n+\n+class PlaceholderGeodesic:\n+    def distance(self, a: np.ndarray, b: np.ndarray) -> float:\n+        # TODO: replace with true E8 geodesic (Coxeter/shortest path)\n+        # proxy: 1 - cosine similarity on normalized vectors\n+        na = a / (np.linalg.norm(a) + 1e-12)\n+        nb = b / (np.linalg.norm(b) + 1e-12)\n+        cos = float(np.dot(na, nb))\n+        return 1.0 - max(min(cos, 1.0), -1.0)\n+\n+def blended_score(q: np.ndarray, docs: np.ndarray, cfg: ScorerConfig, g: Geodesic | None = None) -> np.ndarray:\n+    qn = q / (np.linalg.norm(q) + 1e-12)\n+    dn = docs / (np.linalg.norm(docs, axis=1, keepdims=True) + 1e-12)\n+    cos = dn @ qn\n+    if cfg.beta == 0.0:\n+        return cfg.alpha * cos\n+    if g is None:\n+        g = PlaceholderGeodesic()\n+    # gscore = 1/(1+d), where d is a distance; clip to [0,1]\n+    dists = np.array([g.distance(q, d) for d in docs], dtype=float)\n+    gscore = 1.0 / (1.0 + dists)\n+    return cfg.alpha * cos + cfg.beta * gscore\n+\n+def rerank(q: np.ndarray, docs: np.ndarray, cfg: ScorerConfig, topk: int = 50, g: Geodesic | None = None) -> Tuple[np.ndarray, np.ndarray]:\n+    scores = blended_score(q, docs, cfg, g)\n+    idx = np.argsort(-scores)[:topk]\n+    return idx, scores[idx]\n
diff --git a/retrieval/dual_compare.py b/retrieval/dual_compare.py\nnew file mode 100644\nindex 0000000..e93a9ed\n--- /dev/null\n+++ b/retrieval/dual_compare.py\n@@ -0,0 +1,54 @@\n+\n+from __future__ import annotations\n+import json\n+from dataclasses import dataclass\n+from typing import Dict, Any, List, Tuple\n+import numpy as np\n+\n+from lattice_ai.bridge.dual_stack import load_dual, maybe_emit_snap\n+from retrieval.scorer import ScorerConfig, rerank\n+\n+@dataclass(slots=True)\n+class RetrievalReport:\n+    k: int\n+    overlap_at_k: float\n+    jaccard_at_k: float\n+    rank_corr_tau: float\n+\n+def _kendall_tau(a: List[int], b: List[int]) -> float:\n+    # very small n; naive Kendall tau\n+    n = len(a)\n+    if n <= 1:\n+        return 1.0\n+    pairs = 0\n+    concord = 0\n+    pos_b = {v:i for i,v in enumerate(b)}\n+    for i in range(n):\n+        for j in range(i+1, n):\n+            ai, aj = a[i], a[j]\n+            bi, bj = pos_b[ai], pos_b[aj]\n+            pairs += 1\n+            concord += 1 if bi < bj else 0\n+    return (2*concord/pairs) - 1.0\n+\n+def compare_retrieval(q: np.ndarray, docs: np.ndarray, topk: int = 20, axes: Dict[str, Any] | None = None) -> Dict[str, Any]:\n+    v14, legacy, _ = load_dual()\n+    cfg = ScorerConfig(alpha=1.0, beta=0.15)  # small beta proxy\n+    idx_v14, _ = rerank(q, docs, cfg, topk=topk)\n+    idx_leg, _ = rerank(q, docs, cfg, topk=topk) if legacy else (idx_v14, None)  # same scorer; geometry parity tested elsewhere\n+\n+    a = list(map(int, idx_v14))\n+    b = list(map(int, idx_leg))\n+    overlap = len(set(a) & set(b)) / float(topk)\n+    jaccard = len(set(a) & set(b)) / float(len(set(a) | set(b)))\n+    tau = _kendall_tau(a, b)\n+    report = {\n+        "topk": topk,\n+        "overlap_at_k": overlap,\n+        "jaccard_at_k": jaccard,\n+        "rank_corr_tau": tau,\n+    }\n+    if axes is None:\n+        axes = {"version_container": "v14", "tenant": "demo", "domain": "retrieval"}\n+    maybe_emit_snap(report, axes)\n+    return report\n
diff --git a/shelling/dual_compare_shells.py b/shelling/dual_compare_shells.py\nnew file mode 100644\nindex 0000000..d8d02d4\n--- /dev/null\n+++ b/shelling/dual_compare_shells.py\n@@ -0,0 +1,31 @@\n+\n+from __future__ import annotations\n+from typing import Dict, Any\n+import numpy as np\n+from lattice_ai.bridge.dual_stack import load_dual, maybe_emit_snap\n+\n+def shell_metrics(center: np.ndarray, shell: np.ndarray) -> Dict[str, float]:\n+    # cosine coherence and redundancy\n+    c = center / (np.linalg.norm(center) + 1e-12)\n+    S = shell / (np.linalg.norm(shell, axis=1, keepdims=True) + 1e-12)\n+    coherence = float(np.mean(S @ c))\n+    # redundancy: average pairwise cosine\n+    if len(S) <= 1:\n+        redundancy = 0.0\n+    else:\n+        sims = S @ S.T\n+        n = sims.shape[0]\n+        redundancy = float((np.sum(sims) - n) / (n*(n-1)))\n+    return {"coherence": coherence, "redundancy": redundancy}\n+\n+def compare_shells(center: np.ndarray, neighbors_v14: np.ndarray, neighbors_leg: np.ndarray | None, axes: Dict[str, Any] | None = None) -> Dict[str, Any]:\n+    m14 = shell_metrics(center, neighbors_v14)\n+    if neighbors_leg is None:\n+        rep = {"v14": m14, "legacy": None}\n+    else:\n+        mL = shell_metrics(center, neighbors_leg)\n+        rep = {"v14": m14, "legacy": mL, "delta": {"coherence": m14["coherence"]-mL["coherence"], "redundancy": m14["redundancy"]-mL["redundancy"]}}\n+    if axes is None:\n+        axes = {"version_container":"v14","tenant":"demo","domain":"shelling"}\n+    maybe_emit_snap(rep, axes)\n+    return rep\n
diff --git a/tests/test_scorer.py b/tests/test_scorer.py\nnew file mode 100644\nindex 0000000..89f9961\n--- /dev/null\n+++ b/tests/test_scorer.py\n@@ -0,0 +1,10 @@\n+\n+import numpy as np\n+from retrieval.scorer import ScorerConfig, blended_score\n+\n+def test_blended_monotonicity():\n+    q = np.eye(8)[0]\n+    docs = np.eye(8)\n+    s = blended_score(q, docs, ScorerConfig(alpha=1.0, beta=0.0))\n+    # best match should be index 0 with cosine=1\n+    assert int(np.argmax(s)) == 0\n
diff --git a/tests/test_dual_retrieval.py b/tests/test_dual_retrieval.py\nnew file mode 100644\nindex 0000000..55c689e\n--- /dev/null\n+++ b/tests/test_dual_retrieval.py\n@@ -0,0 +1,10 @@\n+\n+import numpy as np\n+from retrieval.dual_compare import compare_retrieval\n+\n+def test_compare_retrieval_runs():\n+    rng = np.random.default_rng(0)\n+    q = rng.standard_normal(8)\n+    docs = rng.standard_normal((100,8))\n+    rep = compare_retrieval(q, docs, topk=10)\n+    assert "overlap_at_k" in rep and 0.0 <= rep["overlap_at_k"] <= 1.0\n